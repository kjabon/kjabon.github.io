<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://kjabon.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kjabon.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-04-24T15:30:56-05:00</updated><id>https://kjabon.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Continuous Log Likelihood</title><link href="https://kjabon.github.io/blog/2023/ContinuousLogLikelihood/" rel="alternate" type="text/html" title="Continuous Log Likelihood" /><published>2023-04-24T00:00:00-05:00</published><updated>2023-04-24T00:00:00-05:00</updated><id>https://kjabon.github.io/blog/2023/ContinuousLogLikelihood</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/ContinuousLogLikelihood/"><![CDATA[<p>Strangely, I couldn’t find anyone else who had done the math for the log likelihoods of multivariate normal distributions online. Probably because it’s “obvious” to the “mathematically mature,” as if there was an objective definition of that. No no, let’s be good scientists and double check anything that isn’t obvious, especially if it’s widely accepted.</p>

<hr />

<p><a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">PDF</a> of a multivariate normal distribution:</p>

\[f(x) = \frac{1}{\left(2\pi\right)^{\frac{k}{2}}\left| \Sigma \right|^{\frac{1}{2}}}\exp\left(-\frac{1}{2}\left(x-\mu\right)^T\Sigma^{-1}\left(x-\mu\right)\right)\]

<p>where \(\Sigma\) is the (diagonal) covariance matrix, and \(\mu\) is the column vector of means for each dimension.</p>

<p>Take the log of both sides:</p>

\[\log \left(f(x)\right) = \log \left(\frac{1}{\left(2\pi\right)^{\frac{k}{2}}\left| \Sigma \right|^{\frac{1}{2}}}\exp\left(-\frac{1}{2}\left(x-\mu\right)^T\Sigma^{-1}\left(x-\mu\right)\right)\right)\]

<p>Annihilate the exponent:</p>

\[= \log \left(\frac{1}{\left(2\pi\right)^{\frac{k}{2}}\left| \Sigma \right|^{\frac{1}{2}}}\right)  -\frac{1}{2}\left(x-\mu\right)^T\Sigma^{-1}\left(x-\mu\right)\]

<p>Let’s look at the left term first:</p>

\[\log \left(\frac{1}{\left(2\pi\right)^{\frac{k}{2}}\left| \Sigma \right|^{\frac{1}{2}}}\right)\]

<p>Use some log identities:</p>

\[=-\frac{k}{2}\log 2\pi - \log \left|\Sigma \right|^{\frac{1}{2}}\]

<p>The determinant of a diagonal matrix is the product of its diagonal elements:</p>

\[=-\frac{k}{2}\log 2\pi - \log \prod_{i=1}^{k}\Sigma_{i,i}^{\frac{1}{2}}\]

<p>The log of a product of terms is the sum of the logs of those terms (and applying the element-wise square root, and some rearranging):</p>

\[=-\frac{1}{2}\left(k\log 2\pi +  \sum_{i=1}^{k}2\log\sigma_{i}\right)\]

<p>Halfway done. Now let’s look at the right term of the main equation:</p>

\[-\frac{1}{2}\left(x-\mu\right)^T\Sigma^{-1}\left(x-\mu\right)\]

<p>Ok, time to break out the linear algebra:</p>

\[=-\frac{1}{2}\begin{bmatrix} x_1-\mu_1, &amp; x_2-\mu_2, &amp; \dots, &amp; x_k-\mu_k\end{bmatrix} \;
\begin{bmatrix} 1/\sigma^2_1 &amp; &amp; &amp;\\&amp; 1/\sigma^2_2 &amp;&amp;\\ &amp;&amp;\ddots &amp;\\ &amp;&amp;&amp;1/\sigma^2_n \end{bmatrix} \;
\begin{bmatrix} x_1-\mu_1 \\ x_2-\mu_2 \\ \vdots \\ x_k-\mu_k\end{bmatrix} \;\]

\[=-\frac{1}{2}\begin{bmatrix} (x_1-\mu_1)/\sigma^2_1 &amp; (x_2-\mu_2)/\sigma^2_2 &amp; \dots &amp; (x_k-\mu_k)/\sigma^2_k\end{bmatrix} \;

\begin{bmatrix} x_1-\mu_1 \\ x_2-\mu_2 \\ \vdots \\ x_k-\mu_k\end{bmatrix} \;\]

\[=-\frac{1}{2}\sum_{i=1}^k\frac{(x_i-\mu_i)^2}{\sigma_i^2}\]

<p>Adding the first and second terms gives the result:</p>

\[=-\frac{1}{2}\left(k\log 2\pi+\sum_{i=1}^k\left[\frac{(x_i-\mu_i)^2}{\sigma_i^2}+2\log\sigma_i\right]\right)\]

<hr />

<p>I recommend going through it yourself! After all:</p>

<blockquote>
  <p>What I cannot create, I do not understand.</p>
</blockquote>

<p>-Richard Feynman</p>

<p>And one more for the road:</p>

<blockquote>
  <p>Young man, in mathematics you don’t understand things. You just get used to them.</p>
</blockquote>

<p>-John Von Neumann to a student, Felix Smith</p>]]></content><author><name>Kenneth Jabon</name></author><category term="rl" /><summary type="html"><![CDATA[A quick and dirty derivation]]></summary></entry><entry><title type="html">Vanilla Policy Gradient</title><link href="https://kjabon.github.io/blog/2023/VPG/" rel="alternate" type="text/html" title="Vanilla Policy Gradient" /><published>2023-04-14T00:00:00-05:00</published><updated>2023-04-14T00:00:00-05:00</updated><id>https://kjabon.github.io/blog/2023/VPG</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/VPG/"><![CDATA[<p>Vanilla policy gradient is one of the simplest reinforcement learning algorithms. It should serve to form the theoretical foundation for all following policy-based, online algorithms. If you’re more interested in offline algorithms, I recommend you start with TD-learning and deep Q-learning. See chapter 6 of <a href="http://incompleteideas.net/book/the-book-2nd.html">Sutton and Barto</a> for that.</p>

<p>This post assumes you’re familiar with how Markov Decision Processes work, and how the reinforcement learning problem is set up. For a guide to that, see <a href="/blog/2023/RL">here</a>.</p>

<h2 id="a-birds-eye-view">A bird’s eye view</h2>

<p>Let’s describe what happens in a full iteration of the loop before diving in.</p>

<p>First, we collect a batch of trajectories \(\tau\) by allowing our current policy \(\pi\) (represented by a neural network) to unfold in the environment. We collect a full batch because, in general, the policy does not output actions \(a\), but a probability distribution over actions given the current state \(s\): \(\pi(a\vert s)\). To get to the next timestep in our trajectory, we select an action \(a\) by sampling from this distribution. Randomness may also occur as the result of the environment itself, so all in all we want to get plenty of samples to come up with an accurate, representative set of trajectories \(\tau\).</p>

<details><summary>Sampling is great for blind exploration, but…</summary>
<p>Eventually sampling doesn’t do the job any more. At some point we’ll want to exploit what we know and make our way to better states to be able to learn from there, so we’ll just pick the best known action instead of sampling. The amount of “greedy action selection” will be scheduled to increase over time, to progress from pure exploration of the state space to pure exploitation of the policy.</p>
</details>

<p>Now, if a policy gains an above-average sum of rewards in a particular trajectory, we will nudge the policy \(\pi\) in the direction of the actions \(a\) (given their respective states, \(\vert s\)) which resulted in this trajectory. Conversely, if a trajectory comes with a lower sum of rewards, we nudge the policy away from taking those actions. This sum is known as the <strong>return</strong> for a trajectory<d-footnote>This isn't the full return yet, we'll make it more general in a bit.</d-footnote>:</p>

\[R(\tau)=\sum_{t=0}^{T}r_t\]

<p>If any of this feels poorly defined or explained, you’ll want to start with the <a href="/blog/2023/RL">previous</a> post.</p>

<p>To estimate the return at each time step with low variance, we employ generalized advantage estimation (GAE). We train a neural network to represent the value function \(V\), which is incorporated into our advantage term \(A\). This will be discussed in the secion on <a href="#generalized-advantage-estimation">GAE</a>. Also, we actually estimate not the return, but the “future return.” More on that in a bit!</p>

<p>Now, every time we go through our loop, we have a better estimate of what a good trajectory looks like (by training the value function \(V\)), and a better idea of what actions to take to get good trajectories (by training the policy \(\pi\)). This glosses over some details, which we’ll get into in just a bit.</p>

<p>Take a glance over the simplified pseudocode for the algorithm, then let’s get cracking!</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pseudoPseudoVPG.svg-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pseudoPseudoVPG.svg-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pseudoPseudoVPG.svg-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pseudoPseudoVPG.svg" class="img-fluid rounded" width="auto" height="auto" title="pseudoPseudo" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h2 id="breaking-down-the-policy-gradient">Breaking down the policy gradient</h2>

<p>The policy gradient is calculated as follows:</p>

\[\hat{g}_{k}=\frac{1}{\left\vert \mathcal{D}_k \right\vert }\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} \hat{V}_t\]

<p>Let’s break this down to gain an understanding of each term, working from the inside out.</p>

<p>\(\pi_\theta (a_t\vert s_t)\) is the policy, or the function which outputs the probability distribution of all possible actions \(a_t\) at time \(t\) in the trajectory \(\tau\), given the state \(s_t\) at time \(t\). To bring it back to earth, think of this as \(y = f(x,\theta)\).</p>

<details><summary>O_O</summary>
<p>If this is making you feel like a deer in headlights, don’t worry. Let’s be more explicit. \(\theta\) parameterizes our function \(f\), and then we evaluate our function on \(x\). What do I mean? If I had a function \(y = f(x, \theta) = mx + b\), I parameterize this function with \(m\) and \(b\) (represented by the vector \(\theta = [m,b]\)), and then evaluate it on \(x\). In the case of a neural net, \(\theta\) is instead a matrix of numbers (which can be flattened into a vector \([a_1,a_2,a_3,...,a_n]\)) representing its weights and biases.</p>

</details>

<ul>
  <li>If we have a discrete action space, this is a function which outputs a vector with a value for each possible action (in other words, a categorical probability distribution).</li>
  <li>If the action space is continuous, this is a function which outputs the mean \(\mu\) and standard deviation \(\sigma\) representing the action’s probability distribution. <d-footnote> If you're worried about flexibility, it turns out you can also output arbitrary combinations of these, but won't consider that case here.</d-footnote></li>
</ul>

<p><strong>Take the log</strong> of the policy:</p>

\[\log\pi_\theta (a_t\vert s_t)\]

<p>Remember, all that is happening is we’re taking the log of a function, i.e., \(g(x, \theta) = \log(f(x, \theta))\).<br />
<br /></p>

<p><strong>Take the gradient</strong> of this with respect to the policy parameters \(\theta\).</p>

\[\nabla_\theta \log\pi_\theta (a_t\vert s_t)\]

<p>Simply, the gradient of a function, i.e.  \(\nabla g(x, \theta)\) with respect to \(\theta\).<br />
<br /></p>

<p><strong>Evaluate the policy</strong> given the current policy parameters \(\theta_k\):</p>

\[\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\]

<p>Or, we perform inference with the current model on the current state of the environment.</p>

<details><summary>Numerical considerations: feel free to come back to this.</summary>
<p>Let’s take a step back. If we are used to working out derivations with pencil and paper, the order in which I presented the last few steps should not start sounding any alarms.<d-footnote>Assuming you made it through Calculus in one piece. If not, don't worry. Go ahead and take or re-take Calculus, and then come back. You can do it, I promise. If that's too much hassle, luckily we have autodiff which means you can summarily forget about this derivation; so just keep reading with a glazed look for a couple more paragraphs. </d-footnote> Normally in this case one would take the derivative of the symbolic function, then evaluate that to get the derivative at the point of interest, or a similar method of your choice.</p>

<p>However, there are two differences when doing this numerically on your computer; having to do with the log and the derivative.</p>

<h3 id="for-the-log">For the log:</h3>
<p>We’re using a neural network, whose job it is to approximate a function’s output. Well, why go through the trouble of approximating a function, and then taking the log of the output, when we could just encapsulate the log into the function and approximate that directly? And this exactly what we do. We’re interested in the log of the output of a probability distribution (an array of log-likelihoods, one for each action).</p>

<p><strong>For categorical distributions</strong>, it’s very simple.</p>

<p>We just consider the output of the network to be the array: \(\log \left[ P_\theta (s)\right]\).</p>

<p><strong>For continuous distributions</strong>, it’s a little trickier</p>

<p>Remember we usually represent the probability distribution in the continuous case as a multivariate normal distribution. To keep things simple, we actually use a diagonal covariance matrix, rather than the full covariance matrix, so each dimension of the action can be represented by a single standard deviation. This way, we only need to output one mean and one standard deviation per dimension, and calculating the log of the distribution also becomes much easier. Now, how do we take the log of a (diagonal) normal distribution?</p>

<p>Like this!</p>

\[=-\frac{1}{2}\left(k\log 2\pi+\sum_{i=1}^k\left[\frac{(x_i-\mu_i)^2}{\sigma_i^2}+2\log\sigma_i\right]\right)\]

<details><summary>I need proof!</summary>
<p>You’re sure you don’t want to skip this? Ok, here you go.</p>

<p><a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">PDF</a> of a multivariate normal distribution:</p>

\[f(x) = \frac{1}{\left(2\pi\right)^{\frac{k}{2}}\left| \Sigma \right|^{\frac{1}{2}}}\exp\left(-\frac{1}{2}\left(x-\mu\right)^T\Sigma^{-1}\left(x-\mu\right)\right)\]

<p>where \(\Sigma\) is the (diagonal) covariance matrix, and \(\mu\) is the column vector of means for each dimension.</p>

<p>Take the log of both sides:</p>

\[\log \left(f(x)\right) = \log \left(\frac{1}{\left(2\pi\right)^{\frac{k}{2}}\left| \Sigma \right|^{\frac{1}{2}}}\exp\left(-\frac{1}{2}\left(x-\mu\right)^T\Sigma^{-1}\left(x-\mu\right)\right)\right)\]

<p>Annihilate the exponent:</p>

\[= \log \left(\frac{1}{\left(2\pi\right)^{\frac{k}{2}}\left| \Sigma \right|^{\frac{1}{2}}}\right)  -\frac{1}{2}\left(x-\mu\right)^T\Sigma^{-1}\left(x-\mu\right)\]

<p>Let’s look at the left term first:</p>

\[\log \left(\frac{1}{\left(2\pi\right)^{\frac{k}{2}}\left| \Sigma \right|^{\frac{1}{2}}}\right)\]

<p>Use some log identities:</p>

\[=-\frac{k}{2}\log 2\pi - \log \left|\Sigma \right|^{\frac{1}{2}}\]

<p>The determinant of a diagonal matrix is the product of its diagonal elements:</p>

\[=-\frac{k}{2}\log 2\pi - \log \prod_{i=1}^{k}\Sigma_{i,i}^{\frac{1}{2}}\]

<p>The log of a product of terms is the sum of the logs of those terms (and applying the element-wise square root, and some rearranging):</p>

\[=-\frac{1}{2}\left(k\log 2\pi +  \sum_{i=1}^{k}2\log\sigma_{i}\right)\]

<p>Halfway done. Now let’s look at the right term of the main equation:</p>

\[-\frac{1}{2}\left(x-\mu\right)^T\Sigma^{-1}\left(x-\mu\right)\]

<p>Ok, time to break out the linear algebra:</p>

\[=-\frac{1}{2}\begin{bmatrix} x_1-\mu_1, &amp; x_2-\mu_2, &amp; \dots, &amp; x_k-\mu_k\end{bmatrix} \;
\begin{bmatrix} 1/\sigma^2_1 &amp; &amp; &amp;\\&amp; 1/\sigma^2_2 &amp;&amp;\\ &amp;&amp;\ddots &amp;\\ &amp;&amp;&amp;1/\sigma^2_n \end{bmatrix} \;
\begin{bmatrix} x_1-\mu_1 \\ x_2-\mu_2 \\ \vdots \\ x_k-\mu_k\end{bmatrix} \;\]

\[=-\frac{1}{2}\begin{bmatrix} (x_1-\mu_1)/\sigma^2_1 &amp; (x_2-\mu_2)/\sigma^2_2 &amp; \dots &amp; (x_k-\mu_k)/\sigma^2_k\end{bmatrix} \;

\begin{bmatrix} x_1-\mu_1 \\ x_2-\mu_2 \\ \vdots \\ x_k-\mu_k\end{bmatrix} \;\]

\[=-\frac{1}{2}\sum_{i=1}^k\frac{(x_i-\mu_i)^2}{\sigma_i^2}\]

<p>Adding the first and second terms gives the result:</p>

\[=-\frac{1}{2}\left(k\log 2\pi+\sum_{i=1}^k\left[\frac{(x_i-\mu_i)^2}{\sigma_i^2}+2\log\sigma_i\right]\right)\]

</details>

<p>Take a look <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#stochastic-policies">here</a> for another take on this issue (I did!).</p>

<p>Okay, moving on.</p>

<h3 id="for-the-derivative">For the derivative:</h3>
<p>Instead, we just say hello to our new best friend, the autodiff function <d-footnote>For example, jax.grad()</d-footnote>. I’m going to run the code which evaluates \(\log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\) for an entire batch of states in one vectorized/parallelized operation, across whatever computational resources I have available. Can you please give me \(\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\)? And our new best friend is happy to oblige. Implementation details can be found in the next <a href="/blog/2023/VPGInJax">post</a>.</p>

<p>How does our new friend work? I’ll mostly defer to good explanations elsewhere, for example <a href="https://github.com/HIPS/autograd/blob/master/docs/tutorial.md#whats-going-on-under-the-hood">here</a> and <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">here</a>. The gist is that first, it keeps track of the computation graph of whatever functions you want to differentiate, from state \(s\) and parameters \(\theta\) all the way to \(\log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\). Then, it uses this graph to multiply its way through the chain rule, resulting in the gradient(s) you want. And of course, this is done in parallel for all the time steps in your trajectory.</p>

</details>

<p>\(\sum_{t=0}^{T}\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\) We sum this term over all time steps in our current trajectory. A <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient">brief derivation</a> results in this full term, the grad log of the current policy. This is the meat of VPG. This term is the gradient of the probability of the current trajectory with respect to the policy parameters, evaluated at the current parameters. For that reason, let’s call this the probability gradient. (\(\nabla_\theta P(\tau)\), where \(\tau\) is the current trajectory).</p>

<p>In a moment, we’re going to add the V_t term. Let’s review what we have so far without it: the probability gradient Grad_theta P(tau). Without the sum over time steps, this tells us how much the probability distributions of our actions change, given a small change in each of our neural network parameters. Re-adding the sum over time steps, it instead tells us how much the probability of that sequence of time steps (the trajectory) changes, again for a small change in our parameters. This is great: if we want to change the probability of a particular trajectory, we have the information to do that!</p>

<p>The main remaining question is this: do we want our current trajectory to happen more or less often? Well, I think we can agree, we want to make good trajectories happen more often, and bad trajectories happen less often. So how good (or bad) is our current trajectory?</p>

<h2 id="learning-from-rewards">Learning from rewards</h2>

<p>The return of a trajectory is defined as the discounted sum of rewards obtained in that trajectory. The higher the return, the better the trajectory, and vice-versa. See the RL post() for an intro to this concept. Following directly from the policy gradient discussion above: we now know not only how to change a trajectory’s probability, but also whether to make it more or less probable: each probability gradient in the sum is weighted by the return!</p>

<p>To be explicit: if an action corresponds to an above-average return, we use that information combined with the probability gradient at that time step to nudge the policy in the direction of the action (given the state it was acting on) which resulted in the future return from that time step. (And vice-versa). This summed over all time steps gives us the full policy gradient term.</p>

<p>Show the equation again, except with R_t instead of V_t</p>

<p>Let’s take a step back and think about this.</p>

<p>If I take an action, what parts of the trajectory will this affect? I’m not currently in possession of a time machine, so any action I take will not affect the past. &gt;Even if history is written by the victor, recorded events do not necessarily reflect what actually transpired.&lt; And of course if I take an action, that will carry me to the next time step, so I cannot affect the present either. I can only affect the future. Therefore, the part of the trajectory (and thus, the rewards) that any action has bearing on is the remainder of the trajectory from the next time step to the end of the episode. I’ll call the discounted sum of future rewards “future return” or R^f.</p>
<blockquote>
  <blockquote>
    <p>This set of rewards is sometimes known as the “rewards-to-go.” This term always struck me as kind of confusing, maybe because “to-go” isn’t as precise as I’d like. About half the time I see it, I think “to-go?’ Where are the rewards going? Oh, you mean to-go’ as in what we have to go, or remaining.” Let’s avoid any temporary confusion and use “future return.”</p>
  </blockquote>
</blockquote>

<p>Show the equation again, except with R^f_t instead.</p>

<p>Let’s be as clear as possible. Assume T time steps t in a trajectory, t in [0,T), or t in [0,T-1] if you prefer. The future return R^f_t for the first time step (t=0) in a trajectory, R^f_0, contains all the rewards in that trajectory. R^f_1 contains all rewards, except the first reward. R^f_T-1(the final time step) contains no rewards, because there’s no future! R^f_T-2 contains only the final reward.</p>

<p>Now, how do we actually calculate R^f_t for a particular time step? It seems obvious: simply take the discounted sum of all the future rewards! Well, it’s a bit trickier than that. The obvious obstacle to this is that you never have the future rewards for a time step as you’re playing out that step. I don’t have a time machine that goes forward, either. You only have immediate access to the rewards you don’t care about, those in the past. So you’re forced to keep “rolling out” the trajectory to its conclusion, recording all future time steps, and only then you can accurately calculate R^f_t for all time steps. »R^f_t is easy enough to calculate in one backward-in-time pass. Starting from the final time step, each R_t equals the reward of the current time step plus the sum of the rewards from the next time step.«</p>

<p>Whew. That seems a lot more tedious than we were hoping for. »If my episode is infinite, it’s technically impossible. I can get around the infinite sum by remembering I’m discounting, and truncate my sum at a reasonable time in the future. « I would like my algorithm to be “online,” meaning I would like to avoid having to collect full episodes before knowing what my future return is. If I could do that, I would be able to learn from the reward of every time step in real time, and update my policy accordingly.</p>

<p>Imagine if you were playing soccer for the first time, but the coach said you weren’t allowed to correct your mistakes while you were playing! No no, only in the time between full games can you reflect on the error of your ways, and think how to improve for next time. Until then, suck it up and keep tripping over your feet. Intuitively, this seems inefficient, if not patently ridiculous.</p>

<blockquote>
  <blockquote>
    <p>Now, modern “offline” algorithms aren’t actually this bad. They usually collect short sequences of transitions rather than full trajectories, and store them in a buffer to be sampled and asynchronously learned from. So the “reflection between games” is happening at all times, and “reflecting on all past games,” so to speak, rather than the most recent experience. In other words, in “online” algorithms the state space currently being learned from is the same that’s currently being explored by the active policy. However in “offline” algorithms the current policy (implicit or explicit) sets the explored state space, but the buffer of historical data encapsulates a potentially different state space. There are pros and cons to each. «</p>
  </blockquote>
</blockquote>

<p>The smaller the time lag in learning, the faster I can gain information about my new policy, and update it again. So, how to access the future return now instead of later? Approximate it with the value function V!</p>

<h2 id="the-value-function">The value function</h2>

<p>The value function’s (V_t) job is to approximate the return for a particular time step. This is a lot simpler than it may seem. The value function answers one question: how good is this state to be in? Or, given the state at time step t, what is the best approximation of the expected return? With a deep neural network, we can quickly dispatch this question with good old supervised learning. We have a trajectory of states at each time step, and at the end of an episode (or the effective time horizon defined by the discounting term gamma) we’re able to calculate the future return for each time step. All that follows is to state the loss function to minimize, and we’re off to the races. (Divide this by T)</p>

<p>And with that, here is the full policy gradient:<br />
(Math, no sum over trajectories).</p>

<p>A few caveats.</p>

<p>This is an approximation which may only see a subset of the state space at any given time, and it may never see the full state space. We cannot assume it is unbiased unless it has trained equally as much on data from all parts of the state space, and typically that will only be the case once the training is complete, if it happens at all. In parts of the state space which are relatively less explored, it may even be very inaccurate, which could lead to learning a falsely optimal policy.</p>

<p>One of the best solutions to this problem is seen in the RL algorithm (PPO)[paper]. »Its goal is to avoid veering too far out into unexplored territory too quickly. It allows the value function time to “acclimate” to its new surroundings and give more accurate estimates, and so the policy is always learning from a “good enough” value function. It accomplishes this by limiting the size of the policy update step by clipping a surrogate loss function - but I won’t digress too much on this point in this post, it requires more time to do it justice.«</p>

<p>It also turns out that this simple solution is quite high-variance in practice. Because of this, it’s difficult for the actor to learn a good policy and to do so quickly. Luckily there are many battle-hardened techniques for reducing variance.</p>

<p>Take a look at <a href="gae paper">generalized advantage estimation</a> (GAE) for lower-variance, lower bias estimates of how much to update policy parameters.</p>

<h2 id="generalized-advantage-estimation">Generalized Advantage Estimation</h2>
<p>What is the lowest bias approximation of the return for a state? Well, the return itself! That is, the actual sum of rewards that we calculate. “But hang on, I thought we were trying to avoid doing that to get quick feedback?” Indeed we are. So perhaps we can combine the two approaches. What if I use the actual reward in the next time step, and add it to the value function’s approximation of the future return from that point on? Aha, a slightly more accurate estimate!</p>

<p>Perhaps we can even do a little better by using the next two real rewards instead of only one. Or the next three? Each addition will require a slightly longer delay between taking an action and being able to learn from it. Now the question becomes: what’s the optimal tradeoff between accuracy and this learning delay? At this point you just need to experiment and see what works best for your problem, but I will tell you this: the answer is somewhere between 100% accuracy and zero delay (show the chart here). It also turns out you can do even better by doing a weighted average of all of these options: one, two, three, etc. real rewards, followed by an approximation, and also experimenting to see what the right weighting is. That’s GAE in a nutshell. The sum will of course be discounted as usual, so all but the first term have \gamma^t applied to them.</p>

<p>We’ve left out one important piece, though. This isn’t actually “Advantage” yet: we haven’t addressed how to lower the variance of our approximation. Okay. We have a more accurate approximation with our “immediate reward r_t plus delayed value function V(s_t+1),” as compared to the non-delayed value function V(s_t) alone. What is the source of the variance? Well, the approximation of the value function, of course. Since the value function at each time step is approximated by the same neural network, we should expect it to contribute similar “noise” or “variance” to our “signal” or “true value.” If we subtract “pure approximation” V(s_t) from “slightly more accurate approximation” r_t + V(s_t+1), we remove variance and are mostly left with the true value we care about: the advantage A_t, or how much better is this action than average? Putting all this together gives us GAE: a low variance estimator for the advantage.</p>

<p>I’ve focused on intuition here. If you’re interested in the math of how the weighted averaging and so on works out, go take a look at the paper; your mental picture should now be organized to have all those formalisms swiftly fall into place. Finally we can convert our \(V_t\) to \(A_t\).</p>

<h2 id="batched-training-of-neural-nets">Batched training of neural nets</h2>
<p>Finally, we sum this term over a batch of collected trajectories, dividing by the number of trajectories to get the sampled mean of the above (the grad log time-step summation). Why? Well, to gain a lower variance estimator of the true policy gradient. This should be quite familiar to you if you’ve ever done mini-batched stochastic gradient descent. Many trajectories averaged together will smooth out the noise from any one trajectory, and better represent the optimal policy gradient, or the step towards the true optimal policy.</p>

<p>Ok, finally, we have the sampled policy gradient: (show full eqn)</p>

<p>With this term, we can update the parameters of our policy, which in this case is an MLP.</p>

\[\theta_{k+1} = \theta_{k}+\alpha_k \hat{g}_k\]

<p>Refers to the updated parameters,  the parameters to be updated, and the learning rate (a hyper parameter).</p>

<p>This is the policy learning taken care of. Now we turn to the value function, doing the same sum over trajectories, for the same reason:</p>

<p>All we have to do is perform inference with the model at all time steps along the trajectory, for all collected trajectories, and we have all the ingredients we need to perform batched gradient descent of the model to minimize the loss function, as shown below.<br />
\(\theta_{k+1}=\arg \min_\phi \frac{1}{\left\vert  \mathcal{D}_k \right\vert T}\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\left(V_\phi(s_t)-\hat{R}_t  \right)^2\)<br />
With each gradient descent step, the value function, on average, better approximates the real sum of rewards. The policy then has a more accurate value function with which to adjust its own actions. Together they explore the state and action space until a quality policy is reached. Though every step is not guaranteed to improve due to approximation error, even this “vanilla” reinforcement learning algorithm can learn to complete simple tasks.</p>

<p>This previously scary-looking algorithm should now be more approachable. Go over it and see that everything lines up for you, then we can take a look at the code.</p>]]></content><author><name>Kenneth Jabon</name></author><category term="rl" /><summary type="html"><![CDATA[The theory behind one of our introductory algorithms]]></summary></entry><entry><title type="html">The Reinforcement Learning Problem</title><link href="https://kjabon.github.io/blog/2023/RL/" rel="alternate" type="text/html" title="The Reinforcement Learning Problem" /><published>2023-04-13T00:00:00-05:00</published><updated>2023-04-13T00:00:00-05:00</updated><id>https://kjabon.github.io/blog/2023/RL</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/RL/"><![CDATA[<h1 id="so-you-wanna-do-rl">So, you wanna do RL</h1>
<p>This post is the place to start. RL has successfully defeated grandmasters in Go, Dota 2, and is responsible for training ChatGPT. Here we will lay out the reinforcement learning problem, and in future posts I’ll lay out how various algorithms go about solving it.</p>

<div class="row">
<div class="col">
</div>
<div class="col-8">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/humanfeedbackjump.gif-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/humanfeedbackjump.gif-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/humanfeedbackjump.gif-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/humanfeedbackjump.gif" class="img-fluid rounded" width="auto" height="auto" title="Noodle" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
 <div class="col">
</div>
</div>

<div class="caption">
"Learn some deep reinforcement learning, and you too can train a noodle to do backflip" -Amid Fish
<d-footnote>Learning from human preferences, OpenAI. [https://openai.com/research/learning-from-human-preferences]</d-footnote>
</div>

<p>I briefly talk about policies in this post as an example of a solution, with no mention of TD- or Q-learning, which are equally important. For pedagogical/introductory purposes, policies as are slightly more intuitive and straightforward. Don’t let this scare you away from Q-learning, because it is powerful and eminently learnable! Now, without further ado, let’s jump right in. In the standard setup of the reinforcement learning problem, you have an actor and an environment interacting in a loop.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RLProblem-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RLProblem-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RLProblem-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/RLProblem.png" class="img-fluid rounded" width="auto" height="auto" title="RL Problem" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
Agents act in (or "send actions to") the environment. The environment progresses one time step based on this action, and responds with an observation and reward.
<d-footnote>Figure from Deepmind Acme paper on arXiv. [arXiv:2006.00979]</d-footnote>
</div>

<h3 id="from-the-environments-point-of-view">From the environment’s point of view…</h3>
<p>The environment is always in a well-defined state \(s\), and given some action received by the actor, it will transition to a new state \(s’\) corresponding to that action. When this transition happens, the environment will spit out a reward \(r\) for transitioning from \(s\) to \(s’\): \(r = R(s,a,s’)\). Eventually some special terminal state is reached. We reached our goal or irrevocably failed the task, and our episode ends. At the beginning of each new episode, the environment can be initialized to an initial state \(s_0\).</p>

<p>This environment is a Markov decision process (MDP): go read about those on page 47 of <a href="http://incompleteideas.net/book/the-book-2nd.html">Sutton and Barto</a>.</p>

<h3 id="from-the-actors-point-of-view">From the actor’s point of view…</h3>
<p>The actor will take an observation of the environment’s state, process this information, and output an action to the environment. It will then record the reward \(r\) output by the environment, and continue this loop. This reward is the signifier of “good” or “bad” results from the reinforcement learning actor’s actions.</p>

<p>Following this loop forms a trajectory \(\tau\) made up of the states \(s_t\), actions \(a_t\), and rewards \(r_t\) at each time step \(t\). \((s_0, a_0, r_1), (s_1,a_1,r_2), (s_2, a_2, r_3),…\)</p>

<details><summary>“That looks funny,” I hear you say.</summary>
<p>The reward time step is offset by one for a few reasons. We don’t get any reward for the first time step \(t=0\), i.e. for initializing the state \(s_0\). Why? We haven’t taken any action yet. Also, the reward \(r\) is associated with both state \(s\) and \(s’\), which live in separate time steps. We have to pick a formalism, so we assign the reward to the latter time step \(t+1\), because the environment returns it at the same time as the next state \(s’\). However, we group it with the former tuple in our trajectory \(\tau\), because it’s associated with the action \(a_t\) we took at that time step \(t\). This tuple organization follows an intuitive loop from the actor’s point of view: “observe, act, get a reward,” then repeat.</p>

<details><summary>“But wait,” I hear you say.</summary>
<p>It would make more sense to have a “null” reward \(r_0\) at the beginning which we spit out but don’t do anything with, and form our trajectory like so: <br />
\((r_0, s_0, a_0) (r_1, s_1, a_1),(r_2, s_2, a_2), …\)<br />
The subscripts look nicer, but this doesn’t really end the tuples at natural stopping points.</p>

<p>Often the trajectory is represented without the reward at all, subverting this issue entirely! (Although, this does beg the question of “where do I keep my rewards to learn from?”). Feel free to consider further, but try not to get hung up on this point. Ultimately, we’re representing the same thing, and sometime very soon you will abstract away the whole process. The following example should clear things up.</p>
</details>
</details>
<p><br /></p>

<h1 id="1d-grid-world">1D Grid World</h1>
<h2 id="a-simple-environment">A Simple Environment</h2>

<p>Let’s consider a 1D grid world, where the goal is simply for the actor to be as near as it can to a particular grid space, and x is the target grid space.</p>

\[\begin{array}{|l||c|c|c|c|c|c|c|} 
\hline
\text{x=4} &amp;  &amp; &amp; &amp; &amp; x &amp; &amp; \\ 
\hline  
\text{State} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;5&amp;6\\ 
\hline 
\end{array}\]

<p>Since our goal is to be near to the target space \(x=4\), let’s define a reward function: \(R(s’) = |4-s’| + 4\). Remember \(s’\) is the state we end up in after taking an action.</p>
<details><summary>You dropped this: \(s,a\)</summary>
<p>Well well, you’re a quick learner. Yes in general, the reward is a function of \((s,a,s')\). This simple example only depends on \(s'\).</p>

</details>
<details><summary>Why \(+ 4\)?</summary>
<p>I add the 4 to keep the numbers positive, i.e. a little cleaner, but offsetting the reward function makes no difference to the RL algorithm. I could add or subtract 10,000, and in principle it will still work, especially if you are standardizing the inputs to your neural network.</p>
</details>
<p>We see each grid space take on a reward following this function:</p>

\[\begin{array}{|l||c|c|c|c|c|c|c|} 
\hline
\text{x=4} &amp;  &amp; &amp; &amp; &amp; x &amp; &amp; \\ 
\hline  
\text{State} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;5&amp;6\\ 
\hline 
\text{Reward} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;3&amp;2\\ 
\hline 
\end{array}\]

<p>Now, our actor may start at a random location, but let’s suppose it starts at 0:</p>

\[\begin{array}{|l||c|c|c|c|c|c|c|} 
\hline
\text{x=4}  &amp; o &amp; &amp; &amp; &amp; x &amp; &amp; \\ 
\hline  
\text{State} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;5&amp;6\\ 
\hline 
\text{Reward} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;3&amp;2\\ 
\hline 
\end{array}\]

<p>where o is the location of our actor. Notice here the initialization of the environment state doesn’t spit out a reward. <d-footnote>In RL, we don’t get a reward just for showing up, we get rewards for participation!</d-footnote></p>

<p>Suppose we have 3 actions available to us at a given time step. We can move left, right, or stay put. Encode these actions as -1, 1, and 0 respectively. This environment follows a deterministic state transition, i.e. a left action will always move us left one grid space, and so on. If we bop into a wall, then we stay put.</p>

<p>When we transition to the new state, we obtain the associated reward: \(r = R(s’)\). The goal in the RL problem is defined as maximizing the “return,” or the sum of rewards \(r\) in a trajectory \(\tau\). If you prefer, the trajectory’s return is</p>

\[R(\tau)=\sum_{t=0}^{T}r_t\]

<h2 id="introducing-the-policy">Introducing the Policy</h2>
<p>The actor maintains a policy \(\pi(a\vert s)\). For a given time step \(t\) in the trajectory \(\tau\), this function outputs the probability distribution of all possible actions \(a_t\), given the state \(s_t\). We can see the optimal policy \(\pi(a\vert s)\) which achieves this goal immediately:</p>

\[\begin{array}{|l||c|c|c|c|c|c|c|} 
\hline
\text{x=4}  &amp;  &amp; &amp; &amp; &amp; x &amp; &amp; \\ 
\hline  
\text{State} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;5&amp;6\\ 
\hline 
\text{Reward} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;3&amp;2\\ 
\hline 
\text{Policy} &amp;1 &amp; 1 &amp;1&amp;1&amp;0&amp;-1&amp;-1&amp;\text{1, 0, -1 = right, stay, left}\\ 
\hline 
\end{array}\]

<p>That is, step towards the target, and if you’re on the target, stay put. At the risk of being obvious, let’s show the optimal trajectory following this policy for our actor starting at position 0. Remember a trajectory \(\tau\) follows the form \((s_0, a_0, r_1), (s_1,a_1,r_2), (s_2, a_2, r_3),…\)</p>

\[\begin{array}{|c|c|c|c|c|c|c|l|} 
\hline
 o &amp; &amp; &amp; &amp; x &amp; &amp; &amp;s_0=0, a_0=1 &amp;\text{ initial state; no reward}\\ 
\hline  
  &amp;o &amp; &amp; &amp; x &amp; &amp; &amp;r_1 = 1, s_1=1, a_1=1&amp;\text{ move right}\\ 
\hline  
  &amp; &amp;o &amp; &amp; x &amp; &amp; &amp;r_2 = 2, s_2=2, a_2=1&amp;\text{ move right}\\ 
\hline  
  &amp; &amp; &amp;o&amp; x &amp; &amp; &amp;r_3 = 3, s_3=3, a_3=1&amp;\text{ move right}\\ 
\hline  
 &amp; &amp; &amp; &amp; o &amp; &amp; &amp;r_4 = 4, s_4=4&amp;\text{ terminal state; no action}\\ 
\hline  
\end{array}\]

<p>At this point the actor receives the final reward and state, and notices it has reached the goal/terminal state. No further actions are taken and the episode ends. Our return, or sum of rewards, is</p>

\[R(\tau)=\sum_{t=0}^{T}r_t = r_1+r_2+r_3+r_4 = 10\]

<details><summary>Trajectory? Episode?</summary>
<p>For the purposes of this post, the trajectory is just the full episode. In general, a trajectory is any contiguous subsequence of an episode, while an episode is the full sequence from initial state \(s_0\) to terminal state \(s_{T-1}\) for an episode of length \(T\), if it ends at all.</p>

</details>

<details><summary>What if: bad grid spaces?</summary>
<p>We also could have put “pitfalls” at each end, such that the actor would receive a large negative reward, and the episode could end then, as well. Clearly an optimal policy would involve avoiding these “bad” spaces.</p>

</details>

<p>We’ll leave representing and learning the policy \(\pi\), which can be handled by all manner of RL algorithms, to <a href="/blog/2023/VPG">future posts</a> and external <a href="https://spinningup.openai.com/en/latest/user/algorithms.html">resources</a>. This post’s purpose is merely to lay out the problem to be solved.</p>

<p>Let’s make this picture more general so it can describe any environment interaction. <br />
<br /></p>
<h1 id="extending-the-simple-picture">Extending the Simple Picture</h1>

<p>By the way, if this gets to be a bit much, there is a handy picture of an MDP at the bottom. May I suggest opening it in a new tab or window for reference?</p>

<h2 id="imperfect-information">Imperfect information</h2>
<p>Earlier I said the actor “takes an observation” rather than “records the state.” This is because in general, the observation \(o\) recorded by the actor may be an imperfect representation of the well-defined environment state \(s\).</p>

<p>Suppose our actor is Paul Revere, and he is deciding whether to hang one or two lanterns at the Old North Church (action \(a\): 0, 1, or 2, encoding a signal to the militia: “don’t know”, “by land” and “by sea” respectively). There is an advancing British force coming in ships off the coast (state \(s\): 15,000 troops coming by sea).</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/revereride-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/revereride-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/revereride-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/revereride.jpg" class="img-fluid rounded" width="auto" height="auto" title="American dog is fully prepared to fight invaders, 1774." onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">The British are coming! Maybe.</figcaption>

</figure>

<p>However, Mr. Revere can only see a boat or two off the coast, and similarly a few carriages shuttling around on land. The British force is concealed by the fog and dark of night <d-footnote>Or worse, a sepia tone</d-footnote>. His observation \(o\) is an imperfect representation of the environment state \(s\) (observation \(o\): ~ 0, or maybe -10 (a few more people on land) or 10 (a few more at sea)).</p>

<h2 id="infinitely-long-episodes">Infinitely long episodes</h2>
<p>Next, what if our loop has no foreseeable end? In general this will be the case. Some environments go on forever, and there is nothing in our MDP picture which prevents that from happening.</p>

<p>Suppose our actor is a bipedal robot. Its task is to push an AWS server to the top of Mount Everest, because there are excellent ambient temperatures for computing up there. Unfortunately for the robot, every time it gets near the top, its servos freeze over, it loses control of the server rack, and it rolls all the way back to the bottom. And so it will try again until the end of time, or at least the end of its Amazonian overlords. All is well for the robot, who has no shortage of energy or enthusiasm.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Mount-Everest.webp-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Mount-Everest.webp-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Mount-Everest.webp-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Mount-Everest.webp" class="img-fluid rounded" width="auto" height="auto" title="Stay frosty" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">You got this, chief</figcaption>

</figure>

<p>How do we support infinite episodes? We simply mandate that every state \(s\) must accept a set of valid actions, and that such actions result in a state transition \((s,s')\). There is nowhere to “end,” and the MDP goes on forever.</p>

<p>Does this picture still support finite episodes? Notice that a state \(s\) can transition to itself \((s’ = s)\). To mark a state as terminal, we only allow it to transition to itself. Technically this is still an infinite MDP: our picture hasn’t changed, it will transition to itself forever. But if we reach a particular state or set of states, we can decide to stop traversing and end the infinite episode prematurely.</p>

<h2 id="discounted-rewards">Discounted rewards</h2>
<p>Now, let me ask you a question. You’ve won a million dollars. Congrats. Would you like your prize now, or in 30 years? Everyone can agree on the answer. If you have it now, you can improve your life, or others lives, now. If you’re worried about self control, stick it in a trust and only touch the dividends. What use is there in waiting?</p>

<p>In other words, reward now is better than reward later, else you’re just wasting time. What’s more, remember our trajectory is infinitely long in general. Ultimately we need to do calculations with the sum to learn from it, and we can’t do that with infinite numbers. To handle this, our actor’s more generalized goal is to maximize the <em>discounted</em> sum of rewards.</p>

\[R(\tau)=\sum_{t=0}^{T}\gamma^tr_t\]

<p>\(\gamma\) will usually be set to some number very close to 1, like 0.98. We add the discounting term so that our return converges to some finite value. We can see that as the time step gets further into the future, the discounting factor \(\gamma\) will make reward term decay to 0, and assuming no one reward is infinite, the sum will never be infinity.</p>
<details><summary>We have to go back…</summary>
<p>Notice we can recover our original un-discounted picture simply by setting \(\gamma=1\).</p>

</details>

<h2 id="probabilistic-state-transitions">Probabilistic state transitions</h2>
<p>Our robot from earlier is halfway up the mountain and tries to push forward one more step, but randomly a gust of wind causes him to lose his grip on the AWS server, rolling back down.</p>

<p>What if a particular action \(a\) from a particular observation \(o\) doesn’t always result in the same state transition \((s,s')\)? This is supported by probabilistic state transitions in the MDP. A definite action is taken, then state \(s\) will transition to \(s’\), but \(s’=0\) (the bottom of the mountain) with 5% probability, and \(s’=s+1\) the rest of the time.</p>

<p>In a simulated environment this can be represented by a transition function \(p(s’\vert s, a)\), with a vector of probabilities for all reachable \(s’\) from \(s\), for a particular action \(a\).</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Markov_Decision_Process.svg-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Markov_Decision_Process.svg-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Markov_Decision_Process.svg-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Markov_Decision_Process.svg.png" class="img-fluid rounded" width="auto" height="auto" title="MDP" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
An example of the full MDP. This encapsulates states (green nodes), actions (red nodes), rewards upon state transition (emitted squiggles), and nondeterministic transitions (arrows from red nodes).
<d-footnote>From the wikipedia page for MDPs.</d-footnote>
</div>

<p>That’s our MDP picture and the RL problem; not so bad, is it? Of course, we haven’t done any learning yet! See the next post on <a href="/blog/2023/VPG">VPG</a> for how to learn from interacting with the environment.</p>

<hr />
<p>My understanding of the subject comes from David Silver’s excellent <a href="https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver">lectures</a>, Open AI’s <a href="https://spinningup.openai.com/en/latest/">spinning up</a>, the 2017 Berkeley <a href="https://sites.google.com/view/deep-rl-bootcamp/home">Deep RL Bootcamp</a>, Pieter Abbeel’s and Sergey Levine’s various lectures on YouTube, and Sutton and Barto’s “<a href="http://incompleteideas.net/book/the-book-2nd.html">Reinforcement Learning</a>” (which was referenced by the others). These are excellent resources, and I recommend you check them out.</p>]]></content><author><name>Kenneth Jabon</name></author><category term="rl" /><summary type="html"><![CDATA[Before anything else, define the problem you need to solve.]]></summary></entry><entry><title type="html">Some Acme Speedbumps</title><link href="https://kjabon.github.io/blog/2023/AcmeIssues/" rel="alternate" type="text/html" title="Some Acme Speedbumps" /><published>2023-03-22T00:00:00-05:00</published><updated>2023-03-22T00:00:00-05:00</updated><id>https://kjabon.github.io/blog/2023/AcmeIssues</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/AcmeIssues/"><![CDATA[<p>Check out the companion <a href="https://github.com/kjabon/AcmeGPUHelper">github repo</a>.</p>

<h2 id="make-acme-work-for-you">Make Acme work for you!</h2>

<p>Parallelizing training of RL agents in Acme required a deeper understanding of the inner workings of Launchpad, DeepMind’s framework for distributed platforms, and the intricacies of checkpointing, which after deep diving used quite a bit of code from Tensorflow, took some dives into the code/documentation rabbit hole. This should be enough of a tipoff for you to figure this stuff out yourself. I’ll leave the details to the enterprising reader, but there are some things that were particularly obfuscated/necessary to progress.</p>

<h2 id="acme-uses-tensorflow">Acme uses Tensorflow!</h2>

<p>I didn’t understand at the outset that so much code from Tensorflow was allocating GPU memory behind the scenes, at the same time as the JAX code, in different processes, which is largely the result of things not being rewritten in JAX in Acme (no judgment here). Resolving cryptic errors, most of which ended up being these same GPU memory allocation issues, took a significant amount of rabbit hole digging and understanding the environment variables related to memory allocation for each library. Eventually I created a “GPU startup” file which solves all these issues without too much thought, which I have ported over to other projects. See <a href="https://github.com/kjabon/AcmeGPUHelper/blob/main/gpu.py">here</a> for the code.</p>

<h2 id="hindsight-buy-homogenous-set-of-gpus">Hindsight: Buy Homogenous set of GPUs</h2>
<p>Furthermore, JAX specifically does not allow mapping of work to heterogeneous device arrays (different GPUs), which is quite a shame in my opinion, as I currently have a 3080 and a 3080 Ti, which must instead be used for different sets of tasks. The JAX authors (Google) are likely used to using hundreds or thousands of server GPUs of the same type, not to mention TPUs, so I’m not holding my breath for a fix.</p>

<hr />

<p>Check out more <a href="/blog/">blog</a> postings on my RL projects!</p>]]></content><author><name>Kenneth Jabon</name></author><category term="rl" /><summary type="html"><![CDATA[Messing around with parallelization.]]></summary></entry><entry><title type="html">CoachRL Back End</title><link href="https://kjabon.github.io/blog/2023/CoachRLDetails/" rel="alternate" type="text/html" title="CoachRL Back End" /><published>2023-03-14T00:00:00-05:00</published><updated>2023-03-14T00:00:00-05:00</updated><id>https://kjabon.github.io/blog/2023/CoachRLDetails</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/CoachRLDetails/"><![CDATA[<p>See previous posts for <a href="/blog/2023/distill/">discussion of habits and rewards</a>, and the <a href="/blog/2023/CoachRLHighLevel/">daily use of CoachRL</a>. This post covers the technical details which power the project. <a href="https://github.com/kjabon/coachRL">See the GitHub Repo.</a></p>

<h2 id="outline">Outline</h2>

<p>Someday, this may become neatly contained in an iPhone app. For now, it is composed of several programs stitched together. First I’ll lay out these components, and touch on RL frameworks. I’ll talk about data pipelines, then I’ll cover the usage of the trained model to output daily suggestions. Finally, we train the model in simulation before introducing it to the real world.</p>

<h2 id="ingredients">Ingredients</h2>

<ul>
  <li>A Mac/Linux device. (Underlying libraries used do not currently run on Windows, though this could be changed).</li>
  <li>A device on which you can install the messaging app Telegram; used for notifications and requesting rewards for completing tasks.</li>
</ul>

<p>Optional but recommended:</p>
<ul>
  <li>A tablet, for a daily to-do list without rewriting your routine every day. I use an iPad with GoodNotes.</li>
  <li>An iPhone for auto-weight tracking.</li>
</ul>

<p>Software used:</p>

<ul>
  <li>A Renpho Bluetooth scale for weigh-in, with the associated iPhone app.</li>
  <li>iCloud to transfer weight data to a local csv file.</li>
  <li>Google sheets for an interactive log of daily habits, and running averages.</li>
  <li>Python, and many libraries, most importantly Acme from Deepmind, which uses a JAX implementation of PPO. In the future I plan on switching to MPO/MuZero for sample efficiency and performance. Also, libraries for communicating with iCloud and Google sheets. CoachRL shoots daily statistics and provides rewards via a Telegram interface, handled in Python with their bot API. (Any questions on this point: please leave a comment below).</li>
</ul>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/acme-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/acme-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/acme-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/acme.png" class="img-fluid rounded " width="auto" height="auto" title="Use JAX" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Use Acme</figcaption>

</figure>

<h2 id="rl-frameworks">RL Frameworks</h2>

<p>This project originated in PyTorch, using Stable Baselines 3. However, scaling actors used a lot of overhead, killing the benefit of parallelizing. Furthermore, extending SB3 was not very intuitive, which limited its usefulness for more advanced use-cases. I cast about for good libraries for parallelization in RL, and RLLib seemed to be a popular choice.</p>

<p>Before settling on this, I read Google is shifting to JAX from Tensorflow for its internal deep learning use cases. If we assume Tensorflow to be the industry standard for production deep learning, and its creators have come up with something more performant, then I’m not going to argue.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/jax.svg-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/jax.svg-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/jax.svg-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/jax.svg" class="img-fluid rounded " width="auto" height="auto" title="Use JAX" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
"JAX has the potential to be orders of magnitude faster than NumPy <br />(n.b. JAX is using TPU and NumPy is using CPU in order to highlight that JAX's speed ceiling...)." <d-footnote>Figure and caption from blog post by Ryan O'Connor at AssemblyAI. [https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2023/]</d-footnote>

</div>

<p>A recent <a href="https://arxiv.org/abs/2208.07860">paper</a> out of <a href="https://sites.google.com/berkeley.edu/walk-in-the-park">Berkeley</a> additionally saw a 16x speedup using JAX over PyTorch, enabling fast (20 minutes!) in-the-wild robot locomotion learning - something previously thought to be impossible due to sample efficiency. Read more about why JAX rules <a href="https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2023/">here</a>. I found <a href="https://github.com/deepmind/acme">Acme</a>, geared towards JAX, while using Tensorflow, to be the most comprehensive, maintained, and extendable framework.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/YO1USfn6sHY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
<div class="caption">
Robot locomotion, learned in-the-wild in less than 20 minutes with JAX.
</div>

<h2 id="data-pipelines">Data Pipelines</h2>
<p>What data are we processing, and what observation does the RL algorithm receive? Every day, you perform your habits to a certain level. For example, perhaps I go for a 30-minute run and practice violin for 15 minutes. I would manually input these values into the Google spreadsheet.</p>

<p>The spreadsheet maintains a daily record of the <a href="https://en.wikipedia.org/wiki/Exponential_smoothing">exponential moving average</a> (EMA) of the habit performance, normalized to your goal (see below figure in blue). Suppose my goal is to run 30 minutes a day, 5 days a week. If I don’t run at all, I would input a 0, and if this goes on for long, the EMA would eventually go to 0 as well. If I ran today for 30 minutes, I would input a 1, which would be normalized to 1*7(days in a week)/5 (days to run per week). 7/5 would be used as today’s update to the ongoing EMA, which will eventually converge to 1 if interspersed with two 0’s per week. The exact values and normalizations can be changed to suit your needs. Inputting 30 for a 30 minute run would work, so long as you’re normalizing it to 1 in the EMA calculation. (1 is assumed to be the optimal value for all goals). This is repeated for every daily habit.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/habitSheet-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/habitSheet-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/habitSheet-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/habitSheet.jpg" class="img-fluid rounded " width="auto" height="auto" title="Use JAX" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Values circled in red are the manually entered habits for the day. EMA in blue. Let's avoid the 'manual' part.</figcaption>

</figure>

<p>Now, if that sounds tedious, I agree! Having to manually input your habits every day, planning exactly what to do to meet your goals, etc. is a right pain. What if we could manage our habits without having to manually input anything? That is one problem we are here to solve.</p>

<h2 id="other-data-collection">Other Data Collection</h2>
<p>If one’s goal is to “be healthy,” one may more concretely define this as maintaining a certain weight, body fat percentage, resting heart rate, or all manner of health-related data. While this section can apply to any manner of metrics (perhaps not even health-related), I’m going to stick with weight.</p>

<p>Now, you’re going to have to actually weigh yourself and punch that in to the spreadsheet. No getting around it. Since our goal is to avoid tedium and pain, let’s be slightly smarter about this. I step on a scale every day, which connects via Bluetooth to my iPhone. An automation app on the phone saves this health data once a day to iCloud, which CoachRL can then access and stick in the spreadsheet for you. All that is required of me is grabbing my phone and stepping on the scale. The enterprising user can consider other ways to automatically collect data that is important to them.</p>
<h2 id="how-to-avoid-manual-habit-entry">How to avoid manual habit entry?</h2>
<p>Get a reinforcement learning algorithm to do it for us. All the pieces required to do this have been explained. Now, let’s put them together.</p>

<p>As you may recall, a reinforcement learning algorithm progresses through a trajectory of time steps. Each time step is composed of an observation, an action, an observation following that action, and a reward associated with the transition. Read <a href="http://www.incompleteideas.net/book/the-book-2nd.html">Sutton and Barto</a> for more details.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RLProblem-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RLProblem-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RLProblem-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/RLProblem.png" class="img-fluid rounded" width="auto" height="auto" title="RL Problem" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
Agents act in (or "send actions to") the environment. The environment progresses one time step based on this action, and responds with an observation and reward.
<d-footnote>Figure from Deepmind Acme paper on arXiv. [arXiv:2006.00979]</d-footnote>
</div>

<p>The observation is a vector of yesterday’s EMAs: one entry for each habit. Depending on the EMA, we will want to take a different action. What if we are falling behind on our running schedule?</p>

<p>The action taken by the actor is today’s planned level of performance for every tracked habit, based on yesterday’s EMA. If we’re behind our running schedule, run today!</p>

<p>The reward is simply 1-mean(abs(1-observation)): see below figure. If all the EMA values yesterday are at 1 (perfectly aligned with the goals of all habits), then the reward is 1. Any deviation will cause the reward to linearly decrease from this max value. We add 1 to make rewards (usually) lie between 0 and 1, just to keep it intuitive.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rewards-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rewards-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rewards-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/rewards.jpg" class="img-fluid rounded " width="auto" height="auto" title="RL Problem" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Environment rewards as a function of EMA, averaged across habits.</figcaption>

</figure>

<p>Beyond technical details, this is it! The RL actor performs inference on new observations, probabilistically selects actions for each habit, which is then filled in to the Google spreadsheet via the Google Docs API in Python.</p>

<p>Now, if your actions for any reason disagree with what the RL actor spits out, simply edit those values in the spreadsheet. If they consistently deviate, a little massaging of the output before it is filled into the spreadsheet is in order. For instance, if you are “coached” to run, work out, stretch, and play basketball in the same day, you may think this is not realistic for your schedule. In this particular case, I (in Python) randomly select one of those marked active, and set the rest to 0. Any massaging is also implemented when training the model.</p>

<h2 id="model-and-environment-setup">Model and Environment Setup</h2>
<p>Most of my time was spent creating an environment, grokking iCloud and Google Sheets APIs, and tweaking how actions were processed as I learned about the algorithm’s behavior. A simple N-day rolling average of habit performance was replaced by an exponential moving average halfway through the project. This hugely improved previous erratic behavior: instead of a number dropping out of the dataset entirely once per day, the EMA smoothed this effect out over time.</p>

<p>I did not have a good intuition for how large a network may be required, how many samples, what hyper parameters, etc., were necessary for optimal performance. More often than not, when I tackled strange behavior in the training with creative feature engineering, it ended in more confusion and was ultimately retconned in favor of splitting into multiple actors for the huge action space (10^15 possible combinations for 23 habits with varying action space sizes), one actor for every 5 habits or so.</p>

<p>Finally, the PPO implementation in Acme did not support MultiDiscrete action spaces. That is, 3 possible actions for habit A, 5 possible actions for habit B, and so on. To fix this, given neural network. Taking inspiration from the Stable Baselines codebase, I wrote a similar implementation in JAX, a sample of which is below. <a href="https://github.com/kjabon/coachRL">See the GitHub Repo.</a></p>

<d-code block="" language="python">
  # Compute log probabilities
  def log_prob(params, actions):
    logProb = jnp.stack([dist.log_prob(action) for dist, action in
      zip(getDistribution(params), jaxUnstack(actions, axis=1))], axis=1).sum(axis=1)
    return logProb
  
  # Sample actions
  def sample(params, key: networksLib.PRNGKey):
    samp = jnp.stack([dist.sample(seed=key) for dist in getDistribution(params)], axis=1)
    return samp
</d-code>

<div class="caption">
JAX/Python code for interfacing with neural networks to compute the log probabilities for use in policy gradient, and sampling from the Multi-Categorical probability distributions given a stack of parameters for those distributions.
</div>

<h2 id="simulation-training">Simulation Training</h2>
<p>Before thinking about training in simulation, I made an environment for reading EMAs from Google sheets, inputting this observation into the model, massaging the output a little, and writing the result back into Google sheets. All that was needed to simulate this process was to replace the calls to the Google Docs API with reading and writing from a queue whose elements represented all actions and EMAs for the day, and calculating said EMAs in Python instead.</p>

<h2 id="future-work">Future work</h2>
<p>Why bother with the RL model? Well, you may not know the optimal arrangement of your habits every day, or you might not want to slog through manually crafting hard-coded rules. Furthermore, perhaps there are long-term correlations between habits. Perhaps exercise gives you more energy long-term, causing you to get your work done in less time, leaving room for other habits. In the long run, training the model on real life data from the individual will give the best results. This was the main motivation for using RL in the first place.</p>

<p>However, sample efficiency prevents PPO from accomplishing these tasks. 1-5 million samples may be alright in a simulator with multiple actors in parallel, but most people don’t have time to wait that many days to get a good recommendation. This value may be lower when fine tuning a model from simulation on real life data, but is still substantially too large.</p>

<p>The solution is to use a more sample efficient algorithm. For various projects with costly actions (in particular, <a href="/blog/2023/PRL/">PRL</a>), I reviewed the literature in late 2022 and experimented with different RL algorithms to gain optimal sample efficiency. The two that rose to the top in this regard were MPO and MuZero (and follow-up papers). MuZero in particular has stellar performance in the discrete domain, because of its usage of Monte Carlo Tree Search, and stellar sample efficiency largely because it is able to “<a href="https://www.furidamu.org/blog/2020/12/22/muzero-intuition/">Reanalyse</a>” the data.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/reanalyse.webp-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/reanalyse.webp-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/reanalyse.webp-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/reanalyse.webp" class="img-fluid rounded " width="auto" height="auto" title="EfficientZero" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
Training loop of MuZero Reanalyse. <d-footnote>From a MuZero blog post by the paper's author. [https://www.furidamu.org/blog/2020/12/22/muzero-intuition/]</d-footnote>
</div>

<p><a href="https://arxiv.org/abs/2111.00210">EfficientZero</a> takes this sample efficiency further. (See my implementation and explanation of EfficientZero in blog posts to come!) As one can imagine, this comes at the cost of computational efficiency. However, since the time between steps for our environment is a full day, this is not a dealbreaker for us. While I have moved on to other projects in the meantime, I plan on updating this to use one of these more sample-efficient algorithms to take advantage of this long term “cross pollination” of habit effects. I hope this will result in a more holistic recommendation policy.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/efficientZero-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/efficientZero-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/efficientZero-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/efficientZero.png" class="img-fluid rounded " width="auto" height="auto" title="EfficientZero" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
The gains in sample efficiency in EfficientZero<d-footnote>https://arxiv.org/abs/2111.00210, Figure 1</d-footnote> make one hopeful for the future of RL in the real world.
</div>

<p>Finally, something I found necessary to gain optimal performance for every habit recommendation was to split habits into groups of 5 or so. My use case consists of 23 habits, each with multiple possible actions (up to 20). This is an extremely large action space! This splitting is less than ideal, because it requires more manual work, but particularly because you lose some of the possibility of getting aforementioned “cross-pollination.” It is possible I did not have the patience/compute power to make give optimal recommendations with a single network, or perhaps I simply wasn’t using a large enough network for the task. More reading and experimenting required.</p>

<hr />

<p>Thank you for reading; suggestions are welcome in the comments below! This article was already pretty long, so I skipped over many details; please feel free to ask! <a href="https://github.com/kjabon/coachRL">See the GitHub Repo.</a></p>

<p>See this <a href="/blog/2023/AcmeIssues/">short post</a> for other Acme issues I resolved surrounding multiple GPUs and parallelization.</p>]]></content><author><name>Kenneth Jabon</name></author><category term="habits" /><category term="rl" /><category term="coachrl" /><summary type="html"><![CDATA[Digging in to dirty details]]></summary></entry><entry><title type="html">Organizing Your Day with CoachRL</title><link href="https://kjabon.github.io/blog/2023/CoachRLHighLevel/" rel="alternate" type="text/html" title="Organizing Your Day with CoachRL" /><published>2023-02-10T00:00:00-06:00</published><updated>2023-02-10T00:00:00-06:00</updated><id>https://kjabon.github.io/blog/2023/CoachRLHighLevel</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/CoachRLHighLevel/"><![CDATA[<h2 id="to-summarize-the-previous-post">To Summarize the Previous <a href="/blog/2023/distill/">Post</a></h2>
<p>Suppose you want to form a lot of habits and do it quickly. You need to get organized. You need to track everything, you need to reward yourself for accomplishing tasks/habits, and you need to do all of this in a well-balanced, proportional way.<br />
But that’s hard.</p>

<h2 id="enter--coachrl">Enter- CoachRL</h2>

<p>No more tedium of tracking and decision-making. Automate everything to oblivion! Just a “personal coach” that tells you what to do and gives you a treat for doing it. <a href="https://github.com/kjabon/coachRL">See the GitHub Repo.</a></p>

<p>So what does a typical day look like?</p>

<p>First thing over coffee, you open your spreadsheet on Google Docs to find out what you’re going to be doing today. The row for today has already been filled in, based on the moving average of your “performance” for each habit.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/habitSheet-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/habitSheet-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/habitSheet-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/habitSheet.jpg" class="img-fluid rounded " width="auto" height="auto" title="example" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
In red: today's example activities, suggested by CoachRL. In blue: associated moving average of habit performances.
</div>

<p>Suppose you’re trying to work on a personal project for an hour a day. You were a bit overzealous last weekend and worked on it all weekend, sending your average over the one hour mark. The program will probabilistically output actions from a predefined, categorical distribution, so that over the next few days, your average will come down back to the goal. (You can see each habit can output whatever set of values makes sense - like 1-3 hours worked, or 0-4 “chunks of 30 minutes” practicing piano).<br />
If you differ from this suggestion, at the end of the day you should input the actual behavior, so tomorrow has accurate data for its suggestions.</p>

<h2 id="automation-of-fiddling-and-edits">Automation of Fiddling and Edits</h2>

<p>Typically I like to review my suggestions up front to get a handle on what I’ll be doing today, and make sure they make sense in aggregate. (Over time as I’ve smoothed out the bugs, I simply trust the output is correct and carry on with the following step.) E.g., I’d rather not run, go to the gym, play basketball, and do yoga all in one day - one exercise a day, at most, please! If I notice I’m making this kind of correction a lot, I’ll add it to the code which fills out the row with some simple logic. This further reduces my daily work and mental load. Ahh, the power of automation.</p>

<p>Now that I have today’s numbers in front of me, I open my note taking app on my iPad, and copy my daily schedule template over. I erase the things that have no bearing on today, alter today’s work time, and rearrange things according to decreasing difficulty (see <a href="https://todoist.com/productivity-methods/eat-the-frog">eating the frog</a>), and usually put my exercise in the middle of the day, just before lunch - the perfect midday break from my work day. The template is pre-organized so all of this takes just a minute or two with minimal fiddling. You can see a truncated example of this below. I also like to space out my work with errands/habits, so I’m not forced to work on something for 4 hours straight and get nowhere out of fatigue - but I leave myself the option to keep working indefinitely if I’m in the zone, simply crossing off future work items.</p>

<div class="row">
<div class="col">
</div>
<div class="col-8">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ipadTemplate-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ipadTemplate-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ipadTemplate-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/ipadTemplate.jpg" class="img-fluid rounded " width="auto" height="auto" title="example" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
 <div class="col">
</div>
</div>
<div class="caption">
Example daily template for a to-do list. Please pardon the chicken-scratch.
</div>

<p>Presto, you’re ready to start your day! Perhaps you have a morning routine involving water, spritz of lemon, and a cold shower. Then you can dig straight into your daily habits as the opportunity arises. Time before or after work? Go for a run or plug away at your novel.</p>

<p>Finally, for each item you complete - say 30 minutes of work, practicing piano for 30 minutes -  you request a reward from Telegram, which will tell you what you’ve earned for your hard work. A picture of this interface is in the figure below. Rewards are key, and keeps you trucking through the day. <br />
Pick a reward you can encapsulate and treat yourself with at will, that you find intrinsically rewarding. A YouTube jaunt, good book, or gaming session will do. Enjoy your spoils, and repeat! Add more habits as you feel you can handle the extra load. And that’s all there is to it!</p>

<p>Find the code for the Telegram rewards <a href="https://github.com/kjabon/rewardGenerator">here</a>: it’s very simple, and you should modify it to suit your needs.</p>

<div class="row">
<div class="col">
</div>
<div class="col-8">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/telegramCoach-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/telegramCoach-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/telegramCoach-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/telegramCoach.jpg" class="img-fluid rounded " width="auto" height="auto" title="example" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
 <div class="col">
</div>
</div>
<div class="caption">
Telegram interface for getting rewards from CoachRL.
</div>

<hr />

<p>See more for how and why to tune rewards, and snowballing good behaviors, in this previous blog <a href="/blog/2023/distill/">post</a>.</p>

<p>For the technical details of how the backend works, see the next <a href="/blog/2023/CoachRLDetails/">post</a>.</p>

<p>If there’s interest (via Github stars or comments below), I can make a post about setting it up for yourself. <a href="https://github.com/kjabon/coachRL">See the GitHub Repo.</a></p>]]></content><author><name>Kenneth Jabon</name></author><category term="habits" /><category term="rl" /><category term="coachrl" /><summary type="html"><![CDATA[Using the project, automate away the tedium of organizing your efforts.]]></summary></entry><entry><title type="html">Habits and Rewards</title><link href="https://kjabon.github.io/blog/2023/distill/" rel="alternate" type="text/html" title="Habits and Rewards" /><published>2023-01-16T00:00:00-06:00</published><updated>2023-01-16T00:00:00-06:00</updated><id>https://kjabon.github.io/blog/2023/distill</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/distill/"><![CDATA[<h2 id="why-habits">Why habits?</h2>

<p>Stephen King is one of the most prolific writers of our time. How has he managed to write more than 60 novels, and sell more than 400 million copies? Consistent and sustainable effort. As he recounted in “On Writing,” the vast majority of his books weren’t the result of 16-hour sprints. He follows one rule: to write for 4 hours a day, every day, no exceptions, like a stream carving out a canyon through hard rock.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/onWriting-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/onWriting-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/onWriting-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/onWriting.jpg" class="img-fluid rounded " width="auto" height="auto" title="example" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
    I highly recommend Stephen King's "On Writing." The lessons within are not writing-exclusive!
</div>

<p>You likely agree that exercise, healthy eating, a consistent sleep schedule, and reading every day are all things we should be doing, and probably benefit the rest of our activities indirectly.</p>

<h2 id="i-hear-you-counterpoint-tv">I hear you. Counterpoint: TV</h2>

<p>Waking up an hour early to get a run in? Gross. It’s raining. How about some hot chocolate instead? Now add a fuzzy blanket and some X-Files.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/kronk.gif-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/kronk.gif-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/kronk.gif-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/kronk.gif" class="img-fluid rounded " width="auto" height="auto" title="example" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>If only we had someone around to say hey man, I know it’s raining outside, and you’re not even sure you want to get into this whole cardio thing. But I’m gonna make it worth your while. Go do that run for me, and you can watch as much X-Files as you want.</p>

<p>Ok, sounds pretty good. But what if I’m a good little optimizer, and I want to form as many habits as possible in as short a time as possible, without burning myself out and falling off the wagon of every last thing in an endless loop? And what if I don’t want to run every day: I just want to run two days a week, and go to the gym the other two, and maybe do hot yoga the other two? It sucks keeping track of all this stuff every day! And another thing! If I’m being rewarded with endless X-Files, how am I going to have time for anything beyond exercise and X-Files?</p>

<p>Basically, use rewards (read on), and get organized <a href="/blog/2023/CoachRLHighLevel/">(next post)</a>.</p>

<h2 id="use-rewards">Use rewards</h2>

<p>Now for the juicy part - and I mean that literally in the context of rewarding brain chemicals. For your typical person, running or writing a novel is not intrinsically rewarding, or at least it is less intrinsically rewarding than playing Skyrim.<br />
Until it becomes an automatic habit, you must reward yourself for doing it! This may sound a lot like <a href="https://en.wikipedia.org/wiki/Operant_conditioning">operant conditioning!</a> This can be thought of very similarly to reinforcement learning. Put it to work for yourself and change your life.</p>

<p>Suppose you don’t do this. Your determination and willpower will take a day (or week) off, and sure - maybe you’ll get back on the bus, sooner or later. But relying on your own grit and available mind space is going to take longer, at best. It will leave you with less daily energy to apply to other things! Your work, hobbies, relationships, other habits - everything will suffer just a little bit. At worst, you’ll find yourself in an endless cycle of trying to start running; things go well for a week, and then you burn yourself out with your enthusiasm, and then start all over again.</p>

<h2 id="rewarding-good-behaviors">Rewarding good behaviors</h2>
<p>So, what’s the mechanism we use here?</p>

<p>First, you have to come up with a set of rewards for yourself. What’s your favorite thing to do? This can range from eating sugary treats, playing video games, to reading manga, to watching TV, to reading books, to reading about your favorite hobby, to playing a musical instrument, to cooking, and even to things like running and going to the gym, if that’s a habit you’ve already formed. Whatever it happens to be, set those things aside as rewards.</p>
<div class="row">

<div class="col-sm">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mineral-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mineral-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mineral-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mineral.png" class="img-fluid rounded " width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>
 
</div>
</div>
<div class="caption">
    Find the mineral you crave, and use it as a reward.
</div>

<p>Now, every time you clear an item off of your daily to-do list, you reward yourself (more details on automating this process in later posts). These rewards start out small (1-5 minutes). As previously mentioned, you don’t want to spend your whole day watching TV as a nice reward for flossing in the morning.</p>

<p>As more items are completed, the rewards grow. As your willpower is depleted throughout the day, you’ll find yourself making excuses not to tackle the next item on your list. Rewards grow throughout the day to counteract this effect, and keep you forming your habits for longer.</p>

<h2 id="individual-caveats">Individual caveats</h2>

<p>It is important to recognize that every individual has limits, which vary from person to person. Perhaps your work is particularly demanding, or your health has gotten away from you. Taking this into account, you should increase the amount of rewards you get for completing your tasks, and/or decrease the amount you’re trying to accomplish. If you stick to it, habits will become more automatic, requiring less mental energy, and the amount of reward you need to keep it up will decrease over time. Now you have extra energy for the day. If you find yourself at the end of the day with plenty of extra energy, add a new habit, or deepen the goals for an existing one. As I alluded to earlier, perhaps you can instead convert an automatic habit you particularly enjoy (e.g. going for a run) into a reward. One can imagine this can have a compounding effect.</p>

<p>It’s helpful to me to categorize all kinds of habits as how “drug-like” they are. E.g., bingeing TV or video games, alcohol and caffeine intake, etc. These are things that you will necessarily gain (at least) a psychological tolerance to, and withdraw from if you stop. If these are minimized, not only do you have more time for other opportunities, but, anecdotally at least, I find I gain a greater mental acuity. I believe this is due to a greater amount of active <a href="https://pubmed.ncbi.nlm.nih.gov/31905114/">dopamine receptors</a> in the brain. Flood the brain with dopamine via various drug-like activities, and to achieve homeostasis, the brain will reduce the amount of receptors to that particular reward. Tolerance to drug-like things is your brain’s way of normalizing rewards.</p>

<p>If you actively minimize drug-like behaviors, this normalization will happen in the opposite direction, and instead of becoming like the typical drug addict for which normal life holds no interest, normal life will be intensely interesting! In this way, you have an innate motivation to seek rewards, and these rewards feel more rewarding! Conversely, we have “negative drugs,” otherwise known as healthy living: exercise, healthy eating, and the like. Forming such habits will give you this same innate motivation.</p>

<h2 id="minimize-all-fun-not-so-fast">Minimize all fun? Not so fast</h2>

<p>The willful optimizing reader may say: why not completely eliminate anything drug-like, and only live in a completely healthy fashion?</p>

<p>I don’t think the brain is designed to work this way; that is, with no rewards, or constant rewards. The brain’s reward system is greedy. It always seeks to maximize rewards experienced, with as minimal effort as possible - unless there is a system in place to prevent this, e.g. a moral system, a value system, and their physical realizations/extensions. One may argue that these kinds of systems and organizations (personal principles, one’s religion, political beliefs, family, the workplace) are simply different sources of rewards, with social or biological foundations themselves. Aside from a good book waiting for you at the end of the day, your reward may be seeing your values play out in the world, your children succeeding in their endeavors, flourishing relationships, a successful career, etc.</p>

<p>These sorts of things are intrinsic, and typically good and healthy. One may argue that if an individual has found sustainable ways to achieve all of  these kinds of long-term rewards, they’re pretty much set for life. Maybe this is how many define success! But, perhaps you’re not quite there yet to a sustainable, satisfying life. If you were satisfied and sustained, you probably wouldn’t be reading this article. Let’s continue with the assumption that you want to make a change.</p>

<h2 id="balance-in-all-things">Balance in all things</h2>

<p>Often these day-to-day, external, intrinsic rewards compete with other rewards in your life for your attention. It is not easy to encapsulate these kinds of rewards and use them as a treat for going out for a run. Furthermore, if you are not intentionally giving yourself a controllable kind of nugget in exchange for “going for a run,” or whatever your goal habit may be, and doing so in a repeated, organized manner, your goal habits are all but guaranteed to fall by the wayside, and be consumed by more intrinsically rewarding things (drug-like things and base instinct).</p>

<p>Maybe that’s not so bad, depending on what your goals may be. For the purposes of this article, we’ll assume you have a goal that is not intrinsically rewarding up front, as happens to be the case with many worthwhile things.<br />
Going back to our question: why not completely eliminate anything drug-like, and only live in a completely healthy fashion? Because if the nugget waiting for you on the other side of your task or goal isn’t juicy enough, you’re not going to form that habit. You’re not going to train your brain the way you consciously desire, it will be trained by other, external factors. Too juicy, and risk being consumed by your reward. Not juicy enough, and risk being consumed by external factors and making no habitual progress.</p>

<p>Play this balancing act with care; I recommend linearly ramping down rewards (starting high)/ ramping up your habit demands (starting low) until it’s just a little hard - then maintaining this challenge level for awhile, seeing that you don’t burn out, before making further perturbations.</p>

<hr />

<p>See the next article, on using CoachRL to put the above into action, <a href="/blog/2023/CoachRLHighLevel/">here</a>.</p>

<p>See the articles in this series <a href="/blog/tag/habits/">here</a>.</p>]]></content><author><name>Kenneth Jabon</name></author><category term="habits" /><summary type="html"><![CDATA[Forming good habits is a lasting way to change your life.]]></summary></entry></feed>