<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://kjabon.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kjabon.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-07-26T18:21:35-05:00</updated><id>https://kjabon.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">Image Regression Lessons Learned (in JAX)</title><link href="https://kjabon.github.io/blog/2023/ImageRegressionJax/" rel="alternate" type="text/html" title="Image Regression Lessons Learned (in JAX)" /><published>2023-07-10T00:00:00-05:00</published><updated>2023-07-10T00:00:00-05:00</updated><id>https://kjabon.github.io/blog/2023/ImageRegressionJax</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/ImageRegressionJax/"><![CDATA[<p>To get up to speed with image processing, and doing so in JAX, I decided to try something not typically found in your starter image classification notebook. This project was (heavily) modified from the ResNet example found on the <a href="https://github.com/deepmind/dm-haiku/tree/main/examples/imagenet">haiku</a> GitHub page. Find my code on <a href="https://github.com/kjabon/image_regression">GitHub</a>.</p>

<p>In this post, partly inspired by a <a href="http://karpathy.github.io/2019/04/25/recipe/">list</a> of NN training tips by Andrej Karpathy, I’ll walk through the process of trying to train with low loss on the following dataset.</p>

<h2 id="the-problem">The problem</h2>
<ul>
  <li>~100k samples: x: X-ray images of lungs, y: age label (this is a <a href="https://www.kaggle.com/datasets/nih-chest-xrays/data">dataset</a> found on kaggle)</li>
  <li>Perform regression from image to age.</li>
</ul>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/xray1-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/xray1-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/xray1-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/xray1.png" class="img-fluid rounded " width="auto" height="auto" title="X-ray 1" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Example x-ray image.</figcaption>

</figure>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/xray2-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/xray2-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/xray2-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/xray2.png" class="img-fluid rounded " width="auto" height="auto" title="X-ray 2" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Example x-ray image.</figcaption>

</figure>

<h2 id="get-signs-of-life-and-starter-tips">Get signs of life, and starter tips</h2>

<p>First, you want to make sure your training pipeline actually works and is bug-free. Does your training loss curve reliably decrease and converge to some reasonable (in the context of your dataset) minimum? If it is merely oscillating in place with no sign of training on the dataset, or forever increasing, many things could be going wrong. A bug in your code should always be your first guess. In deep learning, things have a sneaky tendency to fail silently. Without correct code, you could easily spend days chasing barely perceptible gains by tweaking architecture, hyperparameters, trying different optimizers, etc., when it turned out you were computing your loss incorrectly, or simply forgot to apply your gradient.</p>

<h3 id="start-simple">Start simple</h3>

<p>Anything is on the table as far as bugs go, and for this reason it’s important to just get <em>something that works</em> as soon as possible. Start with a relatively low-complexity model, like a basic CNN. If you’re at the point where this seems menacing, just start with a basic MLP, or even linear regression. Start simpler and work your way up in complexity later. Prove to yourself that your training loop is computing loss and gradients and applying them correctly. Prove to yourself that your dataset is actually labelled correctly by going through it manually. Better yet, start from a known working dataset online to prove that your training pipeline works, then swap in the dataset of interest.</p>

<h3 id="be-one-with-the-dataset">Be one with the dataset</h3>

<p>Even with correct code, your dataset may have little to no real relationships to be learned at all! For this reason it’s important to attune yourself to the dataset. Really immerse yourself in the relationships you expect and the mechanisms behind them, if any. Any deep learning practitioner needs some data scientist tools in their belt. Visualize, normalize, and clean your dataset.</p>

<p>Once you are satisfied the code is correct, you may still not be training effectively. Your learning rate could be too low, or too high (this is typically the first thing to check). Your model may not have enough capacity to capture the relationships in the data.</p>

<p>Once you’re getting off the ground…</p>

<h2 id="overfit-on-purpose">Overfit on purpose</h2>

<p>As with any supervised learning problem, your first goal is to overfit the problem. I.e., while paying no mind to the validation loss, see that your model is actually capable of learning the dataset, whether or not it is generalizing well or overfitting. Put another way, make sure you have enough model capacity to accurately map from x to y.</p>

<p>We’ll start with a Resnet with as few layers as we can, and start adding blocks until we get a desirable training error. In this case, let’s consider ourselves satisfied once our training error is less than 1 (estimating age within +/- 1 years). In theory we could go arbitrarily low, effectively memorizing the dataset.</p>

<p>In the figure below, we’ve run each of these Resnet models with early stopping on the train loss.  Clearly, increasing the number of blocks (parameters) allows it to more accurately model the dataset, but we see diminishing returns, possibly suggesting that we are nearing the actual relationship that the data actually shows. It’s also possible that <em>not</em> using early stopping and continuing to train these networks would decrease the training loss further. If this weren’t a demo problem, this would be the next thing to check (we’ll skip it in this post for the sake of time).</p>

<p>Here we standardize the inputs to the dataset mean and variance which we’ve calculated separately. We also use Batchnorm for training stability.</p>

<details><summary>Standardize inputs</summary>
<d-code block="" language="python">
# ...get the next batch from the dataset...
# extract the images
images = batch['image']


# Example mean and stddev for an RGB dataset (not used here)
# MEAN_RGB = jnp.array([88.94542, 75.03732, 71.47706])
# STDDEV_RGB = jnp.array([71.67816, 66.773605, 65.78593])
stddev_greyscale = jnp.array([42.48881573])
mean_greyscale = jnp.array([126.52482573])
images -= mean_greyscale
images /= stddev_greyscale
</d-code>
</details>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/imageRegressionOverfit-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/imageRegressionOverfit-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/imageRegressionOverfit-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/imageRegressionOverfit.png" class="img-fluid rounded " width="auto" height="auto" title="Overfit" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Training loss as we increase model size, for as long as it helps, then a little longer.</figcaption>

</figure>

<h3 id="maybe-even-go-a-little-bigger">Maybe even go a little bigger</h3>

<p>It’s often a good idea to leave some wiggle room, rather than using a model that is just barely good enough. It helps to allow the model multiple avenues to approximate the dataset, rather than there being one perfect set of trained weights. As it turns out, having a model that is just barely capable of accurately representing the dataset may actually be detrimental to performance, so it’s best to give it a little extra capacity once you think you’ve hit a minimum. See <a href="https://arxiv.org/pdf/1912.02292.pdf">this paper</a> for thoughts on why this may be the case. Once we start regularizing, a little more capacity won’t hurt either: e.g. because we use only a fraction of the network during training during dropout. We’ll worry about increasing the capacity later if need be. Let’s use ResNet34 from here on.</p>

<h2 id="regularize">Regularize</h2>

<p>At this point, it’s practically guaranteed we are overfitting the data, particularly as our model size increases. Let’s take a look at the validation error curves from the previous training run.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/imRegValOverfit-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/imRegValOverfit-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/imRegValOverfit-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/imRegValOverfit.png" class="img-fluid rounded " width="auto" height="auto" title="Overfit" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Validation loss as we increase model size (same runs as previous figure).</figcaption>

</figure>

<p>Yes, so we can see that our validation error is quite high.</p>

<p>As a side note, the generalization error is the difference between validation and training errors. If generalization error is higher than training error, (as it is here), this indicates we need to regularize our model. Since this is already the case even at the minimum of the above curves, early stopping will not help us here.</p>

<p>So, our next step is to drive down this validation error with some kind of regularization. This will allow our model to generalize to the dataset’s distribution, rather than overfitting to the training data.</p>

<h3 id="data-augmentation">Data augmentation</h3>

<h4 id="its-a-kind-of-regularization">It’s a kind of regularization.</h4>

<p>Often we overfit to particular details in a dataset when, in actuality, more general features are the only ones that apply. Suppose that patients over 60 getting x-rayed are recommended to a particular hospital or clinic for their specialty in older patients. Dependent on the x-ray machine or the post processing of its outputs, perhaps all the x-rays to come out of this clinic have a grey tint, or some form of aberration or postmark around the edges of the image. The neural network will learn (incorrectly, for the global dataset) that grey-tinted x-rays correspond to older patients. Perhaps some similar fluke has patients under 30 have their x-rays mirrored along the y axis.</p>

<p>Enter data augmentation. Images are randomly transformed to be slightly different from the original each time they’re passed through the network, so that incidental details are not learned to be the cause for particular labels. Put another way, we add noise to artificially increase the size of the dataset. (If getting more data is a relatively cheap option, you should absolutely do that first).</p>

<p><a href="https://arxiv.org/pdf/1909.13719.pdf">RandAugment</a> is the name of the game at the time of this writing, appearing to have the best performance out of comprehensive image augmentation techniques. It blends many different kinds of classic image augmentations, from contrast to translation.</p>

<p>However, dependent on the type of image to be classified or regressed, don’t blindly use augmentations that would obscure necessary info to you. Use common sense here.</p>

<details><summary>Here’s the preprocessing code implementing RandAugment</summary>
<d-code block="" language="python">
# Each of these layers is implemented as a function in JAX
# in the same file, preprocessing.py

randomLayers = [
    flipX,
    translate,
    color_degen, 
    auto_contrast,
    equalize,
    random_contrast, 
    brightness,
    cutout,
    rotate,
]

def randomAugmentJax(images, rng, policy, augmentations_per_image = 3, rate = 10/11):
    # For each image, we're going to want a different rng, so we need to split
    keys = jax.random.split(rng, images.shape[0])

    def apply_rand_layer(image, layer_select_key, layer_key):
        randIndex = jax.random.randint(layer_select_key, (), 0, len(randomLayers)-1)
        for i in range(len(randomLayers)):
            image = jnp.where(randIndex == i, randomLayers[i](image, layer_key, policy), image)
        return image

    # Now, we need to define a function we'll vmap to our images
    def augment(image, loop_key):

        #within this function, we just copy the rand_augment apply
        original_image = image

        for _ in range(augmentations_per_image):
            # get a skip uniform random number
            loop_key, skip_key, layer_select_key, layer_key = jax.random.split(loop_key,4)
            skip_augment = jax.random.uniform(skip_key)
            #skip based on rng above (identity)
            #else, apply a random layer to the input
            image = jnp.where(skip_augment &gt; rate,
                              original_image,
                              apply_rand_layer(image, layer_select_key, layer_key))
        return image

    return jax.vmap(augment)(images, keys)
</d-code>
</details>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/resnet34wAug-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/resnet34wAug-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/resnet34wAug-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/resnet34wAug.png" class="img-fluid rounded " width="auto" height="auto" title="With data augmentation" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Training and validation loss upon adding data augmentation.</figcaption>

</figure>

<p>The clear difference here is our validation loss tracks the training loss curve. It isn’t a one-to-one comparison, as data augmentations aren’t applied during eval, so we expect the training loss to be artificially higher. However, as the training loss decreases, the validation loss does as well; they don’t split apart early on as before, with an increasing generalization error. This gives us confidence that we can continue training for a long time without overfitting. We see our validation loss drops from 4.4 to 3.4. Now we’re seeing some results!</p>

<p>I used early stopping here, but it is likely the validation loss would decrease even further. Additionally, a run I did before writing this post with the same parameters, only adding dropout to the final MLP after the ResNet blocks, got down to 2.91 years mean absolute error when taken to 80k training steps. (Dropout rate was set to 0.25). With that in mind, let’s turn on dropout for the rest of this post.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/resnet34wAugAndDropout-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/resnet34wAugAndDropout-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/resnet34wAugAndDropout-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/resnet34wAugAndDropout.png" class="img-fluid rounded " width="auto" height="auto" title="With dropout" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Training and validation loss upon adding dropout.</figcaption>

</figure>

<h2 id="speed-is-the-key">Speed is the key</h2>

<p>While doing all these tasks, your concern should also be speed of training for a low iteration time.</p>

<p>I recommend you don’t use the python multiprocessing library for data augmentation: inter-process communication is too slow because it writes to disk, and you’ll be moving a lot of data around. There are python libraries out there which purport to use RAM for communication, but I did not find them to be particularly plug-and-play.<br />
Instead, learn to use tfds for your data pipeline and dataset creation via their examples and documentation. It may seem like a big hurdle at first, but it <em>will</em> pay off in speed, and it’s applicable to any supervised learning problem you can imagine. (If you’re using pytorch, there is a similar dataloader for you to learn).</p>

<details><summary>Here’s the tfds code preparing the dataset pipeline including some preprocessing</summary>
<d-code block="" language="python">
import tensorflow_datasets as tfds
def load_dataset(split):
    # Maximize batch size for the available VRAM
    # Bigger images &amp; bigger networks = smaller batch size
    imgSize = imgSizes[FLAGS.img_size_num]
    resNetRelSize = resnetSizes[FLAGS.resnet_num] / 50
    batchSize = nearest2(int(FLAGS.batch_size/resNetRelSize*(128/imgSize)**2))
    
    train_dataset = tfds.load(datasetName, split=split)
    num_datapoints = train_dataset.cardinality().numpy()
    train_dataset = train_dataset.repeat().shuffle(1024)

    train_dataset = train_dataset.map(lambda x: (preprocess(x, True)), num_parallel_calls=-1)

    train_dataset = train_dataset.batch(batchSize).batch(jax.local_device_count()).prefetch(-1)

    train_dataset = tfds.as_numpy(train_dataset)

    return train_dataset, num_datapoints
    
# Found in preprocessing.py
def preprocess(x):
    # This is a little barebones; before moving preprocessing to GPU, 
    # everything was done here via tfds on cpu.
    x,y = deconstruct(x)
    if x.get_shape()[2] == 1:
        x = rgb(x) # convert greyscale to 3 channels for uniform architecture.
    x = cast_fn(x)
    return reconstruct(x, y)
</d-code>
</details>

<p>This said, if you’re doing all your data preprocessing (e.g. image augmentation) in tfds, you might find you’re CPU-bottlenecked; this was the case for me. With access to highly optimized libraries which can easily execute batched operations on the GPU, or better yet, multiple GPUs (JAX!), you would be remiss to ignore that possibility for your data augmentation. GPUs are not just for neural networks! When in doubt, see if you can implement your preprocessing as part of your network, with a flag for training/eval. This enabled me to go from 20 to near-100% utilization across 3 GPUs.</p>

<details><summary>This code defines the network, but importantly includes preprocessing steps on the GPU in JAX!</summary>
<d-code block="" language="python">
def _forward(
        batch,
        is_training: bool,
        apply_rng=None
) -&gt; jax.Array:
    """Forward application of the resnet."""
    images = batch['image']
    
    # Do preprocessing on the GPU!!
    # randomAugmentJax is found in preprocessing.py
    if is_training:
        apply_rng, aug_rng = jax.random.split(apply_rng)
        images = randomAugmentJax(images,aug_rng,get_policy(),
                                  thruLayer=FLAGS.thruLayer,
                                  onlyLayer=FLAGS.onlyLayer)

    # Normalize the inputs on the GPU!!
    STDDEV_RGB = jnp.array([42.48881573])
    MEAN_RGB = jnp.array([126.52482573])
    images -= MEAN_RGB
    images /= STDDEV_RGB


    net = resnets[FLAGS.resnet_num](FLAGS.layer_size,
                                    resnet_v2=FLAGS.model_resnet_v2,
                                    bn_config={'decay_rate': FLAGS.model_bn_decay})
    mlp = hk.nets.MLP(
        output_sizes=[FLAGS.layer_size, FLAGS.layer_size],
        activate_final=True
    )

    yHold = net(images, is_training=is_training)
    y = hk.BatchNorm(True, True, FLAGS.model_bn_decay)(yHold, is_training=is_training)
    y = mlp(y, FLAGS.dropout_rate, apply_rng) if apply_rng is not None else mlp(y)
    # skip connection
    y = y + yHold
    y = hk.Linear(1)(y)  # No final activation
    return y.flatten()
# Transform our forwards function into a pair of pure functions.
# This will later be run on all devices with jax.pmap()
forward = hk.transform_with_state(_forward)
</d-code>
</details>

<p>It may also be worth using half precision (f16) computation. I won’t get into it here, but the code found on the accompanying GitHub (link at the bottom of this post) allows this by simply changing the <code class="language-plaintext highlighter-rouge">mp_policy</code> flag to <code class="language-plaintext highlighter-rouge">'p=f32,c=f16,o=f32'</code>. In my testing this approximately doubles the speed, sometimes at the cost of training stability (you are likely to see NaNs until you fiddle with your learning rate and batch size to make them go away). So far I haven’t found much difference in the overall performance.</p>

<p>Last but not least, balance workload between CPU and GPU so that you’re not bottlenecked anywhere. This may be more hassle than it’s worth, but is worth considering if you have nothing to do but wait for your training runs to complete.</p>

<h2 id="further-optimization">Further optimization</h2>

<h3 id="hyperparameter-tuning">Hyperparameter tuning</h3>

<p>Once you have a reasonable h-param range and decent performance, optuna is your friend, but not before. You may have bugs in your code, or the model may under/overfit without the right details as above.</p>

<p>One of these hyperparameters may even be deeper networks. Now that we’ve added augmentation, we may squeeze a few % more out of the data with a higher capacity model.</p>

<p>I will leave this as an exercise for the reader :) .</p>

<h3 id="modern-architectures-and-more">Modern architectures and more</h3>

<p>I did not touch on it in this post, but at this point if the application requires, it may be worth checking out newer networks. Checking out the ImageNet leaderboard shows Vision Transformers, among many others, outperforming ResNet. However, you may find these have their own trade-offs, perhaps requiring much larger datasets or pre-training to get this performance.</p>

<p>Furthermore, in my experience, feature extraction from / fine-tuning a pre-trained network (e.g., a ResNet-50 trained on ImageNet) is likely to give you <strong>astounding</strong> results with minimal training time, in particular if your dataset of interest is similar to the images found in the larger dataset the network was pre-trained on. ImageNet may not help much with X-rays, but other applicable pre-trained networks may be out there.</p>

<p>Good luck in your exploration!</p>

<hr />

<p>Thanks for reading! Find the code on <a href="https://github.com/kjabon/image_regression">GitHub</a>.</p>]]></content><author><name>Kenneth Jabon</name></author><category term="rl" /><summary type="html"><![CDATA[An exercise in image processing]]></summary></entry><entry><title type="html">Vanilla Policy Gradient In JAX</title><link href="https://kjabon.github.io/blog/2023/VPGJAX/" rel="alternate" type="text/html" title="Vanilla Policy Gradient In JAX" /><published>2023-05-03T00:00:00-05:00</published><updated>2023-05-03T00:00:00-05:00</updated><id>https://kjabon.github.io/blog/2023/VPGJAX</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/VPGJAX/"><![CDATA[<p>For this blog post, We’ll not only get into a Vanilla Policy Gradient (VPG) implementation, but perhaps more interestingly, do it in JAX. Note this post focuses on implementation; the theory of VPG is broken down in <a href="/blog/2023/VPG">this post</a>. Feel free to take a look at that first if you find yourself asking “why?”</p>

<p>We’re going to implement it piece by piece. The full code can be found at this <a href="https://github.com/kjabon/vpg_acme_jax">repo</a>.</p>

<h2 id="the-first-line">The first line</h2>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pseudoPseudo.svg-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pseudoPseudo.svg-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pseudoPseudo.svg-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pseudoPseudo.svg" class="img-fluid rounded" width="auto" height="auto" title="Algorithm 0" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="initialize-networks">Initialize networks</h3>
<p>Warning: this section makes up the first half of this post. If you don’t feel you need to learn how to implement neural networks for RL in JAX, feel free to <a href="https://kjabon.github.io/blog/2023/VPGJAX/#the-rl-loop">skip this section</a> and head directly to the rest of the reinforcement learning loop.</p>

<p>We need a policy and a value function. Both will be represented by neural networks. I have found that for most starter Gym <a href="https://gymnasium.farama.org/">environments</a>, two layers with 128 weights each do the job just fine (and often even less than this is needed).</p>

<p>The below code is adapted from the Acme <a href="https://github.com/deepmind/acme">implementation</a> of PPO. Kudos to DeepMind for providing excellent baseline implementations of many RL algorithms.</p>

<p>Before we get started, let’s define some convenience classes which will allow us to organize our neural network and the functions related to it. This will also serve to give us a roadmap for building and initializing our neural networks.</p>

<d-code block="" language="python">
class VPGNetworks:
    network:   networks_lib.FeedForwardNetwork
    sample:    networks_lib.SampleFn    # = Callable[[NetworkOutput, PRNGKey], Action]
    log_prob:  networks_lib.LogProbFn   # = Callable[[NetworkOutput, Action], LogProb]
</d-code>

<p>This is pretty straightforward: for RL we need a network, a way to sample from the outputs of that network, and a way to compute the log probabilities of those samples (i.e., the actions). If you read that and didn’t think “yes, very straightforward,” consider reading the previous <a href="/blog/2023/VPG">post</a>.</p>

<p><code class="language-plaintext highlighter-rouge">sample()</code> will take in the network output (i.e., info encoding the probability distribution) and a random number key, and return a sampled action.</p>

<p><code class="language-plaintext highlighter-rouge">log_prob()</code> will take in the network output (again, the probability distribution) and an action, and return the log probability of that action.</p>

<p>In general our action will be a vector of real numbers (sometimes ints for discrete spaces), and our network will operate on batches of these.</p>

<p>Notice above that <code class="language-plaintext highlighter-rouge">network</code> is of type <code class="language-plaintext highlighter-rouge">FeedForwardNetwork</code>. Let’s take a look at the <code class="language-plaintext highlighter-rouge">FeedForwardNetwork</code> class. This is, again, a skeleton for us to fill out, and also a roadmap.</p>
<d-code block="" language="python">
class FeedForwardNetwork:
  # A pure function: ``params = init(rng, *a, **k)`` 
  # Initializes and returns the networks parameters.
  init

  # A pure function: ``out = apply(params, rng, *a, **k)`` 
  # Computes and returns the outputs of a forward pass.
  apply
  
</d-code>

<p>So, we need a way to initialize and apply our neural network, and for JAX, we need to ensure these are pure functions. Luckily, the <code class="language-plaintext highlighter-rouge">haiku</code> library takes care of most of the heavy lifting here, as we’ll see. For clarity, I’ve swept some type checking under the rug.</p>

<h3 id="lets-get-cracking-on-the-network">Let’s get cracking on the network!</h3>

<p>Our VPG pseudocode specifies to initialize networks. The following MLP and Linear functions allow starting with various well-known initializations, falling under <a href="https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.initializers.variance_scaling.html#jax.nn.initializers.variance_scaling">variance scaling</a>. For this post we’ll just use the defaults without specifying those arguments.</p>

<h4 id="starting-with-the-value-function">Starting with the value function:</h4>
<d-code block="" language="python">
import haiku as hk
import jax.numpy as jnp 
# jax.numpy has the same interface as numpy, but uses XLA backend for hardware acceleration!

layer_sizes = (128,128)
V = hk.nets.MLP(layer_sizes, activate_final=True)
V = hk.Linear(1)(V)
</d-code>

<details><summary>Why haiku?</summary>
<p>By the way, you may be wondering “why haiku instead of flax?” Well, the immediate reason is that personally, I’m more used to it. They’re very similar, application-wise, so you should just stick with what works for you.</p>

<p>Highly subjective opinion warning: the feeling I get is that the collection of the Deepmind repositories are less the result of a “wild-west, every small team publishing for themself” environment. Rather, the collection of repos has a sense of cohesion and coordination. I could be wrong about this. Anyway, Deepmind has done a good job <a href="https://github.com/deepmind?q=reinforcement&amp;type=all&amp;language=&amp;sort=">open sourcing</a> a ton of their software stack related to reinforcement learning and deep learning, not limited to haiku. Check it out!</p>
</details>

<p>Now, we almost have an MLP which can serve as our value function, with a single output: the value given the observation! However we’re missing two things: proper input and output dimensions.</p>

<p>We need to tell the model what input dimensions (from the batch of observations) we expect. We’ll leave this as a dummy <code class="language-plaintext highlighter-rouge">obs</code> variable for now - we’ll come back to it in a minute. We would like to flatten the input, except for the batch dimension. That is, for batch size n, we want n observation vectors. We define the following function to do so:</p>

<d-code block="" language="python">
def batch_flatten(obs):
	if obs.ndim &gt; 0: 
		return jnp.reshape(obs, [obs.shape[0], -1])
	return input

</d-code>

<p>…and prepend a call to this function to our code like so:</p>

<d-code block="" language="python">
V = batch_flatten(obs)
V = hk.nets.MLP(layer_sizes, activate_final=True)(V)
V = hk.Linear(1)(V)
</d-code>

<details><summary>More than one batch dimension in JAX?</summary>
<p>We assume the observation above has one batch dimension (e.g., 64, 128, etc. This is the batch size.). However, if you’ve gone poking around in other JAX examples online, you may notice an extra dimension out front in the main training loop. This is typically the device dimension.</p>

<p>Suppose typically I have a set of dimensions that looks like (batch_size, obs_size), like (128, 64). Adding a device dimension would look like (num_devices, batch_size, obs_size). Across one device, this would look like (1, 128, 64). However, parallelizing across two devices, this would look like (2, 64, 64). Our total batch size remains unchanged, but you can see the batch has been split across the two devices.</p>

<p>Our inner functions (like the past few code blocks) which take in <code class="language-plaintext highlighter-rouge">obs</code> don’t see the device dimension; from their perspective it doesn’t exist, so you can write these as if you only had one device. <code class="language-plaintext highlighter-rouge">pmap()</code> takes care of mapping the full computation to devices, though I won’t cover its usage here; see the <a href="https://jax.readthedocs.io/en/latest/jax-101/06-parallelism.html">documentation</a>. As you can see in those examples, it’s only in the main training loop you need to worry about adding a device dimension to your tensors.</p>
</details>

<p>Finally, we’d like to make sure the output of our value function is a batch-sized vector of values if it isn’t already, so we squeeze the output results. Putting all of this into a haiku <code class="language-plaintext highlighter-rouge">Sequential</code> model yields:</p>

<d-code block="" language="python">
value_network = hk.Sequential([
	batch_flatten,
	hk.nets.MLP(value_layer_sizes, activate_final=True),
	hk.Linear(1),
	lambda x: jnp.squeeze(x, axis=-1)
])
</d-code>

<p>This completes our value network!</p>

<h4 id="now-our-policy-model-is-slightly-different">Now, our policy model is slightly different.</h4>
<p>First, because we are assuming a continuous action space, we need to output a distribution over actions, from which the actual action for some observation will be sampled. The policy model will output mean and variance to describe this multivariate normal distribution. Two fully connected layers are branched from the torso, to be able to learn mean and variance from the same embedding of the observation. For this reason, we don’t use <code class="language-plaintext highlighter-rouge">hk.Sequential</code>.</p>

<p>Here is the torso:</p>

<d-code block="" language="python">
h = utils.batch_concat(obs)
h = hk.nets.MLP(policy_layer_sizes, activate_final=True)(h)
</d-code>

<p>Now we add a branch each for the mean and variance.</p>

<d-code block="" language="python">
min_scale = 1e-3
num_dimensions = np.prod(environment_spec.actions.shape, dtype=int)
mean_layer = hk.Linear(num_dimensions)
var_layer = hk.Linear(num_dimensions)

mean_out = mean_layer(h)
var_out = var_layer(h)
</d-code>

<p>Constrain to a positive scale, as variance must be positive.</p>

<d-code block="" language="python">
var_out = jax.nn.softplus(var_out) + 1e-3 #some epsilon; avoid div-by-zero
</d-code>

<details><summary>Why softplus?</summary>
<p>ReLU will compute faster, and may be a choice worth considering if compute power/training speed is a concern. However, if the user has problems with ReLU “dying” in the zero region (when \(x &lt; 0\), \(y = 0\)), we cannot use leaky ReLU, the usual first solution to this problem, because the output must be positive. Softplus is a better option in this case.</p>
</details>

<p>We’re not quite done just yet, because of the vagaries of JAX. We need to wrap and transform the models we’ve just written.</p>

<p>First, we encapsulate our policy code into a function:</p>
<d-code block="" language="python">
def policy_network(obs):
	#…previous code defining policy model…
	return (mean_out, var_out)
</d-code>

<p>Then, we wrap both the policy and value functions into one forward function, outputting everything we infer from the observation from this function.</p>
<d-code block="" language="python">
def forward_fn(inputs: networks_lib.Observation):
	inputs = jnp.array(inputs, dtype=jnp.float32) #ensure we are working with JAX NumPy

	#…previous code defining policy and value functions…

	policy_output = policy_network(inputs)
	value = value_network(inputs)
	return (policy_output, value)
</d-code>

<p>Finally, we use the haiku <code class="language-plaintext highlighter-rouge">transform()</code> function.</p>

<details><summary>An aside on <code class="language-plaintext highlighter-rouge">transform()</code>, JAX, and pure functions</summary>
<p>If you don’t care about why we use transform, so much as that it makes deep learning fast, feel free to skip this section.</p>

<p>From the haiku <a href="https://dm-haiku.readthedocs.io/en/latest/notebooks/basics.html">documentation</a>:</p>
<blockquote>
  <p>…Haiku modules are Python objects that hold references to their own parameters, other modules, and methods that apply functions on user inputs. On the other hand, since JAX operates on pure function transformations, Haiku modules cannot be instantiated verbatim. Rather, the modules need to be wrapped into pure function transformations.<br />
Haiku provides a simple function transformation, hk.transform, that turns functions that use these object-oriented, functionally “impure” modules into pure functions that can be used with JAX.</p>
</blockquote>

<p>See <a href="https://en.wikipedia.org/wiki/Pure_function">here</a> for pure functions, and <a href="https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#pure-functions">here</a> for why pure functions are required.</p>

<p>In short, JAX uses cached compilations of functions to speed everything up.</p>

<p>As you, a computer scientist, probably know, an optimizing compiler (here, XLA) converts your code into a form that is both lower level (closer to machine instructions), and able to make certain assumptions to run your computations in fewer steps. Oh yeah, compilation is great!</p>

<p>But of course, anyone who’s written anything moderately sized in C/C++ knows that repeated compilation will send your productivity straight in the trash, right next to the greasy pizza boxes and used Kleenex, never to be seen again. Can we avoid unnecessary compilation?</p>

<p>By mandating pure functions, JAX knows the function’s behavior is only dependent on its inputs, and so not only will it never need to be recompiled after the first time it’s used. Tt will be able to make more assumptions to improve its computational efficiency all the more so!</p>
</details>

<p>So, without further ado, let’s transform our model!</p>

<d-code block="" language="python">
forward = hk.without_apply_rng(hk.transform(forward_fn))
</d-code>

<p>Now that we’ve transformed (“purified/JAX-ified”) our model, we’ve exposed a pure interface to our model we can use without worry: that is, <code class="language-plaintext highlighter-rouge">forward.init()</code>, and <code class="language-plaintext highlighter-rouge">forward.apply()</code>.</p>

<p>We’ve additionally wrapped this with <code class="language-plaintext highlighter-rouge">hk.without_apply_rng()</code>. Our forward function’s <code class="language-plaintext highlighter-rouge">apply</code> method may require randomness in general, e.g. if it uses dropout layers during training. In our case, an rng key is not required, so we use this convenience wrapper to avoid needing to pass in an extra parameter when calling <code class="language-plaintext highlighter-rouge">apply()</code>.</p>

<p>Let’s initialize our model by creating a dummy observation with the same dimensions as the real inputs to the model, and passing this to <code class="language-plaintext highlighter-rouge">init()</code>. The model will then be configured to handle the correct input and batch dimensions.</p>

<d-code block="" language="python">
dummy_obs = utils.zeros_like(environment_spec.observations)
dummy_obs = utils.add_batch_dim(dummy_obs)
network = networks_lib.FeedForwardNetwork(
	lambda rng: forward.init(rng, dummy_obs), forward.apply)
</d-code>

<p>As we saw in the beginning of this post, <code class="language-plaintext highlighter-rouge">FeedForwardNetwork</code> simply wraps <code class="language-plaintext highlighter-rouge">haiku.transform</code>’s pure functions (<code class="language-plaintext highlighter-rouge">init</code> and <code class="language-plaintext highlighter-rouge">apply</code>) into an Acme container.</p>

<details><summary>Why rng key?</summary>
<p>JAX shuttles around these explicit (pseudo-)random number generator parameters in the functions that require them. In this way, you can reproduce experiments without worrying that a different seed is causing you uncertainty in your results behind the scenes. All in the name of good science!</p>

<p>Why is this explicitness necessary? Normally, Python will, from an initial seed, maintain a global key which functions will then access, generating random numbers. This is fine and dandy if there is a single thread of execution. For example, consider the following code.<d-footnote> This is effectively the same example as the JAX documentation (https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html#random-numbers-in-jax), read that for further consideration of the implications and solutions.</d-footnote></p>

<d-code block="" language="python">
import random

random.seed(42)

def funA(): return random.random( )
def funB(): return random.random( )

def randomSum( ): return funA( ) + 2 * funB( )

print(randomSum( ))
</d-code>

<p>Python will always run <code class="language-plaintext highlighter-rouge">randomSum()</code> the same way, according to a well-defined <a href="https://docs.python.org/3/reference/expressions.html#operator-precedence">operator precedence</a>. <code class="language-plaintext highlighter-rouge">funA()</code> is called (first random number from seed 42), then <code class="language-plaintext highlighter-rouge">2*funB()</code> is calculated, at which time <code class="language-plaintext highlighter-rouge">funB()</code> is called (second random number from seed 42), then they are summed. If <code class="language-plaintext highlighter-rouge">funB</code> ran before <code class="language-plaintext highlighter-rouge">funA</code> on occasion, the final output would change. This never happens. The output of <code class="language-plaintext highlighter-rouge">randomSum()</code> will always be the same, i.e. reproducible!</p>

<p>Feel free to add some print statements and run the code to satisfy any doubts.</p>

<p>If we are parallelizing our code (the whole point of using JAX!), we <em>don’t</em> know what order functions will be called in, due to how your OS handles <a href="https://en.wikipedia.org/wiki/Scheduling_(computing)#Process_scheduler">process scheduling</a>. <d-footnote>These adverse effects are the same for threads and processes assuming every concurrent unit has access to the global key, so we use them interchangeably here.</d-footnote> If we delegated the calling of <code class="language-plaintext highlighter-rouge">funA()</code> and <code class="language-plaintext highlighter-rouge">funB()</code> above to different processes, sometimes <code class="language-plaintext highlighter-rouge">funB()</code> might run first. As a result, using a global random key causes us to sacrifice our beloved reproducibility. Thus, we must explicitly give each process its own specific key, which it can then use in its own good time, regardless of execution order, in a reproducible, well-defined fashion.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/neverLate.gif-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/neverLate.gif-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/neverLate.gif-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/neverLate.gif" class="img-fluid rounded" width="auto" height="auto" title="never late" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">A process executes precisely when it means to.</figcaption>

</figure>

</details>

<p>So, we have our network defined and transformed. Recalling our container class:</p>

<d-code block="" language="python">
class VPGNetworks:
    network:   networks_lib.FeedForwardNetwork
    sample:    networks_lib.SampleFn    # = Callable[[NetworkOutput, PRNGKey], Action]
    log_prob:  networks_lib.LogProbFn   # = Callable[[NetworkOutput, Action], LogProb]
</d-code>

<p>All that remains is a little book-keeping to expose <code class="language-plaintext highlighter-rouge">sample()</code> and <code class="language-plaintext highlighter-rouge">log_prob()</code> functions. First, assume we have a way of getting at the policy’s output parameterizing a normal distribution, <code class="language-plaintext highlighter-rouge">params</code> (which, recall, is <code class="language-plaintext highlighter-rouge">(mean_out, var_out)</code>). With this we’ll be able <code class="language-plaintext highlighter-rouge">sample()</code> the policy. Now that we have the <code class="language-plaintext highlighter-rouge">action</code> returned from this <code class="language-plaintext highlighter-rouge">sample()</code>, we can also calculate the <code class="language-plaintext highlighter-rouge">log_prob()</code>. We just need to use some functions from the <code class="language-plaintext highlighter-rouge">tensorflow_probability</code> library. While we focus on continuous distributions in this post, the procedure is similar for categorical distributions.</p>

<d-code block="" language="python">
import tensorflow_probability
tfp = tensorflow_probability.substrates.jax
tfd = tfp.distributions

def sample(params, key):
	return tfd.MultivariateNormalDiag(
		loc=params.loc, scale_diag=params.scale_diag).sample(seed=key)
		
def log_prob(params, action):
	return tfd.MultivariateNormalDiag(
		loc=params.loc, scale_diag=params.scale_diag).log_prob(action)

vpgNetwork = VPGNetworks(
	network=network,
	log_prob=log_prob
	sample=sample
)
</d-code>

<p>Finally, just a bit more glue and book-keeping to take care of exposing an easy and organized way of getting our actions and associated log probabilities in one function call, <code class="language-plaintext highlighter-rouge">inference(params, key, observations)</code>.</p>

<d-code block="" language="python">
def make_inference_fn(
    vpg_networks: VPGNetworks,
    evaluation: bool = False) -&gt; actor_core_lib.FeedForwardPolicyWithExtra:
  """Returns a function to be used for inference by a PPO actor."""

  def inference(params, key: networks_lib.PRNGKey,
                observations: networks_lib.Observation):
    dist_params, _ = vpg_networks.network.apply(params, observations)
    if evaluation and vpg_networks.sample_eval:
      actions = vpg_networks.sample_eval(dist_params, key)
    else:
      actions = vpg_networks.sample(dist_params, key)
    if evaluation:
      return actions, {}
    log_prob = vpg_networks.log_prob(dist_params, actions)
    return actions, {'log_prob': log_prob}

  return inference
</d-code>

<p>Whew! We made it past the first line! Surprisingly, most of the hard code is behind us, and we get to jump into the algorithm proper.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/whereTheFunBegins-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/whereTheFunBegins-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/whereTheFunBegins-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/whereTheFunBegins.jpg" class="img-fluid rounded" width="auto" height="auto" title="the fun" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">This is where the fun begins.</figcaption>

</figure>

<h2 id="the-rl-loop">The RL loop</h2>

<p>You should already have an understanding of the full loop before we proceed (see <a href="/blog/2023/VPG">here</a>). <br />
Let’s dive into each component individually.</p>

<h2 id="the-third-line">The third line</h2>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pseudoPseudo.svg-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pseudoPseudo.svg-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pseudoPseudo.svg-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pseudoPseudo.svg" class="img-fluid rounded" width="auto" height="auto" title="Algorithm 0" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="collect-set-of-trajectories">Collect set of trajectories</h3>

<p>We’re going to start at the level of the training loop and drill our way down. We’re going to start seeing a fair bit of Acme machinery for creating and training our agents, but I’ll sweep most of the extraneous stuff under the rug.</p>

<d-code block="" language="python">
def run(environment, actor, num_episodes)

	train_loop = acme.EnvironmentLoop(
	      environment,
	      actor)


	def should_terminate(episode_count: int) -&gt; bool:
		return num_episodes is not None and episode_count &gt;= num_episodes

	episode_count: int = 0
	while not should_terminate(episode_count):
		train_loop.run_episode()
		episode_count += 1

</d-code>

<p>Entering <code class="language-plaintext highlighter-rouge">run_episode()</code>:</p>

<d-code block="" language="python">
class EnvironmentLoop:
  def run_episode(self):

    # Start the environment.
    
    timestep = self._environment.reset()
    # Make the first observation. This is where the trajectories are recorded to a data server.
    self._actor.observe_first(timestep)

    # Run an episode.
    while not timestep.last():
    
      # Generate an action from the agent's policy. 
      # This will call our inference function above!
      action = self._actor.select_action(timestep.observation)

      # Step the environment with the agent's selected action.
      timestep = self._environment.step(action)

      # Have the agent observe the timestep.
      # This is where the trajectories are recorded to a data server.
      self._actor.observe(action, next_timestep=timestep)

      # Give the actor the opportunity to update itself.
      self._actor.update()


</d-code>

<p>There’s quite a bit going on here. First the environment is initialized (reset), and we store the first observation in Reverb. Because VPG is an online algorithm, we don’t have a replay buffer; we’re simply storing enough transitions from our trajectory for the next learner step.</p>

<p>Through a series of RL/Acme boilerplate, we send our observation through the neural net representing the policy and sample an action from the output distribution. We <code class="language-plaintext highlighter-rouge">step()</code> the <code class="language-plaintext highlighter-rouge">environment</code> with this <code class="language-plaintext highlighter-rouge">action</code>, and store the transition/timestep in Reverb with <code class="language-plaintext highlighter-rouge">observe()</code>. Finally, in <code class="language-plaintext highlighter-rouge">update()</code>, if there are enough transitions to make up a full batch, the learner (contained within the actor object) will perform a training step (detailed in the following sections), and update the <code class="language-plaintext highlighter-rouge">actor</code> with the resultant new parameters.</p>

<h2 id="the-fourth-line">The fourth line</h2>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pseudoPseudo.svg-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pseudoPseudo.svg-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pseudoPseudo.svg-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pseudoPseudo.svg" class="img-fluid rounded" width="auto" height="auto" title="Algorithm 0" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="compute-future-returns">Compute future returns.</h3>
<p>The theory of this has been discussed in a <a href="https://kjabon.github.io/blog/2023/VPG2/#future-return">previous post</a>; with that in mind the following code block should be self-explanatory.</p>
<d-code block="" language="python">
def truncated_discounted_sum_of_rewards(r_t, discount_t, v_t, n):

  local_batch_size = r_t.shape[0]
  seq_len = r_t.shape[1]
  pad_size = min(n - 1, seq_len)
  targets = jnp.concatenate([v_t[:,n - 1:], jnp.ones((local_batch_size, pad_size))*v_t[0,-1]], axis=1)
  
  # Pad sequences. Shape is now (T + n - 1,).
  r_t = jnp.concatenate([r_t, jnp.zeros((local_batch_size, n - 1))], axis=1)
  discount_t = jnp.concatenate([discount_t, jnp.ones((local_batch_size, n - 1))], axis=1)

  # Work backwards to compute n-step returns.
  for i in reversed(range(n)):
      r_ = r_t[:,i:i + seq_len]
      discount_ = discount_t[:,i:i + seq_len]
      targets = r_ + discount_ * targets

                                
</d-code>

<p>However, we wish to use <a href="https://kjabon.github.io/blog/2023/VPG2/#generalized-advantage-estimation">generalized advantage estimation</a>, so we take advantage of an <code class="language-plaintext highlighter-rouge">rlax</code> function to compute the advantages for us, vectorizing over the batch dimension with <code class="language-plaintext highlighter-rouge">jax.vmap()</code>. This takes place on a single device (GPU).</p>
<d-code block="" language="python">
vmapped_rlax_truncated_generalized_advantage_estimation = jax.vmap(
  rlax.truncated_generalized_advantage_estimation,
  in_axes=(0, 0, None, 0))
advantages = vmapped_rlax_truncated_generalized_advantage_estimation(
  rewards[:, :-1], discounts[:, :-1], gae_lambda, behavior_values)
</d-code>

<h2 id="lines-5--7a">Lines 5 &amp; 7a</h2>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pseudoPseudo.svg-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pseudoPseudo.svg-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pseudoPseudo.svg-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pseudoPseudo.svg" class="img-fluid rounded" width="auto" height="auto" title="Algorithm 0" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="compute-the-policy-gradient-and-compute-the-value-function-gradient">Compute the policy gradient, and compute the value function gradient</h3>
<p>The theory of the policy gradient has been discussed in a <a href="https://kjabon.github.io/blog/2023/VPG/#learning-from-rewards">previous post</a>, and we discuss using the <code class="language-plaintext highlighter-rouge">advantages</code> instead of the future return \(R_t\) <a href="https://kjabon.github.io/blog/2023/VPG2/#generalized-advantage-estimation">here</a>, and we describe the value function loss <a href="https://kjabon.github.io/blog/2023/VPG2/#the-value-function">here</a>.</p>

<p>With that in mind the following code block should be self-explanatory. We run inference on the observations to get our policy and value network outputs, and use this output combined with the <code class="language-plaintext highlighter-rouge">advantages</code> and <code class="language-plaintext highlighter-rouge">target_values</code> (\(R_t\)), to calculate the policy loss and value functions, respectively. Then we run <code class="language-plaintext highlighter-rouge">jax.grad()</code>, and voila, we have the gradient to update our networks with! Very simple. The function also takes care of normalization of input advantages.</p>

<p>As a reminder, here’s the policy gradient:</p>

\[\hat{g}=\frac{1}{\left| \mathcal{D}_k \right|}\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} A_t\]

<p>And here’s the value loss, used to compute the value gradient:</p>

\[\frac{1}{\left\vert  \mathcal{D}_k \right\vert T}\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\left(V_\phi(s_t)-\hat{R}_t  \right)^2\]

<d-code block="" language="python">
class VPGLearner:
    def vpg_loss(
        params: networks_lib.Params,
        observations: networks_lib.Observation,
        actions: networks_lib.Action,
        advantages: jnp.ndarray,
        target_values: networks_lib.Value,
        behavior_values: networks_lib.Value,
        behavior_log_probs: networks_lib.LogProb,
        value_mean: jnp.ndarray,
        value_std: jnp.ndarray,
        key: networks_lib.PRNGKey,
    ) -&gt; Tuple[jnp.ndarray, Dict[str, jnp.ndarray]]:
      """VPG loss for the policy and the critic."""
      distribution_params, values = vpg_networks.network.apply(
          params, observations)
      if normalize_value:
        target_values = (target_values - value_mean) / jnp.fmax(value_std, 1e-6)
      policy_log_probs = vpg_networks.log_prob(distribution_params, actions)

      policy_loss = - (policy_log_probs * advantages).mean()

      value_loss = (values - target_values) ** 2
      value_loss = jnp.mean(value_loss)

      total_vpg_loss = policy_loss + value_cost * value_loss #value cost is a hyperparameter passed to the VPGLearner class

      return total_vpg_loss, {
          'loss_total': total_vpg_loss,
      }

    vpg_loss_grad = jax.grad(vpg_loss, has_aux=True)

                                
</d-code>

<h2 id="lines-6--7b">Lines 6 &amp; 7b</h2>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pseudoPseudo.svg-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pseudoPseudo.svg-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pseudoPseudo.svg-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pseudoPseudo.svg" class="img-fluid rounded" width="auto" height="auto" title="Algorithm 0" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h3 id="update-the-policy-and-value-functions-with-their-respective-gradients">Update the policy and value functions with their respective gradients.</h3>

<p>This part should be familiar to anyone with a passing interest in deep learning. With our optimizer (e.g., Adam), we update the model parameters with our computed gradients.</p>

<d-code block="" language="python">
class VPGLearner:
    def sgd_step(state: TrainingState, minibatch: Batch):
      observations = minibatch.observations
      actions = minibatch.actions
      advantages = minibatch.advantages
      target_values = minibatch.target_values
      behavior_values = minibatch.behavior_values
      behavior_log_probs = minibatch.behavior_log_probs
      key, sub_key = jax.random.split(state.random_key)

      loss_grad, metrics = vpg_loss_grad(
          state.params,
          observations,
          actions,
          advantages,
          target_values,
          behavior_values,
          behavior_log_probs,
          state.value_mean,
          state.value_std,
          sub_key,
      )

      # Apply updates
      loss_grad = jax.lax.pmean(loss_grad, axis_name=pmap_axis_name) # Broadcast to devices
      updates, opt_state = optimizer.update(loss_grad, state.opt_state)
      model_params = optax.apply_updates(state.params, updates)

      state = state._replace(params=model_params, opt_state=opt_state, random_key=key)

      return state, metrics
</d-code>

<h2 id="putting-it-together">Putting it together</h2>

<p>The remainder of the code is relatively boilerplate as far as reinforcement learning goes. In short, <code class="language-plaintext highlighter-rouge">sgd_step()</code> is run over the batch (collected trajectories $\mathcal{D}_k$) in a loop of minibatches with <code class="language-plaintext highlighter-rouge">jax.lax.scan()</code>. This happens on each device (GPU), by broadcasting to all available devices with <code class="language-plaintext highlighter-rouge">jax.pmap()</code>. It’s really as simple as it sounds, once you understand the syntax!</p>

<p>The outer RL loop then starts again, collecting trajectories $\tau$ with the actor, which may update its policy from the learner, which acts as a network parameter server to all running actors. Trajectories are stored with DeepMind’s Reverb, and accessed by the learner each learner step by accessing Reverb through a Python iterator interface. You can think of Reverb as a fancy FIFO implemented as a table.</p>

<p>For more information about this information flow, and also how it is easily extended to multiple actors and learners, potentially across multiple machines, check out DeepMind’s Acme library on <a href="https://github.com/deepmind/acme">GitHub</a>.</p>

<h2 id="performance-and-improvements">Performance and improvements</h2>

<p>Unfortunately, VPG does not perform very well at all. There are reasonable additions and improvements which weren’t discussed in this post which are typically added to any reinforcement learning algorithm to improve its robustness, mainly boiling down to normalization of inputs to the loss functions used for calculating gradients. You’ll see these have been added to the code. And yet, even on a simple environment like gym:Pendulum-v1, we see practically no learning. Check out these <a href="https://spinningup.openai.com/en/latest/spinningup/bench.html">benchmarks</a> from OpenAI; VPG’s performance is pitiful compared to modern algorithms.</p>

<p>So this begs the question… why on earth did we go through all these pesky details if it was all for nothing?</p>

<h3 id="vpg-is-a-stepping-stone">VPG is a stepping stone!</h3>

<p>It was not a waste; we’ll see why. Recall all we’ve learned up to this point: we’ve…</p>
<ul>
  <li>Understood policies: acting from state</li>
  <li>Collected trajectories with said policies</li>
  <li>Understood future return, value functions, and advantage estimation</li>
  <li>Understood how to define networks in JAX/haiku</li>
  <li>Understood how to take loss gradients in JAX and apply gradient updates</li>
  <li>Learned the policy and value function in an RL loop</li>
</ul>

<p>Let’s take our VPG code which uses all of the concepts we’ve discussed, and make one single change. <br />
The line which computes the policy loss (the policy gradient without the grad) looks like this:</p>

<d-code block="" language="python">
policy_loss = -(policy_log_probs * advantages).mean()
</d-code>

<p>Let’s just swap this out with</p>
<d-code block="" language="python">
rhos = jnp.exp(policy_log_probs - behavior_log_probs)
policy_loss = rlax.clipped_surrogate_pg_loss(
  rhos, advantages, clipping_epsilon=0.2)
</d-code>

<p>where the <code class="language-plaintext highlighter-rouge">behavior_log_probs</code> are the same term as <code class="language-plaintext highlighter-rouge">policy_log_probs</code>, only they were calculated earlier, with an older version of the policy network. Let’s skip the details of the <code class="language-plaintext highlighter-rouge">rlax.clipped_surrogate_pg_loss()</code> function.</p>

<p>With this simple change, we go from no performance to great performance on Pendulum-v1.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/vpg_ppo_curves-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/vpg_ppo_curves-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/vpg_ppo_curves-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/vpg_ppo_curves.png" class="img-fluid rounded" width="auto" height="auto" title="VPG vs. PPO" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Swap out the loss term, and poof, it works! (Green is PPO, pink is VPG, run on Pendulum-v1)</figcaption>

</figure>

<h3 id="a-stepping-stone-to-ppo">…a stepping stone to PPO</h3>

<p>Wow! How on earth did this happen? Well, this isn’t VPG anymore, it’s PPO, or Proximal Policy Optimization, a ubiquitous and effective algorithm which many practitioners use as a baseline for all other algorithms. (Also, it does take a few more lines to calculate <code class="language-plaintext highlighter-rouge">behavior_log_probs</code>.) There are many, many resources from which to learn PPO, which I’ll leave you to find on your own.</p>

<p>Suffice it to say that by swapping out our policy loss term with a “surrogate loss” which doesn’t allow for too-large policy updates, we can stabilize training and be off to the races. The important takeaway here is that you are now 90% of the way to understanding all policy gradient algorithms, and you’re not doing too bad with all actor-critic algorithms either. Things do get a little more subtle from here on, but you have the foundation. Pat yourself on the back!</p>

<hr />

<p>I recommend you take a look to see the rest of the code that pieces this together <a href="https://github.com/kjabon/vpg_acme_jax">here</a>. As this has merely been modified from the Acme PPO implementation, the obvious next step would be to take a look at that and other resources explaining PPO, and perhaps the <code class="language-plaintext highlighter-rouge">rlax</code> surrogate loss function above. As your path continues, you’ll find that even PPO has a lot of room for improvement. Good luck!</p>

<p>Thank you for reading, and come again soon!</p>]]></content><author><name>Kenneth Jabon</name></author><category term="rl" /><summary type="html"><![CDATA[A simple implementation using Acme]]></summary></entry><entry><title type="html">A Bit on Baselines</title><link href="https://kjabon.github.io/blog/2023/Baseline/" rel="alternate" type="text/html" title="A Bit on Baselines" /><published>2023-05-02T00:00:00-05:00</published><updated>2023-05-02T00:00:00-05:00</updated><id>https://kjabon.github.io/blog/2023/Baseline</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/Baseline/"><![CDATA[<p>This article discusses baselines in the context of <a href="/blog/2023/VPG">vanilla policy gradient</a>, more specifically when discussing <a href="/blog/2023/VPG2/#generalized-advantage-estimation">generalized advantage estimation</a>. In that post, we talk more about the intuition of baselines. This could of course be applied to many other algorithms.</p>

<hr />

<p>Supposing you’ve gone through the intuition of baselines…</p>

<p>A more satisfying explanation for why subtracting a baseline to come up with Advantage works is usually swept under the rug or banished to a “see this reference” link, but as it’s not perfectly intuitive, I’d like to at least take a whack at explaining it.</p>

<p>To begin with, we need to consider at a higher level what we’re trying to accomplish. Consider the policy gradient we’ve derived so far:</p>

\[\hat{g}=\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} (r_{t+1} + \gamma V(s_{t+1}))\]

<p>This term is the sum over time steps in a <strong>single</strong> trajectory \(\tau\).</p>

<p>Now for the new part. The above term actually gives us a sample of the optimal policy gradient for <strong>all</strong> possible trajectories. What we would ideally like is the expectation over all trajectories:</p>

\[\mathbb{E}\left[\hat{g}\right]=\mathbb{E}\left[\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} (r_{t+1} + \gamma V(s_{t+1})) \right]\]

\[=\int_{\mathcal{T}} \sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} (r_{t+1} + \gamma V(s_{t+1})) d\tau\]

<p>where $\mathcal{T}$ is the set of all possible trajectories \(\tau\).</p>

<p>In order to calculate the true optimal policy gradient, we would need to collect every possible trajectory, which of course is intractable for any environment of more than trivial size. Instead, we collect a set \(\mathcal{D}\) of some batch size (say, 128) trajectories \(\tau\), and compute a sampled (Monte Carlo) expectation:</p>

\[\approx \frac{1}{\left|\mathcal{D}\right|}\sum_{\tau\in\mathcal{D}}\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} (r_{t+1} + \gamma V(s_{t+1}))\]

<p>Now, our goal is to reduce the variance of this policy gradient estimate. Let’s revert to the integral form:</p>

\[\int_{\mathcal{T}} \sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} (r_{t+1} + \gamma V(s_{t+1})) d\tau\]

<p>and encapsulate everything into an integral of one function:</p>

\[\int_{\mathcal{T}}f(\tau)\]

<p>Now add and subtract the integral of a similar function we can evaluate, \(\phi \approx f\):</p>

\[\int_{\mathcal{T}}f(\tau) = \int_{\mathcal{T}}\left(f(\tau)-\phi (\tau)\right)+\int_{\mathcal{T}}\phi(\tau)\]

<p>Instead of estimating \(\int_{\mathcal{T}}f(\tau)\) (our original problem), we estimate \(\int_{\mathcal{T}}\left(f(\tau)-\phi (\tau)\right)\). Now</p>

\[\text{Var}(f-\phi) = \text{Var}(f) - 2 \text{Cov}(f,\phi)+\text{Var}(\phi)\]

<p>Since \(\phi\approx f\), \(\text{Cov}(f,\phi)\) is positive. If \(- 2 \text{Cov}(f,\phi)+\text{Var}(\phi)\) is negative, \(\text{Var}(f-\phi) \lt \text{Var}(f)\), so \(\int_{\mathcal{T}}\left(f(\tau)-\phi (\tau)\right)\), the new thing we need to estimate, has lower variance. This is exactly what we’re looking for!</p>

<p>Are you beginning to notice the similarity to our punchline above? Since it may not be completely obvious, I’ll make it painfully explicit. Starting with</p>

\[\int_{\mathcal{T}} \sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} (r_{t+1} + \gamma V(s_{t+1})) d\tau\]

<p>we subtract \(V(s_t)\):</p>

\[\int_{\mathcal{T}} \sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} (r_{t+1} + \gamma V(s_{t+1})-V(s_t)) d\tau\]

\[=\int_{\mathcal{T}} \sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} (r_{t+1} + \gamma V(s_{t+1})) d\tau - \int_{\mathcal{T}} \sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} (V(s_t)) d\tau\]

\[=\int_{\mathcal{T}} f(\tau) - \int_{\mathcal{T}} \phi(\tau)=\int_{\mathcal{T}} f(\tau) - \phi(\tau)\]

<p>Now just convert all that to the Monte Carlo, sampled-over-trajectories version, and Bob’s your uncle. This is why subtracting the value function from the n-step return reduces the variance of the samples from all trajectories.</p>

<p>One last thing. We added \(\int_{\mathcal{T}}\phi(\tau)\) so that we were still calculating \(\int_{\mathcal{T}}f(\tau)\), so shouldn’t we add \(V(s_t)\) back in somehow? Turns out you don’t have to; subtracting \(\phi\) has no effect on the bias of the Monte Carlo estimate, i.e. the expectation over trajectories.</p>

<p>I will banish <em>some</em> things to a “see this reference” link. If any of this seems suspect, take a look <a href="https://www.jmlr.org/papers/volume5/greensmith04a/greensmith04a.pdf">here</a>. See equation 7 in that reference for why we can subtract a baseline without affecting the bias of the estimate. If you’re still not convinced this reduces variance, take a look <a href="https://en.wikipedia.org/wiki/Control_variates">here</a> and perhaps also at section 5.5 (p.59) in <a href="http://www.cs.fsu.edu/~mascagni/Hammersley-Handscomb.pdf">this</a> book on Monte Carlo methods. Pat yourself on the back, now you know “advantage” is just a clever rebranding of control variates.</p>]]></content><author><name>Kenneth Jabon</name></author><category term="rl" /><summary type="html"><![CDATA[Reducing variance and taking names]]></summary></entry><entry><title type="html">Continuous Log Likelihood</title><link href="https://kjabon.github.io/blog/2023/ContinuousLogLikelihood/" rel="alternate" type="text/html" title="Continuous Log Likelihood" /><published>2023-04-24T00:00:00-05:00</published><updated>2023-04-24T00:00:00-05:00</updated><id>https://kjabon.github.io/blog/2023/ContinuousLogLikelihood</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/ContinuousLogLikelihood/"><![CDATA[<p>Strangely, I couldn’t find anyone else who had done the math for the log likelihoods of multivariate normal distributions online. Let’s be good scientists and double check anything that isn’t obvious, especially if it’s widely accepted.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/MultivariateNormal-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/MultivariateNormal-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/MultivariateNormal-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/MultivariateNormal.png" class="img-fluid rounded" width="auto" height="auto" title="From wikipedia" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
A multivariate normal distribution.
</div>

<hr />

<p><a href="https://en.wikipedia.org/wiki/Multivariate_normal_distribution">PDF</a> of a multivariate normal distribution:</p>

\[f(x) = \frac{1}{\left(2\pi\right)^{\frac{k}{2}}\left| \Sigma \right|^{\frac{1}{2}}}\exp\left(-\frac{1}{2}\left(x-\mu\right)^T\Sigma^{-1}\left(x-\mu\right)\right)\]

<p>where \(\Sigma\) is the (diagonal) covariance matrix, and \(\mu\) is the column vector of means for each dimension. \(x\) is the column vector of positions along the probability density function whose likelihoods we’re interested in.</p>

<p>Take the log of both sides:</p>

\[\log \left(f(x)\right) = \log \left(\frac{1}{\left(2\pi\right)^{\frac{k}{2}}\left| \Sigma \right|^{\frac{1}{2}}}\exp\left(-\frac{1}{2}\left(x-\mu\right)^T\Sigma^{-1}\left(x-\mu\right)\right)\right)\]

<p>Annihilate the exponent and use a log identity to add the multiplied terms:</p>

\[= \log \left(\frac{1}{\left(2\pi\right)^{\frac{k}{2}}\left| \Sigma \right|^{\frac{1}{2}}}\right)  -\frac{1}{2}\left(x-\mu\right)^T\Sigma^{-1}\left(x-\mu\right)\]

<p>Let’s look at the left term first:</p>

\[\log \left(\frac{1}{\left(2\pi\right)^{\frac{k}{2}}\left| \Sigma \right|^{\frac{1}{2}}}\right)\]

<p>Use some log identities:</p>

\[=-\frac{k}{2}\log 2\pi - \log \left|\Sigma \right|^{\frac{1}{2}}\]

<p>The determinant of a diagonal matrix is the product of its diagonal elements:</p>

\[=-\frac{k}{2}\log 2\pi - \log \prod_{i=1}^{k}\Sigma_{i,i}^{\frac{1}{2}}\]

<p>The log of a product of terms is the sum of the logs of those terms (and applying the element-wise square root, and some rearranging):</p>

\[=-\frac{1}{2}\left(k\log 2\pi +  \sum_{i=1}^{k}2\log\sigma_{i}\right)\]

<p>Halfway done. Now let’s look at the right term of the main equation:</p>

\[-\frac{1}{2}\left(x-\mu\right)^T\Sigma^{-1}\left(x-\mu\right)\]

<p>Ok, time to break out the linear algebra. Notice here \(n\) stands for the number of dimensions (in our RL application, the dimensionality of the continuous action space).</p>

\[=-\frac{1}{2}\begin{bmatrix} x_1-\mu_1, &amp; x_2-\mu_2, &amp; \dots, &amp; x_n-\mu_n\end{bmatrix} \;
\begin{bmatrix} 1/\sigma^2_1 &amp; &amp; &amp;\\&amp; 1/\sigma^2_2 &amp;&amp;\\ &amp;&amp;\ddots &amp;\\ &amp;&amp;&amp;1/\sigma^2_n \end{bmatrix} \;
\begin{bmatrix} x_1-\mu_1 \\ x_2-\mu_2 \\ \vdots \\ x_n-\mu_n\end{bmatrix} \;\]

\[=-\frac{1}{2}\begin{bmatrix} (x_1-\mu_1)/\sigma^2_1 &amp; (x_2-\mu_2)/\sigma^2_2 &amp; \dots &amp; (x_n-\mu_n)/\sigma^2_n\end{bmatrix} \;

\begin{bmatrix} x_1-\mu_1 \\ x_2-\mu_2 \\ \vdots \\ x_n-\mu_n\end{bmatrix} \;\]

\[=-\frac{1}{2}\sum_{i=1}^k\frac{(x_i-\mu_i)^2}{\sigma_i^2}\]

<p>Adding the first and second terms gives the result:</p>

\[\log \left(f(x)\right)=-\frac{1}{2}\left(k\log 2\pi+\sum_{i=1}^k\left[\frac{(x_i-\mu_i)^2}{\sigma_i^2}+2\log\sigma_i\right]\right)\]

<hr />

<p>I recommend going through it yourself! After all:</p>

<blockquote>
  <p>What I cannot create, I do not understand.</p>
</blockquote>

<p>-Richard Feynman</p>

<p>And one more for the road:</p>

<blockquote>
  <p>In mathematics you don’t understand things. <br />
You just get used to them.</p>
</blockquote>

<p>-John Von Neumann</p>]]></content><author><name>Kenneth Jabon</name></author><category term="rl" /><summary type="html"><![CDATA[A quick and dirty derivation]]></summary></entry><entry><title type="html">Intro to Vanilla Policy Gradient</title><link href="https://kjabon.github.io/blog/2023/VPG/" rel="alternate" type="text/html" title="Intro to Vanilla Policy Gradient" /><published>2023-04-14T00:00:00-05:00</published><updated>2023-04-14T00:00:00-05:00</updated><id>https://kjabon.github.io/blog/2023/VPG</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/VPG/"><![CDATA[<p>Vanilla policy gradient is one of the simplest reinforcement learning algorithms. It should serve to form the theoretical foundation for many following policy-based, online algorithms. If you’re more interested in offline algorithms, I recommend you start with TD-learning and deep Q-learning. See chapter 6 of <a href="http://incompleteideas.net/book/the-book-2nd.html">Sutton and Barto</a> for that.</p>

<p>This post assumes you’re familiar with how Markov Decision Processes work, and how the reinforcement learning problem is set up. For a guide to that, see <a href="/blog/2023/RL">here</a>.</p>

<h2 id="a-birds-eye-view">A bird’s eye view</h2>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/eagle-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/eagle-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/eagle-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/eagle.jpg" class="img-fluid rounded" width="auto" height="auto" title="bird" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>Let’s describe what happens in a full iteration of the loop before diving in.</p>

<p>First, we collect a batch of trajectories \(\tau\) by allowing our current policy \(\pi\) (represented by a neural network) to unfold in the environment. We collect a full batch because, in general, the policy does not output actions \(a\), but a probability distribution over actions given the current state \(s\): \(\pi(a\vert s)\). To get to the next timestep in our trajectory, we select an action \(a\) by sampling from this distribution. Randomness may also occur as the result of the environment itself, so all in all we want to get plenty of samples to come up with an accurate, representative set of trajectories \(\tau\).</p>

<details><summary>Sampling is great for blind exploration, but…</summary>
<p>Eventually sampling doesn’t do the job any more. At some point we’ll want to exploit what we know and make our way to better states to be able to learn from there, so we’ll just pick the best known action instead of sampling. The amount of “greedy action selection” will be scheduled to increase over time, to progress from pure exploration of the state space to pure exploitation of the policy.</p>
</details>

<p>Now, if a policy gains an above-average sum of rewards in a particular trajectory, we will nudge the policy \(\pi\) in the direction of the actions \(a\) (given their respective states, \(\vert s\)) which resulted in this trajectory. Conversely, if a trajectory comes with a below-average sum of rewards, we nudge the policy away from taking those actions. This sum is known as the <strong>return</strong> for a trajectory<d-footnote>This isn't the full return yet, we'll make it more general in a bit.</d-footnote>:</p>

\[R(\tau)=\sum_{t=0}^{T}r_t\]

<p>If any of this seems unclear, you’ll want to start with the <a href="/blog/2023/RL">previous</a> post.</p>

<p>To estimate the return at each time step with low variance, we employ generalized advantage estimation (GAE). We train a neural network to represent the value function \(V\), which is incorporated into our advantage term \(A\). This will be discussed in the <a href="#generalized-advantage-estimation">section on GAE</a>.</p>

<p>Now, every time we go through our loop, we have a better estimate of what a good trajectory looks like (by training the value function \(V\)), and a better idea of what actions to take to get good trajectories (by training the policy \(\pi\)). This glosses over some details, which we’ll get into in just a bit.</p>

<p>Take a glance over the simplified pseudocode for the algorithm, then let’s get cracking!</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pseudoPseudo.svg-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pseudoPseudo.svg-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pseudoPseudo.svg-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pseudoPseudo.svg" class="img-fluid rounded" width="auto" height="auto" title="Algorithm 0" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<h2 id="breaking-down-the-policy-gradient">Breaking down the policy gradient</h2>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/jackhero-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/jackhero-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/jackhero-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/jackhero.jpg" class="img-fluid rounded" width="auto" height="auto" title="jack" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>The policy gradient is calculated as follows:</p>

\[\hat{g}_{k}=\frac{1}{\left\vert \mathcal{D}_k \right\vert }\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} \hat{V}_t\]

<p>Let’s break this down to gain an understanding of each term, working from the inside out. Let’s start with the policy:</p>

\[\pi_\theta (a_t\vert s_t)\]

<p>which is the function which outputs the probability distribution of all possible actions \(a_t\) at time \(t\) in the trajectory \(\tau\), given the state \(s_t\) at time \(t\). To bring it back to earth, think of this as \(y = f(x,\theta)\).</p>

<details><summary>O_O</summary>
<p>If this is making you feel like a deer in headlights, don’t worry. Let’s be more explicit. \(\theta\) parameterizes our function \(f\), and then we evaluate our function on \(x\). What do I mean? If I had a function \(y = f(x, \theta) = mx + b\), I parameterize this function with \(m\) and \(b\) (represented by the vector \(\theta = [m,b]\)), and then evaluate it on \(x\). In the case of a neural net, \(\theta\) is instead a matrix of numbers (which can be flattened into a vector \([a_1,a_2,a_3,...,a_n]\)) representing its weights and biases.</p>

</details>

<ul>
  <li>If we have a discrete action space, this is a function which outputs a vector with a logit for each possible action (in other words, a categorical probability distribution).</li>
  <li>If the action space is continuous, this is a function which outputs the mean \(\mu\) and standard deviation \(\sigma\) representing the action’s probability distribution. <d-footnote> If you're worried about flexibility, it turns out you can also output arbitrary combinations of these, but we won't consider that case here.</d-footnote></li>
</ul>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Fig0_ManimCE_v0.17.3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Fig0_ManimCE_v0.17.3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Fig0_ManimCE_v0.17.3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Fig0_ManimCE_v0.17.3.png" class="img-fluid rounded" width="auto" height="auto" title="nn outputs" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p><strong>Take the log</strong> of the policy:</p>

\[\log\pi_\theta (a_t\vert s_t)\]

<p>Remember, all that is happening is we’re taking the log of a function, i.e., \(g(x, \theta) = \log(f(x, \theta))\).<br />
<br /></p>

<p><strong>Take the gradient</strong> of this with respect to the policy parameters \(\theta\).</p>

\[\nabla_\theta \log\pi_\theta (a_t\vert s_t)\]

<p>Simply, the gradient of a function, i.e.  \(\nabla g(x, \theta)\) with respect to \(\theta\).<br />
<br /></p>

<p><strong>Evaluate the policy</strong> given the current policy parameters \(\theta_k\):</p>

\[\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\]

<p>Or, we perform inference with the current model on the current state of the environment.</p>

<details><summary>Numerical considerations</summary>
<p>Let’s take a step back. If we are used to working out derivations with pencil and paper, the order in which I presented the last few steps should not start sounding any alarms.<d-footnote>Assuming you made it through Calculus in one piece. If not, don't worry. Go ahead and take or re-take Calculus, and then come back. You can do it, I promise. If that's too much hassle, luckily we have autodiff which means you can summarily forget about this derivation; so just keep reading with a glazed look for a couple more paragraphs. </d-footnote></p>

<p>Normally in this case one would take the derivative of the symbolic function, then evaluate that to get the derivative at the point of interest, or a similar method of your choice.</p>

<p>However, there are two differences when doing this numerically on your computer; having to do with the derivative and the log.</p>

<h3 id="for-the-derivative">For the derivative:</h3>
<p>Instead, we just say “hello, new best friend, the autodiff function <d-footnote>For example, jax.grad()</d-footnote>! I’m going to run the code which evaluates \(\log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\) for an entire batch of states in one vectorized/parallelized operation, across whatever computational resources I have available. Can you please give me \(\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\)?” And our new best friend is happy to oblige.</p>

<p>How does our new friend work? I’ll mostly defer to good explanations elsewhere, for example <a href="https://github.com/HIPS/autograd/blob/master/docs/tutorial.md#whats-going-on-under-the-hood">here</a> and <a href="https://en.wikipedia.org/wiki/Automatic_differentiation">here</a>. The gist is that first, it keeps track of the computation graph of whatever functions you want to differentiate, as you execute them, from state \(s\) and parameters \(\theta\) all the way to \(\log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\). Then, it uses this graph to multiply its way through the chain rule, resulting in the gradient(s) you want.</p>

<h3 id="for-the-log">For the log:</h3>

<p><strong>For categorical distributions</strong>, it’s very simple.</p>

<p>We configure our neural network output to be logits, and later follow this with a softmax to convert to probabilities. We then simply compute the log of these values: \(\log \left[ P_\theta (s)\right]\).</p>

<p>Since this is a vector, to get the log probability for the action of interest \(a\), (\(a\) will be an integer from 0 to n-1, n being the dimensionality of the actions), we simply grab the \(a\)‘th value from this vector. In other words, we compute \(\log \left[ P_\theta (s)\right]_a\). <d-footnote>Thank you to ALPH2H on the RL discord for pointing out a discrepancy here.</d-footnote></p>

<p><strong>For continuous distributions</strong>, it’s a little trickier.</p>

<p>Remember we usually represent the probability distribution in the continuous case as a multivariate normal distribution. To keep things simple, we actually use a diagonal covariance matrix, rather than the full covariance matrix, so each dimension of the action can be represented by a single standard deviation. This way, we only need to output one mean and one standard deviation per dimension, and calculating the log of the distribution also becomes much easier. Now, how do we take the log of a (diagonal) normal distribution?</p>

<p>Like this!</p>

\[\log\pi_\theta (a\vert s) = -\frac{1}{2}\left(n\log 2\pi+\sum_{i=1}^n\left[\frac{(a_i-\mu_i)^2}{\sigma_i^2}+2\log\sigma_i\right]\right)\]

<p>where n is the dimensionality of the action space. If you’re not convinced (I wasn’t), <a href="/blog/2023/ContinuousLogLikelihood/">here’s a derivation</a>.</p>

<p>Our neural network spits out a vector each of \(\mu\) and \(\sigma\), one element for each action dimension. With these values, we sample from a diagonal MVN distribution, yielding a vector of actions \(a\). Then, the above equation describes the log likelihoods of that vector of actions.</p>

<p>See <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#stochastic-policies">here</a> for reference.</p>

</details>

<p>Finally, <strong>summing this term</strong> over all time steps in our current trajectory gives us…</p>

<h3 id="the-meat-of-vpg">The meat of VPG</h3>

\[\sum_{t=0}^{T}\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\]

<p>A <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient">brief derivation</a> results in this full term, the grad log of the current policy. This term is the gradient of the probability of the current trajectory with respect to the policy parameters, evaluated using the current parameters. For that reason, let’s call this the probability gradient. (\(\nabla_\theta P(\tau)\), where \(\tau\) is the current trajectory).</p>

<p>In a moment, we’re going to add the \(V\) term we introduced in our bird’s eye view. Let’s review what we have so far without it: the probability gradient \(\nabla_\theta P(\tau)\).</p>

<ul>
  <li>Without the sum over time steps (\(\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\)), this tells us how much the probability distributions of our actions change, given a small change in each of our neural network parameters \(\theta\).</li>
  <li>With the sum over time steps (\(\sum_{t=0}^{T}\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\) ), it instead tells us how much the probability of that sequence of time steps (the trajectory \(\tau\)) changes, again for a small change in our parameters \(\theta\).</li>
</ul>

<p>This is great: if we want to change the probability of a particular trajectory (\(P(\tau)\)), we have the information to do that! (\(\nabla_\theta P(\tau)\))</p>

<p>The main remaining question is this: do we want our current trajectory \(\tau\) to happen more or less often? Well, I think we can agree, we want to make good trajectories happen more often, and bad trajectories happen less often. So how good (or bad) is our current trajectory?</p>

<h2 id="learning-from-rewards">Learning from rewards</h2>

<p>The return \(R\) of a trajectory \(\tau\) is defined as the discounted sum of rewards obtained in that trajectory. The higher the return, the better the trajectory, and vice-versa. See the <a href="/blog/2023/RL">RL post</a> for an intro to this concept.</p>

<p>Following directly from the policy gradient discussion above: we now know not only how to change a trajectory’s probability using \(\nabla_\theta P(\tau)\), but also whether to make it more or less probable: each probability gradient in the sum is weighted by the return \(R(\tau)\). That is, we have:</p>

\[\hat{g}=\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} R\]

<p>which, again, tells us both how to change the probability of actions leading to the trajectory \(\tau = (s_0, a_0, r_1), (s_1,a_1,r_2), (s_2, a_2, r_3),…\), and now that we have the weight \(R(\tau)\), we know whether to make the action \(a_t\vert s_t\) more or less likely to happen. This summed over all time steps gives us the full policy gradient term.</p>

<h3 id="congrats-youve-made-it-to-the-policy-gradient">Congrats! You’ve made it to the policy gradient!</h3>

<p>If you stopped here, this rudimentary policy gradient would fit into our algorithm above, and that would be able to solve simple environments, albeit a little inefficiently. A golden retriever might eventually be trained to drive a car, but man, is it going to be hard going. And you know that he’s going to stop for treats and crash into mailmen.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/goldenRetriever-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/goldenRetriever-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/goldenRetriever-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/goldenRetriever.png" class="img-fluid rounded" width="auto" height="auto" title="watered down vanilla ice cream" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
He's trying, give him credit.
<d-footnote>From DALL-E</d-footnote>
</div>

<p>No, we can do better than this. Take a breather, then let’s extend this picture.</p>

<hr />

<p>Well, this is about a 15 minute read (or more) already.</p>

<p>Let’s wrap this up in the <a href="/blog/2023/VPG2">next post</a>!</p>]]></content><author><name>Kenneth Jabon</name></author><category term="rl" /><summary type="html"><![CDATA[Theory and intuition behind one of our introductory algorithms]]></summary></entry><entry><title type="html">Intro to Vanilla Policy Gradient, continued…</title><link href="https://kjabon.github.io/blog/2023/VPG2/" rel="alternate" type="text/html" title="Intro to Vanilla Policy Gradient, continued…" /><published>2023-04-14T00:00:00-05:00</published><updated>2023-04-14T00:00:00-05:00</updated><id>https://kjabon.github.io/blog/2023/VPG2</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/VPG2/"><![CDATA[<p>This post is a direct continuation of <a href="/blog/2023/VPG">part 1</a> of introducing vanilla policy gradient. Start there!!</p>

<p>From the most basic possible working version of the algorithm introduced in that post, we now extend VPG with many of the foundational “tricks” in RL to make it into a useable algorithm.</p>

<hr />

<p>…Now, we can do better than the <em>very</em> simple algorithm presented in the last post. Take a breather, then let’s take a step back and think.</p>

<h2 id="future-return">Future return</h2>

<p>If I take an action, what parts of the trajectory will this affect? I’m not currently in possession of a time machine, so any action I take will not affect the past.<d-footnote>History may be written by the victor, but recorded events do not necessarily reflect what actually transpired.</d-footnote> And of course if I take an action, that will carry me to the next time step, so I cannot affect the present either. I can only affect the future.</p>

<p>Therefore, the part of the trajectory \(\tau\) (and thus, the rewards \(r_t\)) that any action has bearing on is the <strong>remainder of the trajectory</strong>, from the next time step to the end of the episode. Let’s call the discounted sum of <em>future</em> rewards “<em>future return</em>” \(R^f\). Our past and present rewards only serve to add noise, or variance, to our estimate of how good the action \(a_t\) is. This randomness slows down our training at best, and may reduce final performance.</p>

<details><summary>A note on terminology</summary>
<p>This set of rewards is sometimes known as the “rewards-to-go.” This term always struck me as kind of confusing, maybe because “to-go” isn’t as precise as I’d like. About half the time I see it, I think “‘to-go?’ Where are the rewards going? Oh, you mean ‘to-go’ as in what we have to go, or remaining.” Let’s avoid any temporary confusion and use “future return.”</p>
</details>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/FigFutureReward_ManimCE_v0.17.3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/FigFutureReward_ManimCE_v0.17.3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/FigFutureReward_ManimCE_v0.17.3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/FigFutureReward_ManimCE_v0.17.3.png" class="img-fluid rounded" width="auto" height="auto" title="future reward" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Rewards r (in red) are emitted at each time step, upon transitioning to a new state s. Future return R for a time step is the (discounted) sum of all future rewards.</figcaption>

</figure>

<p>Let’s be as explicit about this as possible.</p>

<p>Remember our trajectory \(\tau = (s_0, a_0, r_1), (s_1,a_1,r_2), (s_2, a_2, r_3),…\). Assume \(T\) time steps \(t\) in a trajectory \(\tau\); \(t \in \left[0,T\right)\), or \(t \in \left[0,T-1\right]\) if you prefer. Now consider the future return \(R^f_t\) at various time steps:</p>

<ul>
  <li>\(R^f_0\), i.e. \(R^f\) for the first time step (\(t=0\)) in a trajectory, contains all the rewards in that trajectory. Remember the first reward is \(r_1\).</li>
  <li>\(R^f_1\) contains all rewards, except the first reward: \(r_2\) onwards.</li>
  <li>\(R^f_{T-1}\)(the final time step) contains no rewards, because there’s no future!</li>
  <li>\(R^f_{T-2}\) contains only the final reward.</li>
</ul>

<p>In other words,</p>

\[R^f_{t} = \sum_{t'=t+1}^{T}\gamma^{t'-t-1}r_{t'}\]

<p>where T is the total number of time steps in an episode (and we’ve remembered to include the discounting factor \(\gamma\)).</p>

<h3 id="overwrite-your-pointers-and-vocabulary-yn">Overwrite your pointers and vocabulary? (Y/n)</h3>
<p>With the above argument about not being able to affect the past, it turns out there’s little reason to ever use the original full trajectory return $R$. So take a moment to internalize this concept. From here on out, we’re just going to “overwrite” our previous terminology; any time we say “return” or $R$, we’re talking about this new “future return” or $R^f$. It will be <em>implicit!</em> Let’s modify the policy gradient to use our new understanding of return instead:</p>

\[\hat{g}=\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} R_t\]

<p>It looks the same? Oh yes, but now we understand it differently!</p>

<p>Now, how do we actually calculate \(R_t\) for a particular time step? It seems obvious: simply take the discounted sum of all the future rewards! Well, it’s a bit trickier than that, because I don’t have a time machine that goes forward, either.</p>

<p>The obstacle here is that you never have the future rewards for a time step as you’re playing out that step.  You only have immediate access to the rewards you don’t care about, those in the past. So you’re forced to keep “rolling out” the trajectory to its conclusion, recording all future time steps, and only then you can accurately calculate \(R_t\) for all time steps.</p>

<details><summary>Yeah ok, but how do you calculate it??</summary>
<p>Once you’ve collected the rewards, \(R_t\) is easy enough to calculate in one backward-in-time pass with some simple dynamic programming.</p>

<p>Starting from the final time step, each \(R_t\) equals the reward of the current time step plus the discounted return from the next time step. That is,</p>

\[R_{T} = r_T\]

\[R_{T-1} = r_{T-1} + \gamma r_T\]

\[\vdots\]

\[R_{t} = r_t + \gamma R_{t+1}\]

<p>Now just loop this backward in time.</p>

</details>

<p>Whew. That seems a lot more tedious than we were hoping for, doesn’t it?<d-footnote>If my episode is infinite, it's technically impossible. I can get around the infinite sum by remembering I'm discounting, and truncate my sum at a reasonable time in the future.</d-footnote> Imagine you’re playing soccer for the first time, but the coach says you weren’t allowed to correct your mistakes while you’re playing! No no, only in the time between full games can you reflect on the error of your ways, and think how to improve for next time. Until then, suck it up and keep tripping over your feet. Intuitively, this seems inefficient, if not patently ridiculous.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/effective-soccer-coach-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/effective-soccer-coach-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/effective-soccer-coach-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/effective-soccer-coach.jpg" class="img-fluid rounded" width="auto" height="auto" title="Ponder the error of your ways elsewhere, ye goblins" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">No learning for you!</figcaption>

</figure>

<details><summary>Ok, I’m exaggerating a bit</summary>
<p>Now, modern “offline” algorithms aren’t actually this bad. They usually collect short sequences of transitions rather than full trajectories, and store them in a buffer to be sampled and asynchronously learned from. So the “reflection between games” is happening at all times, and the algorithm is “reflecting on all past games,” so to speak, rather than the most recent experience. In other words, in “online” algorithms the state space currently being learned from is the same that’s currently being explored by the active policy. However in “offline” algorithms the current policy (implicit or explicit) sets the explored state space, but the buffer of historical data encapsulates a larger and potentially different state space. There are pros and cons to each approach.</p>
</details>

<p>I would like my algorithm to be “online,” meaning I would like to avoid having to collect full episodes before knowing what my return \(R_t\) is. If I could do that, I would be able to learn from the reward of every time step in real time, and update my policy accordingly. The smaller the time lag in learning, the faster I can gain information about my new policy, and update it again. So, how to access the return now instead of later?</p>

<h2 id="the-value-function">The value function</h2>

<p>The value function \(V\)’s job is to approximate the return \(R\) at whatever time step \(t\) you need, and to generalize across states. A window into the future!</p>

<p>This is a lot simpler than it may seem. The value function \(V\) answers one question: how good is the state \(s\) to be in? Or, given the state \(s_t\) at time step \(t\), what is the best approximation \(V(s_t)\) of the expected return? With a deep neural network, we can quickly dispatch this question with good old supervised learning.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/FigValueFunction_ManimCE_v0.17.3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/FigValueFunction_ManimCE_v0.17.3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/FigValueFunction_ManimCE_v0.17.3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/FigValueFunction_ManimCE_v0.17.3.png" class="img-fluid rounded" width="auto" height="auto" title="value function" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Input state s, output return R. Do supervised learning on pairs calculated from trajectories.</figcaption>

</figure>

<p>We have a trajectory of states \(s_t\), and at the end of an episode (or the effective time horizon defined by the discounting term \(\gamma\)) we know we’re able to calculate the return \(R_t\) for each time step. We define the loss function for this supervised learning problem as the squared error between the value function \(V\)’s estimate and the actual return:</p>

\[\text{Loss}_{V_t}=\left(V(s_t)-R_t\right)^2\]

<p>and minimize this Loss by regression with some kind of gradient descent, training our neural network to predict the return.</p>

<p>And by sticking \(V_t\) in the place of \(R_t\), we <em>almost</em> have the complete policy gradient:</p>

\[\hat{g}=\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} V_t\]

<p>Now for a few caveats.</p>

<h3 id="bias-and-unstable-exploration">Bias and unstable exploration</h3>
<p>This approximation may only see a subset of the state space at any given time, and it may <em>never</em> see the full state space. We cannot assume it is an <em>unbiased estimator</em> unless it has trained uniformly on data from all parts of the state space, and typically that will only be the case once the training is complete, if it happens at all. In parts of the state space which are relatively less explored, it may be <em>very inaccurate</em>, which could lead to learning what the value function <strong>says</strong> is an optimal policy, but in <strong>reality</strong> is nonsense.</p>

<p>One of the best solutions to this problem is seen in the popular RL algorithm <a href="https://arxiv.org/abs/1707.06347">PPO</a>. Once you’ve understood VPG, you should work your way through <a href="https://spinningup.openai.com/en/latest/algorithms/trpo.html">TRPO</a>, and then <a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html">PPO</a>. These links are decent references, though if you feel this blog post helped you understand VPG, let me know in the comments. If there’s enough interest I’ll continue these tutorials!</p>

<details><summary>The gist of PPO</summary>
<p>PPO’s goal is to avoid veering too far out into unexplored territory too quickly. It allows the value function time to “acclimate” to its new surroundings and give more accurate estimates, and so the policy is always learning from a “good enough” value function. It accomplishes this by limiting the size of the policy update step by clipping a surrogate loss function - but I won’t digress too much on this point, it requires its own post to do it justice.</p>
</details>

<h3 id="variance">Variance</h3>

<p>It also turns out that despite removing the noise from past rewards, this simple solution is still quite high-variance in practice. Because of this, it’s difficult for the actor to learn a good policy and to do so quickly. Luckily there are many battle-hardened techniques for reducing variance. Read on for one such technique!</p>

<h2 id="n-step-return">N-step return</h2>
<p>What is the lowest bias approximation of the return \(R\) for a state? Well, the return itself! That is, the actual sum of rewards that we calculate.</p>

<p>Hol’ up. Calculating \(R\) requires collecting a full trajectory \(\tau\). Aren’t we trying to avoid that, to learn faster?</p>

<p>Indeed we are. So perhaps we can combine the two approaches. What if I use the actual reward in the next time step, and add it to the value function’s approximation of the discounted return from that point on? Aha, a slightly more accurate estimate! This one step return is shown in the following figure and equation:</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/FigNStepReturn_ManimCE_v0.17.3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/FigNStepReturn_ManimCE_v0.17.3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/FigNStepReturn_ManimCE_v0.17.3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/FigNStepReturn_ManimCE_v0.17.3.png" class="img-fluid rounded" width="auto" height="auto" title="one-step return" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">A step towards lower bias. Calculate the 1-step return for a time step from the following reward, and the value function evaluated on the following state.</figcaption>

</figure>

\[G_{t:t+1} = r_{t+1} + \gamma V(s_{t+1})\]

<p>Perhaps we can do better still by using the next <strong>two</strong> real rewards instead of only one. Or the next <strong>three</strong>? Each addition will require a slightly longer delay between taking an action and being able to learn from it, because we need to collect a longer sequence out of the full trajectory. This is known as the <strong>n-step return</strong> \(G\).</p>

\[G_{t:t+2} = r_{t+1} + \gamma r_{t+2}+\gamma^2 V(s_{t+2})\]

\[\vdots\]

\[G_{t:t+n} = r_{t+1} + \gamma r_{t+2}+ \cdots +\gamma^{n-1}r_{t+n}+\gamma^n V(s_{t+n})\]

<p>Now the question becomes: what’s the optimal tradeoff between accuracy and this learning delay? At this point you just need to experiment and see what works best for your problem, but I will tell you this: the answer is somewhere between 100% accuracy and zero delay, as seen in this figure:</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/nStepReturn-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/nStepReturn-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/nStepReturn-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/nStepReturn.png" class="img-fluid rounded" width="auto" height="auto" title="Bootstrapping" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
The performance of n-step return with various value of n, for a random walk task<d-footnote>Figure 7.2 from Sutton and Barto. [http://incompleteideas.net/book/the-book-2nd.html] Buy their book!</d-footnote>. The learning rate $\alpha$ is on the x axis. You can see a minimum error for n=4 in this particular instance. YMMV.
</div>

<p>It also turns out you can do even better by doing a weighted average of all of these options: one, two, three, etc. real rewards, followed by an approximation, and also experimenting to see what the right weighting is. There is a similar tradeoff between accuracy and computation speed, yielding a chart like the one above. Optimizing these hyperparameters is problem dependent.</p>

<h2 id="generalized-advantage-estimation">Generalized Advantage Estimation</h2>
<p>We have a more accurate approximation with our n-step return \(G\). We’ve left out one important piece, though: advantage. Before we add it, let’s talk about why we need it.</p>

<h3 id="variance-reduction">Variance reduction</h3>

<p>The n-step return handily deals with the bias problem in approximating the value function, but doesn’t do much to help the variance. To reduce the variance we need to understand where it comes from.</p>

<p>In any trajectory, there are several sources of randomness contributing to what direction it takes as it’s rolled out:</p>
<ul>
  <li>We may have a random initial state \(s_0\).</li>
  <li>For a given policy \(\pi\) and state \(s_t\), in general we <strong>sample</strong> actions \(a_t\) from a distribution.</li>
  <li>The environment may have probabilistic transitions. For a given state $s$ and action $a$, in general, the resulting transition to the next state $s_{t+1}$ is <em>also</em> sampled from a probability distribution, \(p(s' \vert s, a)\). We swept this one under the rug until now (and will continue to do so for the rest of this post, so don’t worry).</li>
</ul>

<p>Each variation at each time step ultimately leads to the variance of the return \(R\). The longer the trajectory, the more pronounced this effect will become. Therefore, we want to remove the effect of this future variation.</p>

<h3 id="advantage">Advantage</h3>

<p>I’ll start with the punchline: it turns out you can take the expression you want (the n-step return), and subtract another similar expression (a baseline) to reduce the variance of your approximation. Take the following figure as a simple example.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/FigAdvantage_ManimCE_v0.17.3-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/FigAdvantage_ManimCE_v0.17.3-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/FigAdvantage_ManimCE_v0.17.3-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/FigAdvantage_ManimCE_v0.17.3.png" class="img-fluid rounded" width="auto" height="auto" title="baseline" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Left plot shows a cubic function plus noise. Subtracting the cubic function yields the plot on the right, which reduces the variance of the possible values of the function,</figcaption>

</figure>

<p>We’re in the business of using “real-time” approximations, so the n_step return is what we’d like a low-variance estimate of. Consider the 1-step return.</p>

\[r_{t+1} + \gamma V(s_{t+1})\]

<p>The most common choice for a baseline is the value function, so the above becomes:</p>

\[A_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)\]

<p>or more generally,</p>

\[A_t = G_{t:t+n} - V(s_t)\]

<p>Now hold on. This is just the same as we had before. It’s the n-step return, but for some reason we’re subtracting the value function evaluated on the current state. What’s the point?</p>

<p>Intuitively, the advantage at its most <em>basic</em> level answers the question: how much better is the <em>actual</em> return \(R\) better than my estimate \(V(s)\)? In a sense, it’s a measure of surprise. If taking an action \(a_t\) given the state \(s_t\) and transitioning to a state \(s_{t+1}\) is more rewarding than we think that state is in general (\(V(s)\)), then surely we ought to adjust our policy to do that more often.</p>

<p>With the addition of advantage, we have generalized advantage estimation (GAE) in a nutshell.  I’ve focused on intuition here. For more specifics in how these calculations are done, see the <a href="https://arxiv.org/abs/1707.06347">paper</a>; your mental picture should now be organized to have all those formalisms swiftly fall into place.</p>

<p>We can now convert our \(V_t\) to \(A_t\).</p>

\[\hat{g}=\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} A_t\]

<p>It is worth noting that GAE is not limited to on-policy methods; it can be applied anywhere you use a value function, or even a Q function.</p>

<details><summary>Ok, but why does it <em>actually</em> work?</summary>
<p>If the intuition isn’t cutting it for you, see <a href="/blog/2023/Baseline">this brief post</a> about why subtracting a baseline reduces variance.</p>

</details>

<h2 id="batched-training-of-neural-nets">Batched training of neural nets</h2>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cookie-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cookie-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cookie-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cookie.jpg" class="img-fluid rounded" width="auto" height="auto" title="kooky" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>Finally, we sum the policy gradient over a set \(\mathcal{D}\) of collected trajectories \(\tau\), dividing by the number of trajectories \(\vert \mathcal{D} \vert\) to get the sampled mean of the above equation.</p>

<p>Why? Well, to gain a lower variance estimator of the true policy gradient. This should be quite familiar to you if you’ve ever done mini-batched stochastic gradient descent. Many trajectories averaged together will smooth out the noise from any one trajectory, and better represent the optimal policy gradient.</p>

<h3 id="policy-gradient-update">Policy gradient update</h3>

<p>With this, we finally have the full policy gradient!</p>

\[\hat{g}=\frac{1}{\left| \mathcal{D}_k \right|}\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} A_t\]

<p>Where \(k\) is the step index in the training loop.<br />
With this term, we can update the parameters of our policy with SGD:</p>

\[\theta_{k+1} = \theta_{k}+\alpha_k \hat{g}_k\]

<p>or otherwise use an optimizer like Adam.</p>

<h3 id="value-function-update">Value function update</h3>

<p>This is the policy learning taken care of. Now we turn to updating the value function. Recalling its loss function:</p>

\[\text{Loss}_{V_t}=\left(V_\phi(s_t)-\hat{R}_t  \right)^2\]

<p>We add a sum over all time steps in the trajectory, and a sum over all trajectories in our gathered set of trajectories \(\mathcal{D}\):</p>

\[\frac{1}{\left\vert  \mathcal{D}_k \right\vert T}\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\left(V_\phi(s_t)-\hat{R}_t  \right)^2\]

<p>We divide by \(T\) because we’re not doing a sum of log likelihoods like for the policy gradient, but instead are calculating mean squared error.</p>

\[\phi_{k+1}=\arg \min_\phi \frac{1}{\left\vert  \mathcal{D}_k \right\vert T}\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\left(V_\phi(s_t)-\hat{R}_t  \right)^2\]

<p>Standing in for the \(\arg \min\), we can use SGD or Adam to compute the update to the value function parameters \(\phi\).</p>

<p>With each gradient descent step, the value function, on average, better approximates the real sum of rewards. The policy then has a more accurate value function with which to adjust its own actions. Together, they explore the state and action space until a locally optimal policy is reached. Though every step is not guaranteed to improve due to approximation error, even this “vanilla” reinforcement learning algorithm can learn to complete simple tasks.</p>

<p>Now you can put all these ingredients together. Convince yourself that you understand the full algorithm!</p>

<hr />

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pseudoVPG.svg-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pseudoVPG.svg-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pseudoVPG.svg-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pseudoVPG.svg" class="img-fluid rounded" width="auto" height="auto" title="Pseudocode" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>Go over it and see that everything lines up for you.</p>

<hr />

<p>Thanks for reading!</p>

<p>Now we can take a look at an implementation in JAX in the <a href="/blog/2023/VPGJAX">next post</a>.</p>]]></content><author><name>Kenneth Jabon</name></author><category term="rl" /><summary type="html"><![CDATA[Extending the Theory and intuition behind one of our introductory algorithms]]></summary></entry><entry><title type="html">The Reinforcement Learning Problem</title><link href="https://kjabon.github.io/blog/2023/RL/" rel="alternate" type="text/html" title="The Reinforcement Learning Problem" /><published>2023-04-13T00:00:00-05:00</published><updated>2023-04-13T00:00:00-05:00</updated><id>https://kjabon.github.io/blog/2023/RL</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/RL/"><![CDATA[<h1 id="so-you-wanna-do-rl">So, you wanna do RL</h1>
<p>This post is the place to start. RL has successfully defeated grandmasters in Go, Dota 2, and is responsible for training ChatGPT. Here we will lay out the reinforcement learning problem, and in future posts I’ll lay out how various algorithms go about solving it.</p>

<div class="row">
<div class="col">
</div>
<div class="col-8">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/humanfeedbackjump.gif-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/humanfeedbackjump.gif-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/humanfeedbackjump.gif-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/humanfeedbackjump.gif" class="img-fluid rounded" width="auto" height="auto" title="Noodle" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
 <div class="col">
</div>
</div>

<div class="caption">
"Learn some deep reinforcement learning, and you too can train a noodle to do backflip" -Amid Fish
<d-footnote>Learning from human preferences, OpenAI. [https://openai.com/research/learning-from-human-preferences]</d-footnote>
</div>

<p>I briefly talk about policies in this post as an example of a solution, with no mention of TD- or Q-learning, which are equally important. For pedagogical/introductory purposes, policies as are slightly more intuitive and straightforward. Don’t let this scare you away from Q-learning, because it is powerful and eminently learnable! Now, without further ado, let’s jump right in. In the standard setup of the reinforcement learning problem, you have an actor and an environment interacting in a loop.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RLProblem-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RLProblem-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RLProblem-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/RLProblem.png" class="img-fluid rounded" width="auto" height="auto" title="RL Problem" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
Agents act in (or "send actions to") the environment. The environment progresses one time step based on this action, and responds with an observation and reward.
<d-footnote>Figure from Deepmind Acme paper on arXiv. [arXiv:2006.00979]</d-footnote>
</div>

<h3 id="from-the-environments-point-of-view">From the environment’s point of view…</h3>
<p>The environment is always in a well-defined state \(s\), and given some action received by the actor, it will transition to a new state \(s’\) corresponding to that action. When this transition happens, the environment will spit out a reward \(r\) for transitioning from \(s\) to \(s’\): \(r = R(s,a,s’)\). Eventually some special terminal state is reached. We reached our goal or irrevocably failed the task, and our episode ends. At the beginning of each new episode, the environment can be initialized to an initial state \(s_0\).</p>

<p>This environment is a Markov decision process (MDP): go read about those on page 47 of <a href="http://incompleteideas.net/book/the-book-2nd.html">Sutton and Barto</a>.</p>

<h3 id="from-the-actors-point-of-view">From the actor’s point of view…</h3>
<p>The actor will take an observation of the environment’s state, process this information, and output an action to the environment. It will then record the reward \(r\) output by the environment, and continue this loop. This reward is the signifier of “good” or “bad” results from the reinforcement learning actor’s actions.</p>

<p>Following this loop forms a trajectory \(\tau\) made up of the states \(s_t\), actions \(a_t\), and rewards \(r_t\) at each time step \(t\). \((s_0, a_0, r_1), (s_1,a_1,r_2), (s_2, a_2, r_3),…\)</p>

<details><summary>“That looks funny,” I hear you say.</summary>
<p>The reward time step is offset by one for a few reasons. We don’t get any reward for the first time step \(t=0\), i.e. for initializing the state \(s_0\). Why? We haven’t taken any action yet. Also, the reward \(r\) is associated with both state \(s\) and \(s’\), which live in separate time steps. We have to pick a formalism, so we assign the reward to the latter time step \(t+1\), because the environment returns it at the same time as the next state \(s’\). However, we group it with the former tuple in our trajectory \(\tau\), because it’s associated with the action \(a_t\) we took at that time step \(t\). This tuple organization follows an intuitive loop from the actor’s point of view: “observe, act, get a reward,” then repeat.</p>

<details><summary>“But wait,” I hear you say.</summary>
<p>It would make more sense to have a “null” reward \(r_0\) at the beginning which we spit out but don’t do anything with, and form our trajectory like so: <br />
\((r_0, s_0, a_0) (r_1, s_1, a_1),(r_2, s_2, a_2), …\)<br />
The subscripts look nicer, but this doesn’t really end the tuples at natural stopping points.</p>

<p>Often the trajectory is represented without the reward at all, subverting this issue entirely! (Although, this does beg the question of “where do I keep my rewards to learn from?”). Feel free to consider further, but try not to get hung up on this point. Ultimately, we’re representing the same thing, and sometime very soon you will abstract away the whole process. The following example should clear things up.</p>
</details>
</details>
<p><br /></p>

<h1 id="1d-grid-world">1D Grid World</h1>
<h2 id="a-simple-environment">A Simple Environment</h2>

<p>Let’s consider a 1D grid world, where the goal is simply for the actor to be as near as it can to a particular grid space, and x is the target grid space.</p>

\[\begin{array}{|l||c|c|c|c|c|c|c|} 
\hline
\text{x=4} &amp;  &amp; &amp; &amp; &amp; x &amp; &amp; \\ 
\hline  
\text{State} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;5&amp;6\\ 
\hline 
\end{array}\]

<p>Since our goal is to be near to the target space \(x=4\), let’s define a reward function: \(R(s’) = |4-s’| + 4\). Remember \(s’\) is the state we end up in after taking an action.</p>
<details><summary>You dropped this: \(s,a\)</summary>
<p>Well well, you’re a quick learner. Yes in general, the reward is a function of \((s,a,s')\). This simple example only depends on \(s'\).</p>

</details>
<details><summary>Why \(+ 4\)?</summary>
<p>I add the 4 to keep the numbers positive, i.e. a little cleaner, but offsetting the reward function makes no difference to the RL algorithm. I could add or subtract 10,000, and in principle it will still work, especially if you are standardizing the inputs to your neural network.</p>
</details>
<p>We see each grid space take on a reward following this function:</p>

\[\begin{array}{|l||c|c|c|c|c|c|c|} 
\hline
\text{x=4} &amp;  &amp; &amp; &amp; &amp; x &amp; &amp; \\ 
\hline  
\text{State} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;5&amp;6\\ 
\hline 
\text{Reward} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;3&amp;2\\ 
\hline 
\end{array}\]

<p>Now, our actor may start at a random location, but let’s suppose it starts at 0:</p>

\[\begin{array}{|l||c|c|c|c|c|c|c|} 
\hline
\text{x=4}  &amp; o &amp; &amp; &amp; &amp; x &amp; &amp; \\ 
\hline  
\text{State} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;5&amp;6\\ 
\hline 
\text{Reward} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;3&amp;2\\ 
\hline 
\end{array}\]

<p>where o is the location of our actor. Notice here the initialization of the environment state doesn’t spit out a reward. <d-footnote>In RL, we don’t get a reward just for showing up, we get rewards for participation!</d-footnote></p>

<p>Suppose we have 3 actions available to us at a given time step. We can move left, right, or stay put. Encode these actions as -1, 1, and 0 respectively. This environment follows a deterministic state transition, i.e. a left action will always move us left one grid space, and so on. If we bop into a wall, then we stay put.</p>

<p>When we transition to the new state, we obtain the associated reward: \(r = R(s’)\). The goal in the RL problem is defined as maximizing the “return,” or the sum of rewards \(r\) in a trajectory \(\tau\). If you prefer, the trajectory’s return is</p>

\[R(\tau)=\sum_{t=0}^{T}r_t\]

<h2 id="introducing-the-policy">Introducing the Policy</h2>
<p>The actor maintains a policy \(\pi(a\vert s)\). For a given time step \(t\) in the trajectory \(\tau\), this function outputs the probability distribution of all possible actions \(a_t\), given the state \(s_t\). We can see the optimal policy \(\pi(a\vert s)\) which achieves this goal immediately:</p>

\[\begin{array}{|l||c|c|c|c|c|c|c|} 
\hline
\text{x=4}  &amp;  &amp; &amp; &amp; &amp; x &amp; &amp; \\ 
\hline  
\text{State} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;5&amp;6\\ 
\hline 
\text{Reward} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;3&amp;2\\ 
\hline 
\text{Policy} &amp;1 &amp; 1 &amp;1&amp;1&amp;0&amp;-1&amp;-1&amp;\text{1, 0, -1 = right, stay, left}\\ 
\hline 
\end{array}\]

<p>That is, step towards the target, and if you’re on the target, stay put. At the risk of being obvious, let’s show the optimal trajectory following this policy for our actor starting at position 0. Remember a trajectory \(\tau\) follows the form \((s_0, a_0, r_1), (s_1,a_1,r_2), (s_2, a_2, r_3),…\)</p>

\[\begin{array}{|c|c|c|c|c|c|c|l|} 
\hline
 o &amp; &amp; &amp; &amp; x &amp; &amp; &amp;s_0=0, a_0=1 &amp;\text{ initial state; no reward}\\ 
\hline  
  &amp;o &amp; &amp; &amp; x &amp; &amp; &amp;r_1 = 1, s_1=1, a_1=1&amp;\text{ move right}\\ 
\hline  
  &amp; &amp;o &amp; &amp; x &amp; &amp; &amp;r_2 = 2, s_2=2, a_2=1&amp;\text{ move right}\\ 
\hline  
  &amp; &amp; &amp;o&amp; x &amp; &amp; &amp;r_3 = 3, s_3=3, a_3=1&amp;\text{ move right}\\ 
\hline  
 &amp; &amp; &amp; &amp; o &amp; &amp; &amp;r_4 = 4, s_4=4&amp;\text{ terminal state; no action}\\ 
\hline  
\end{array}\]

<p>At this point the actor receives the final reward and state, and notices it has reached the goal/terminal state. No further actions are taken and the episode ends. Our return, or sum of rewards, is</p>

\[R(\tau)=\sum_{t=0}^{T}r_t = r_1+r_2+r_3+r_4 = 10\]

<details><summary>Trajectory? Episode?</summary>
<p>For the purposes of this post, the trajectory is just the full episode. In general, a trajectory is any contiguous subsequence of an episode, while an episode is the full sequence from initial state \(s_0\) to terminal state \(s_{T-1}\) for an episode of length \(T\), if it ends at all.</p>

</details>

<details><summary>What if: bad grid spaces?</summary>
<p>We also could have put “pitfalls” at each end, such that the actor would receive a large negative reward, and the episode could end then, as well. Clearly an optimal policy would involve avoiding these “bad” spaces.</p>

</details>

<p>We’ll leave representing and learning the policy \(\pi\), which can be handled by all manner of RL algorithms, to <a href="/blog/2023/VPG">future posts</a> and external <a href="https://spinningup.openai.com/en/latest/user/algorithms.html">resources</a>. This post’s purpose is merely to lay out the problem to be solved.</p>

<p>Let’s make this picture more general so it can describe any environment interaction. <br />
<br /></p>
<h1 id="extending-the-simple-picture">Extending the Simple Picture</h1>

<p>By the way, if this gets to be a bit much, there is a handy picture of an MDP at the bottom. May I suggest opening it in a new tab or window for reference?</p>

<h2 id="imperfect-information">Imperfect information</h2>
<p>Earlier I said the actor “takes an observation” rather than “records the state.” This is because in general, the observation \(o\) recorded by the actor may be an imperfect representation of the well-defined environment state \(s\).</p>

<p>Suppose our actor is Paul Revere, and he is deciding whether to hang one or two lanterns at the Old North Church (action \(a\): 0, 1, or 2, encoding a signal to the militia: “don’t know”, “by land” and “by sea” respectively). There is an advancing British force coming in ships off the coast (state \(s\): 15,000 troops coming by sea).</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/revereride-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/revereride-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/revereride-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/revereride.jpg" class="img-fluid rounded" width="auto" height="auto" title="American dog is fully prepared to fight invaders, 1774." onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">The British are coming! Maybe.</figcaption>

</figure>

<p>However, Mr. Revere can only see a boat or two off the coast, and similarly a few carriages shuttling around on land. The British force is concealed by the fog and dark of night <d-footnote>Or worse, a sepia tone</d-footnote>. His observation \(o\) is an imperfect representation of the environment state \(s\) (observation \(o\): ~ 0, or maybe -10 (a few more people on land) or 10 (a few more at sea)).</p>

<h2 id="infinitely-long-episodes">Infinitely long episodes</h2>
<p>Next, what if our loop has no foreseeable end? In general this will be the case. Some environments go on forever, and there is nothing in our MDP picture which prevents that from happening.</p>

<p>Suppose our actor is a bipedal robot. Its task is to push an AWS server to the top of Mount Everest, because there are excellent ambient temperatures for computing up there. Unfortunately for the robot, every time it gets near the top, its servos freeze over, it loses control of the server rack, and it rolls all the way back to the bottom. And so it will try again until the end of time, or at least the end of its Amazonian overlords. All is well for the robot, who has no shortage of energy or enthusiasm.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Mount-Everest.webp-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Mount-Everest.webp-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Mount-Everest.webp-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Mount-Everest.webp" class="img-fluid rounded" width="auto" height="auto" title="Stay frosty" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">You got this, chief</figcaption>

</figure>

<p>How do we support infinite episodes? We simply mandate that every state \(s\) must accept a set of valid actions, and that such actions result in a state transition \((s,s')\). There is nowhere to “end,” and the MDP goes on forever.</p>

<p>Does this picture still support finite episodes? Notice that a state \(s\) can transition to itself \((s’ = s)\). To mark a state as terminal, we only allow it to transition to itself. Technically this is still an infinite MDP: our picture hasn’t changed, it will transition to itself forever. But if we reach a particular state or set of states, we can decide to stop traversing and end the infinite episode prematurely.</p>

<h2 id="discounted-rewards">Discounted rewards</h2>
<p>Now, let me ask you a question. You’ve won a million dollars. Congrats. Would you like your prize now, or in 30 years? Everyone can agree on the answer. If you have it now, you can improve your life, or others lives, now. If you’re worried about self control, stick it in a trust and only touch the dividends. What use is there in waiting?</p>

<p>In other words, reward now is better than reward later, else you’re just wasting time. What’s more, remember our trajectory is infinitely long in general. Ultimately we need to do calculations with the sum to learn from it, and we can’t do that with infinite numbers. To handle this, our actor’s more generalized goal is to maximize the <em>discounted</em> sum of rewards.</p>

\[R(\tau)=\sum_{t=0}^{T}\gamma^tr_t\]

<p>\(\gamma\) will usually be set to some number very close to 1, like 0.98. We add the discounting term so that our return converges to some finite value. We can see that as the time step gets further into the future, the discounting factor \(\gamma\) will make reward term decay to 0, and assuming no one reward is infinite, the sum will never be infinity.</p>
<details><summary>We have to go back…</summary>
<p>Notice we can recover our original un-discounted picture simply by setting \(\gamma=1\).</p>

</details>

<h2 id="probabilistic-state-transitions">Probabilistic state transitions</h2>
<p>Our robot from earlier is halfway up the mountain and tries to push forward one more step, but randomly a gust of wind causes him to lose his grip on the AWS server, rolling back down.</p>

<p>What if a particular action \(a\) from a particular observation \(o\) doesn’t always result in the same state transition \((s,s')\)? This is supported by probabilistic state transitions in the MDP. A definite action is taken, then state \(s\) will transition to \(s’\), but \(s’=0\) (the bottom of the mountain) with 5% probability, and \(s’=s+1\) the rest of the time.</p>

<p>In a simulated environment this can be represented by a transition function \(p(s’\vert s, a)\), with a vector of probabilities for all reachable \(s’\) from \(s\), for a particular action \(a\).</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Markov_Decision_Process.svg-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Markov_Decision_Process.svg-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Markov_Decision_Process.svg-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Markov_Decision_Process.svg.png" class="img-fluid rounded" width="auto" height="auto" title="MDP" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
An example of the full MDP. This encapsulates states (green nodes), actions (red nodes), rewards upon state transition (emitted squiggles), and nondeterministic transitions (arrows from red nodes).
<d-footnote>From the wikipedia page for MDPs.</d-footnote>
</div>

<p>That’s our MDP picture and the RL problem; not so bad, is it? Of course, we haven’t done any learning yet! See the next post on <a href="/blog/2023/VPG">VPG</a> for how to learn from interacting with the environment.</p>

<hr />
<p>My understanding of the subject comes from David Silver’s excellent <a href="https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver">lectures</a>, Open AI’s <a href="https://spinningup.openai.com/en/latest/">spinning up</a>, the 2017 Berkeley <a href="https://sites.google.com/view/deep-rl-bootcamp/home">Deep RL Bootcamp</a>, Pieter Abbeel’s and Sergey Levine’s various lectures on YouTube, and Sutton and Barto’s “<a href="http://incompleteideas.net/book/the-book-2nd.html">Reinforcement Learning</a>” (which was referenced by the others). These are excellent resources, and I recommend you check them out.</p>]]></content><author><name>Kenneth Jabon</name></author><category term="rl" /><summary type="html"><![CDATA[Before anything else, define the problem you need to solve.]]></summary></entry><entry><title type="html">Some Acme Speedbumps</title><link href="https://kjabon.github.io/blog/2023/AcmeIssues/" rel="alternate" type="text/html" title="Some Acme Speedbumps" /><published>2023-03-22T00:00:00-05:00</published><updated>2023-03-22T00:00:00-05:00</updated><id>https://kjabon.github.io/blog/2023/AcmeIssues</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/AcmeIssues/"><![CDATA[<p>Check out the companion <a href="https://github.com/kjabon/AcmeGPUHelper">github repo</a>.</p>

<h2 id="make-acme-work-for-you">Make Acme work for you!</h2>

<p>Parallelizing training of RL agents in Acme required a deeper understanding of the inner workings of Launchpad, DeepMind’s framework for distributed platforms, and the intricacies of checkpointing, which after deep diving used quite a bit of code from Tensorflow, took some dives into the code/documentation rabbit hole. This should be enough of a tipoff for you to figure this stuff out yourself. I’ll leave the details to the enterprising reader, but there are some things that were particularly obfuscated/necessary to progress.</p>

<h2 id="acme-uses-tensorflow">Acme uses Tensorflow!</h2>

<p>I didn’t understand at the outset that so much code from Tensorflow was allocating GPU memory behind the scenes, at the same time as the JAX code, in different processes, which is largely the result of things not being rewritten in JAX in Acme (no judgment here). Resolving cryptic errors, most of which ended up being these same GPU memory allocation issues, took a significant amount of rabbit hole digging and understanding the environment variables related to memory allocation for each library. Eventually I created a “GPU startup” file which solves all these issues without too much thought, which I have ported over to other projects. See <a href="https://github.com/kjabon/AcmeGPUHelper/blob/main/gpu.py">here</a> for the code.</p>

<h2 id="hindsight-buy-homogenous-set-of-gpus">Hindsight: Buy Homogenous set of GPUs</h2>
<p>Furthermore, JAX specifically does not allow mapping of work to heterogeneous device arrays (different GPUs), which is quite a shame in my opinion, as I currently have a 3080 and a 3080 Ti, which must instead be used for different sets of tasks. The JAX authors (Google) are likely used to using hundreds or thousands of server GPUs of the same type, not to mention TPUs, so I’m not holding my breath for a fix.</p>

<hr />

<p>Check out more <a href="/blog/">blog</a> postings on my RL projects!</p>]]></content><author><name>Kenneth Jabon</name></author><category term="rl" /><summary type="html"><![CDATA[Messing around with parallelization.]]></summary></entry><entry><title type="html">CoachRL Back End</title><link href="https://kjabon.github.io/blog/2023/CoachRLDetails/" rel="alternate" type="text/html" title="CoachRL Back End" /><published>2023-03-14T00:00:00-05:00</published><updated>2023-03-14T00:00:00-05:00</updated><id>https://kjabon.github.io/blog/2023/CoachRLDetails</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/CoachRLDetails/"><![CDATA[<p>See previous posts for <a href="/blog/2023/distill/">discussion of habits and rewards</a>, and the <a href="/blog/2023/CoachRLHighLevel/">daily use of CoachRL</a>. This post covers the technical details which power the project. <a href="https://github.com/kjabon/coachRL">See the GitHub Repo.</a></p>

<h2 id="outline">Outline</h2>

<p>Someday, this may become neatly contained in an iPhone app. For now, it is composed of several programs stitched together. First I’ll lay out these components, and touch on RL frameworks. I’ll talk about data pipelines, then I’ll cover the usage of the trained model to output daily suggestions. Finally, we train the model in simulation before introducing it to the real world.</p>

<h2 id="ingredients">Ingredients</h2>

<ul>
  <li>A Mac/Linux device. (Underlying libraries used do not currently run on Windows, though this could be changed).</li>
  <li>A device on which you can install the messaging app Telegram; used for notifications and requesting rewards for completing tasks.</li>
</ul>

<p>Optional but recommended:</p>
<ul>
  <li>A tablet, for a daily to-do list without rewriting your routine every day. I use an iPad with GoodNotes.</li>
  <li>An iPhone for auto-weight tracking.</li>
</ul>

<p>Software used:</p>

<ul>
  <li>A Renpho Bluetooth scale for weigh-in, with the associated iPhone app.</li>
  <li>iCloud to transfer weight data to a local csv file.</li>
  <li>Google sheets for an interactive log of daily habits, and running averages.</li>
  <li>Python, and many libraries, most importantly Acme from Deepmind, which uses a JAX implementation of PPO. In the future I plan on switching to MPO/MuZero for sample efficiency and performance. Also, libraries for communicating with iCloud and Google sheets. CoachRL shoots daily statistics and provides rewards via a Telegram interface, handled in Python with their bot API. (Any questions on this point: please leave a comment below).</li>
</ul>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/acme-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/acme-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/acme-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/acme.png" class="img-fluid rounded " width="auto" height="auto" title="Use JAX" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Use Acme</figcaption>

</figure>

<h2 id="rl-frameworks">RL Frameworks</h2>

<p>This project originated in PyTorch, using Stable Baselines 3. However, scaling actors used a lot of overhead, killing the benefit of parallelizing. Furthermore, extending SB3 was not very intuitive, which limited its usefulness for more advanced use-cases. I cast about for good libraries for parallelization in RL, and RLLib seemed to be a popular choice.</p>

<p>Before settling on this, I read Google is shifting to JAX from Tensorflow for its internal deep learning use cases. If we assume Tensorflow to be the industry standard for production deep learning, and its creators have come up with something more performant, then I’m not going to argue.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/jax.svg-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/jax.svg-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/jax.svg-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/jax.svg" class="img-fluid rounded " width="auto" height="auto" title="Use JAX" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
"JAX has the potential to be orders of magnitude faster than NumPy <br />(n.b. JAX is using TPU and NumPy is using CPU in order to highlight that JAX's speed ceiling...)." <d-footnote>Figure and caption from blog post by Ryan O'Connor at AssemblyAI. [https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2023/]</d-footnote>

</div>

<p>A recent <a href="https://arxiv.org/abs/2208.07860">paper</a> out of <a href="https://sites.google.com/berkeley.edu/walk-in-the-park">Berkeley</a> additionally saw a 16x speedup using JAX over PyTorch, enabling fast (20 minutes!) in-the-wild robot locomotion learning - something previously thought to be impossible due to sample efficiency. Read more about why JAX rules <a href="https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2023/">here</a>. I found <a href="https://github.com/deepmind/acme">Acme</a>, geared towards JAX, while using Tensorflow, to be the most comprehensive, maintained, and extendable framework.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/YO1USfn6sHY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
<div class="caption">
Robot locomotion, learned in-the-wild in less than 20 minutes with JAX.
</div>

<h2 id="data-pipelines">Data Pipelines</h2>
<p>What data are we processing, and what observation does the RL algorithm receive? Every day, you perform your habits to a certain level. For example, perhaps I go for a 30-minute run and practice violin for 15 minutes. I would manually input these values into the Google spreadsheet.</p>

<p>The spreadsheet maintains a daily record of the <a href="https://en.wikipedia.org/wiki/Exponential_smoothing">exponential moving average</a> (EMA) of the habit performance, normalized to your goal (see below figure in blue). Suppose my goal is to run 30 minutes a day, 5 days a week. If I don’t run at all, I would input a 0, and if this goes on for long, the EMA would eventually go to 0 as well. If I ran today for 30 minutes, I would input a 1, which would be normalized to 1*7(days in a week)/5 (days to run per week). 7/5 would be used as today’s update to the ongoing EMA, which will eventually converge to 1 if interspersed with two 0’s per week. The exact values and normalizations can be changed to suit your needs. Inputting 30 for a 30 minute run would work, so long as you’re normalizing it to 1 in the EMA calculation. (1 is assumed to be the optimal value for all goals). This is repeated for every daily habit.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/habitSheet-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/habitSheet-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/habitSheet-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/habitSheet.jpg" class="img-fluid rounded " width="auto" height="auto" title="Use JAX" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Values circled in red are the manually entered habits for the day. EMA in blue. Let's avoid the 'manual' part.</figcaption>

</figure>

<p>Now, if that sounds tedious, I agree! Having to manually input your habits every day, planning exactly what to do to meet your goals, etc. is a right pain. What if we could manage our habits without having to manually input anything? That is one problem we are here to solve.</p>

<h2 id="other-data-collection">Other Data Collection</h2>
<p>If one’s goal is to “be healthy,” one may more concretely define this as maintaining a certain weight, body fat percentage, resting heart rate, or all manner of health-related data. While this section can apply to any manner of metrics (perhaps not even health-related), I’m going to stick with weight.</p>

<p>Now, you’re going to have to actually weigh yourself and punch that in to the spreadsheet. No getting around it. Since our goal is to avoid tedium and pain, let’s be slightly smarter about this. I step on a scale every day, which connects via Bluetooth to my iPhone. An automation app on the phone saves this health data once a day to iCloud, which CoachRL can then access and stick in the spreadsheet for you. All that is required of me is grabbing my phone and stepping on the scale. The enterprising user can consider other ways to automatically collect data that is important to them.</p>
<h2 id="how-to-avoid-manual-habit-entry">How to avoid manual habit entry?</h2>
<p>Get a reinforcement learning algorithm to do it for us. All the pieces required to do this have been explained. Now, let’s put them together.</p>

<p>As you may recall, a reinforcement learning algorithm progresses through a trajectory of time steps. Each time step is composed of an observation, an action, an observation following that action, and a reward associated with the transition. Read <a href="http://www.incompleteideas.net/book/the-book-2nd.html">Sutton and Barto</a> for more details.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RLProblem-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RLProblem-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RLProblem-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/RLProblem.png" class="img-fluid rounded" width="auto" height="auto" title="RL Problem" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
Agents act in (or "send actions to") the environment. The environment progresses one time step based on this action, and responds with an observation and reward.
<d-footnote>Figure from Deepmind Acme paper on arXiv. [arXiv:2006.00979]</d-footnote>
</div>

<p>The observation is a vector of yesterday’s EMAs: one entry for each habit. Depending on the EMA, we will want to take a different action. What if we are falling behind on our running schedule?</p>

<p>The action taken by the actor is today’s planned level of performance for every tracked habit, based on yesterday’s EMA. If we’re behind our running schedule, run today!</p>

<p>The reward is simply 1-mean(abs(1-observation)): see below figure. If all the EMA values yesterday are at 1 (perfectly aligned with the goals of all habits), then the reward is 1. Any deviation will cause the reward to linearly decrease from this max value. We add 1 to make rewards (usually) lie between 0 and 1, just to keep it intuitive.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rewards-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rewards-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rewards-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/rewards.jpg" class="img-fluid rounded " width="auto" height="auto" title="RL Problem" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Environment rewards as a function of EMA, averaged across habits.</figcaption>

</figure>

<p>Beyond technical details, this is it! The RL actor performs inference on new observations, probabilistically selects actions for each habit, which is then filled in to the Google spreadsheet via the Google Docs API in Python.</p>

<p>Now, if your actions for any reason disagree with what the RL actor spits out, simply edit those values in the spreadsheet. If they consistently deviate, a little massaging of the output before it is filled into the spreadsheet is in order. For instance, if you are “coached” to run, work out, stretch, and play basketball in the same day, you may think this is not realistic for your schedule. In this particular case, I (in Python) randomly select one of those marked active, and set the rest to 0. Any massaging is also implemented when training the model.</p>

<h2 id="model-and-environment-setup">Model and Environment Setup</h2>
<p>Most of my time was spent creating an environment, grokking iCloud and Google Sheets APIs, and tweaking how actions were processed as I learned about the algorithm’s behavior. A simple N-day rolling average of habit performance was replaced by an exponential moving average halfway through the project. This hugely improved previous erratic behavior: instead of a number dropping out of the dataset entirely once per day, the EMA smoothed this effect out over time.</p>

<p>I did not have a good intuition for how large a network may be required, how many samples, what hyper parameters, etc., were necessary for optimal performance. More often than not, when I tackled strange behavior in the training with creative feature engineering, it ended in more confusion and was ultimately retconned in favor of splitting into multiple actors for the huge action space (10^15 possible combinations for 23 habits with varying action space sizes), one actor for every 5 habits or so.</p>

<p>Finally, the PPO implementation in Acme did not support MultiDiscrete action spaces. That is, 3 possible actions for habit A, 5 possible actions for habit B, and so on. To fix this, given neural network. Taking inspiration from the Stable Baselines codebase, I wrote a similar implementation in JAX, a sample of which is below. <a href="https://github.com/kjabon/coachRL">See the GitHub Repo.</a></p>

<d-code block="" language="python">
  # Compute log probabilities
  def log_prob(params, actions):
    logProb = jnp.stack([dist.log_prob(action) for dist, action in
      zip(getDistribution(params), jaxUnstack(actions, axis=1))], axis=1).sum(axis=1)
    return logProb
  
  # Sample actions
  def sample(params, key: networksLib.PRNGKey):
    samp = jnp.stack([dist.sample(seed=key) for dist in getDistribution(params)], axis=1)
    return samp
</d-code>

<div class="caption">
JAX/Python code for interfacing with neural networks to compute the log probabilities for use in policy gradient, and sampling from the Multi-Categorical probability distributions given a stack of parameters for those distributions.
</div>

<h2 id="simulation-training">Simulation Training</h2>
<p>Before thinking about training in simulation, I made an environment for reading EMAs from Google sheets, inputting this observation into the model, massaging the output a little, and writing the result back into Google sheets. All that was needed to simulate this process was to replace the calls to the Google Docs API with reading and writing from a queue whose elements represented all actions and EMAs for the day, and calculating said EMAs in Python instead.</p>

<h2 id="future-work">Future work</h2>
<p>Why bother with the RL model? Well, you may not know the optimal arrangement of your habits every day, or you might not want to slog through manually crafting hard-coded rules. Furthermore, perhaps there are long-term correlations between habits. Perhaps exercise gives you more energy long-term, causing you to get your work done in less time, leaving room for other habits. In the long run, training the model on real life data from the individual will give the best results. This was the main motivation for using RL in the first place.</p>

<p>However, sample efficiency prevents PPO from accomplishing these tasks. 1-5 million samples may be alright in a simulator with multiple actors in parallel, but most people don’t have time to wait that many days to get a good recommendation. This value may be lower when fine tuning a model from simulation on real life data, but is still substantially too large.</p>

<p>The solution is to use a more sample efficient algorithm. For various projects with costly actions (in particular, <a href="/blog/2023/PRL/">PRL</a>), I reviewed the literature in late 2022 and experimented with different RL algorithms to gain optimal sample efficiency. The two that rose to the top in this regard were MPO and MuZero (and follow-up papers). MuZero in particular has stellar performance in the discrete domain, because of its usage of Monte Carlo Tree Search, and stellar sample efficiency largely because it is able to “<a href="https://www.furidamu.org/blog/2020/12/22/muzero-intuition/">Reanalyse</a>” the data.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/reanalyse.webp-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/reanalyse.webp-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/reanalyse.webp-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/reanalyse.webp" class="img-fluid rounded " width="auto" height="auto" title="EfficientZero" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
Training loop of MuZero Reanalyse. <d-footnote>From a MuZero blog post by the paper's author. [https://www.furidamu.org/blog/2020/12/22/muzero-intuition/]</d-footnote>
</div>

<p><a href="https://arxiv.org/abs/2111.00210">EfficientZero</a> takes this sample efficiency further. (See my implementation and explanation of EfficientZero in blog posts to come!) As one can imagine, this comes at the cost of computational efficiency. However, since the time between steps for our environment is a full day, this is not a dealbreaker for us. While I have moved on to other projects in the meantime, I plan on updating this to use one of these more sample-efficient algorithms to take advantage of this long term “cross pollination” of habit effects. I hope this will result in a more holistic recommendation policy.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/efficientZero-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/efficientZero-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/efficientZero-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/efficientZero.png" class="img-fluid rounded " width="auto" height="auto" title="EfficientZero" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
The gains in sample efficiency in EfficientZero<d-footnote>https://arxiv.org/abs/2111.00210, Figure 1</d-footnote> make one hopeful for the future of RL in the real world.
</div>

<p>Finally, something I found necessary to gain optimal performance for every habit recommendation was to split habits into groups of 5 or so. My use case consists of 23 habits, each with multiple possible actions (up to 20). This is an extremely large action space! This splitting is less than ideal, because it requires more manual work, but particularly because you lose some of the possibility of getting aforementioned “cross-pollination.” It is possible I did not have the patience/compute power to make give optimal recommendations with a single network, or perhaps I simply wasn’t using a large enough network for the task. More reading and experimenting required.</p>

<hr />

<p>Thank you for reading; suggestions are welcome in the comments below! This article was already pretty long, so I skipped over many details; please feel free to ask! <a href="https://github.com/kjabon/coachRL">See the GitHub Repo.</a></p>

<p>See this <a href="/blog/2023/AcmeIssues/">short post</a> for other Acme issues I resolved surrounding multiple GPUs and parallelization.</p>]]></content><author><name>Kenneth Jabon</name></author><category term="habits" /><category term="rl" /><category term="coachrl" /><summary type="html"><![CDATA[Digging in to dirty details]]></summary></entry><entry><title type="html">Organizing Your Day with CoachRL</title><link href="https://kjabon.github.io/blog/2023/CoachRLHighLevel/" rel="alternate" type="text/html" title="Organizing Your Day with CoachRL" /><published>2023-02-10T00:00:00-06:00</published><updated>2023-02-10T00:00:00-06:00</updated><id>https://kjabon.github.io/blog/2023/CoachRLHighLevel</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/CoachRLHighLevel/"><![CDATA[<h2 id="to-summarize-the-previous-post">To Summarize the Previous <a href="/blog/2023/distill/">Post</a></h2>
<p>Suppose you want to form a lot of habits and do it quickly. You need to get organized. You need to track everything, you need to reward yourself for accomplishing tasks/habits, and you need to do all of this in a well-balanced, proportional way.<br />
But that’s hard.</p>

<h2 id="enter--coachrl">Enter- CoachRL</h2>

<p>No more tedium of tracking and decision-making. Automate everything to oblivion! Just a “personal coach” that tells you what to do and gives you a treat for doing it. <a href="https://github.com/kjabon/coachRL">See the GitHub Repo.</a></p>

<p>So what does a typical day look like?</p>

<p>First thing over coffee, you open your spreadsheet on Google Docs to find out what you’re going to be doing today. The row for today has already been filled in, based on the moving average of your “performance” for each habit.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/habitSheet-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/habitSheet-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/habitSheet-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/habitSheet.jpg" class="img-fluid rounded " width="auto" height="auto" title="example" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
In red: today's example activities, suggested by CoachRL. In blue: associated moving average of habit performances.
</div>

<p>Suppose you’re trying to work on a personal project for an hour a day. You were a bit overzealous last weekend and worked on it all weekend, sending your average over the one hour mark. The program will probabilistically output actions from a predefined, categorical distribution, so that over the next few days, your average will come down back to the goal. (You can see each habit can output whatever set of values makes sense - like 1-3 hours worked, or 0-4 “chunks of 30 minutes” practicing piano).<br />
If you differ from this suggestion, at the end of the day you should input the actual behavior, so tomorrow has accurate data for its suggestions.</p>

<h2 id="automation-of-fiddling-and-edits">Automation of Fiddling and Edits</h2>

<p>Typically I like to review my suggestions up front to get a handle on what I’ll be doing today, and make sure they make sense in aggregate. (Over time as I’ve smoothed out the bugs, I simply trust the output is correct and carry on with the following step.) E.g., I’d rather not run, go to the gym, play basketball, and do yoga all in one day - one exercise a day, at most, please! If I notice I’m making this kind of correction a lot, I’ll add it to the code which fills out the row with some simple logic. This further reduces my daily work and mental load. Ahh, the power of automation.</p>

<p>Now that I have today’s numbers in front of me, I open my note taking app on my iPad, and copy my daily schedule template over. I erase the things that have no bearing on today, alter today’s work time, and rearrange things according to decreasing difficulty (see <a href="https://todoist.com/productivity-methods/eat-the-frog">eating the frog</a>), and usually put my exercise in the middle of the day, just before lunch - the perfect midday break from my work day. The template is pre-organized so all of this takes just a minute or two with minimal fiddling. You can see a truncated example of this below. I also like to space out my work with errands/habits, so I’m not forced to work on something for 4 hours straight and get nowhere out of fatigue - but I leave myself the option to keep working indefinitely if I’m in the zone, simply crossing off future work items.</p>

<div class="row">
<div class="col">
</div>
<div class="col-8">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ipadTemplate-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ipadTemplate-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ipadTemplate-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/ipadTemplate.jpg" class="img-fluid rounded " width="auto" height="auto" title="example" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
 <div class="col">
</div>
</div>
<div class="caption">
Example daily template for a to-do list. Please pardon the chicken-scratch.
</div>

<p>Presto, you’re ready to start your day! Perhaps you have a morning routine involving water, spritz of lemon, and a cold shower. Then you can dig straight into your daily habits as the opportunity arises. Time before or after work? Go for a run or plug away at your novel.</p>

<p>Finally, for each item you complete - say 30 minutes of work, practicing piano for 30 minutes -  you request a reward from Telegram, which will tell you what you’ve earned for your hard work. A picture of this interface is in the figure below. Rewards are key, and keeps you trucking through the day. <br />
Pick a reward you can encapsulate and treat yourself with at will, that you find intrinsically rewarding. A YouTube jaunt, good book, or gaming session will do. Enjoy your spoils, and repeat! Add more habits as you feel you can handle the extra load. And that’s all there is to it!</p>

<p>Find the code for the Telegram rewards <a href="https://github.com/kjabon/rewardGenerator">here</a>: it’s very simple, and you should modify it to suit your needs.</p>

<div class="row">
<div class="col">
</div>
<div class="col-8">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/telegramCoach-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/telegramCoach-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/telegramCoach-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/telegramCoach.jpg" class="img-fluid rounded " width="auto" height="auto" title="example" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
 <div class="col">
</div>
</div>
<div class="caption">
Telegram interface for getting rewards from CoachRL.
</div>

<hr />

<p>See more for how and why to tune rewards, and snowballing good behaviors, in this previous blog <a href="/blog/2023/distill/">post</a>.</p>

<p>For the technical details of how the backend works, see the next <a href="/blog/2023/CoachRLDetails/">post</a>.</p>

<p>If there’s interest (via Github stars or comments below), I can make a post about setting it up for yourself. <a href="https://github.com/kjabon/coachRL">See the GitHub Repo.</a></p>]]></content><author><name>Kenneth Jabon</name></author><category term="habits" /><category term="rl" /><category term="coachrl" /><summary type="html"><![CDATA[Using the project, automate away the tedium of organizing your efforts.]]></summary></entry></feed>