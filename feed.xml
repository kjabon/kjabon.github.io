<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://kjabon.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://kjabon.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-04-13T11:57:38-05:00</updated><id>https://kjabon.github.io/feed.xml</id><title type="html">blank</title><entry><title type="html">The Reinforcement Learning Problem</title><link href="https://kjabon.github.io/blog/2023/RL/" rel="alternate" type="text/html" title="The Reinforcement Learning Problem" /><published>2023-04-13T00:00:00-05:00</published><updated>2023-04-13T00:00:00-05:00</updated><id>https://kjabon.github.io/blog/2023/RL</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/RL/"><![CDATA[]]></content><author><name>Kenneth Jabon</name></author><category term="rl" /><summary type="html"><![CDATA[Before anything else, define the problem you need to solve.]]></summary></entry><entry><title type="html">Some Acme Speedbumps</title><link href="https://kjabon.github.io/blog/2023/AcmeIssues/" rel="alternate" type="text/html" title="Some Acme Speedbumps" /><published>2023-03-22T00:00:00-05:00</published><updated>2023-03-22T00:00:00-05:00</updated><id>https://kjabon.github.io/blog/2023/AcmeIssues</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/AcmeIssues/"><![CDATA[<p>Check out the companion <a href="https://github.com/kjabon/AcmeGPUHelper">github repo</a>.</p>

<h2 id="make-acme-work-for-you">Make Acme work for you!</h2>

<p>Parallelizing training of RL agents in Acme required a deeper understanding of the inner workings of Launchpad, DeepMind’s framework for distributed platforms, and the intricacies of checkpointing, which after deep diving used quite a bit of code from Tensorflow, took some dives into the code/documentation rabbit hole. This should be enough of a tipoff for you to figure this stuff out yourself. I’ll leave the details to the enterprising reader, but there are some things that were particularly obfuscated/necessary to progress.</p>

<h2 id="acme-uses-tensorflow">Acme uses Tensorflow!</h2>

<p>I didn’t understand at the outset that so much code from Tensorflow was allocating GPU memory behind the scenes, at the same time as the JAX code, in different processes, which is largely the result of things not being rewritten in JAX in Acme (no judgment here). Resolving cryptic errors, most of which ended up being these same GPU memory allocation issues, took a significant amount of rabbit hole digging and understanding the environment variables related to memory allocation for each library. Eventually I created a “GPU startup” file which solves all these issues without too much thought, which I have ported over to other projects. See <a href="https://github.com/kjabon/AcmeGPUHelper/blob/main/gpu.py">here</a> for the code.</p>

<h2 id="hindsight-buy-homogenous-set-of-gpus">Hindsight: Buy Homogenous set of GPUs</h2>
<p>Furthermore, JAX specifically does not allow mapping of work to heterogeneous device arrays (different GPUs), which is quite a shame in my opinion, as I currently have a 3080 and a 3080 Ti, which must instead be used for different sets of tasks. The JAX authors (Google) are likely used to using hundreds or thousands of server GPUs of the same type, not to mention TPUs, so I’m not holding my breath for a fix.</p>

<hr />

<p>Check out more <a href="/blog/">blog</a> postings on my RL projects!</p>]]></content><author><name>Kenneth Jabon</name></author><category term="rl" /><summary type="html"><![CDATA[Messing around with parallelization.]]></summary></entry><entry><title type="html">CoachRL Back End</title><link href="https://kjabon.github.io/blog/2023/CoachRLDetails/" rel="alternate" type="text/html" title="CoachRL Back End" /><published>2023-03-14T00:00:00-05:00</published><updated>2023-03-14T00:00:00-05:00</updated><id>https://kjabon.github.io/blog/2023/CoachRLDetails</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/CoachRLDetails/"><![CDATA[<p>See previous posts for <a href="/blog/2023/distill/">discussion of habits and rewards</a>, and the <a href="/blog/2023/CoachRLHighLevel/">daily use of CoachRL</a>. This post covers the technical details which power the project. <a href="https://github.com/kjabon/coachRL">See the GitHub Repo.</a></p>

<h2 id="outline">Outline</h2>

<p>Someday, this may become neatly contained in an iPhone app. For now, it is composed of several programs stitched together. First I’ll lay out these components, and touch on RL frameworks. I’ll talk about data pipelines, then I’ll cover the usage of the trained model to output daily suggestions. Finally, we train the model in simulation before introducing it to the real world.</p>

<h2 id="ingredients">Ingredients</h2>

<ul>
  <li>A Mac/Linux device. (Underlying libraries used do not currently run on Windows, though this could be changed).</li>
  <li>A device on which you can install the messaging app Telegram; used for notifications and requesting rewards for completing tasks.</li>
</ul>

<p>Optional but recommended:</p>
<ul>
  <li>A tablet, for a daily to-do list without rewriting your routine every day. I use an iPad with GoodNotes.</li>
  <li>An iPhone for auto-weight tracking.</li>
</ul>

<p>Software used:</p>

<ul>
  <li>A Renpho Bluetooth scale for weigh-in, with the associated iPhone app.</li>
  <li>iCloud to transfer weight data to a local csv file.</li>
  <li>Google sheets for an interactive log of daily habits, and running averages.</li>
  <li>Python, and many libraries, most importantly Acme from Deepmind, which uses a JAX implementation of PPO. In the future I plan on switching to MPO/MuZero for sample efficiency and performance. Also, libraries for communicating with iCloud and Google sheets. CoachRL shoots daily statistics and provides rewards via a Telegram interface, handled in Python with their bot API. (Any questions on this point: please leave a comment below).</li>
</ul>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/acme-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/acme-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/acme-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/acme.png" class="img-fluid rounded " width="auto" height="auto" title="Use JAX" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Use Acme</figcaption>

</figure>

<h2 id="rl-frameworks">RL Frameworks</h2>

<p>This project originated in PyTorch, using Stable Baselines 3. However, scaling actors used a lot of overhead, killing the benefit of parallelizing. Furthermore, extending SB3 was not very intuitive, which limited its usefulness for more advanced use-cases. I cast about for good libraries for parallelization in RL, and RLLib seemed to be a popular choice.</p>

<p>Before settling on this, I read Google is shifting to JAX from Tensorflow for its internal deep learning use cases. If we assume Tensorflow to be the industry standard for production deep learning, and its creators have come up with something more performant, then I’m not going to argue.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/jax.svg-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/jax.svg-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/jax.svg-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/jax.svg" class="img-fluid rounded " width="auto" height="auto" title="Use JAX" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
"JAX has the potential to be orders of magnitude faster than NumPy <br />(n.b. JAX is using TPU and NumPy is using CPU in order to highlight that JAX's speed ceiling...)." <d-footnote>Figure and caption from blog post by Ryan O'Connor at AssemblyAI. [https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2023/]</d-footnote>

</div>

<p>A recent <a href="https://arxiv.org/abs/2208.07860">paper</a> out of <a href="https://sites.google.com/berkeley.edu/walk-in-the-park">Berkeley</a> additionally saw a 16x speedup using JAX over PyTorch, enabling fast (20 minutes!) in-the-wild robot locomotion learning - something previously thought to be impossible due to sample efficiency. Read more about why JAX rules <a href="https://www.assemblyai.com/blog/why-you-should-or-shouldnt-be-using-jax-in-2023/">here</a>. I found <a href="https://github.com/deepmind/acme">Acme</a>, geared towards JAX, while using Tensorflow, to be the most comprehensive, maintained, and extendable framework.</p>

<iframe width="560" height="315" src="https://www.youtube.com/embed/YO1USfn6sHY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe>
<div class="caption">
Robot locomotion, learned in-the-wild in less than 20 minutes with JAX.
</div>

<h2 id="data-pipelines">Data Pipelines</h2>
<p>What data are we processing, and what observation does the RL algorithm receive? Every day, you perform your habits to a certain level. For example, perhaps I go for a 30-minute run and practice violin for 15 minutes. I would manually input these values into the Google spreadsheet.</p>

<p>The spreadsheet maintains a daily record of the <a href="https://en.wikipedia.org/wiki/Exponential_smoothing">exponential moving average</a> (EMA) of the habit performance, normalized to your goal (see below figure in blue). Suppose my goal is to run 30 minutes a day, 5 days a week. If I don’t run at all, I would input a 0, and if this goes on for long, the EMA would eventually go to 0 as well. If I ran today for 30 minutes, I would input a 1, which would be normalized to 1*7(days in a week)/5 (days to run per week). 7/5 would be used as today’s update to the ongoing EMA, which will eventually converge to 1 if interspersed with two 0’s per week. The exact values and normalizations can be changed to suit your needs. Inputting 30 for a 30 minute run would work, so long as you’re normalizing it to 1 in the EMA calculation. (1 is assumed to be the optimal value for all goals). This is repeated for every daily habit.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/habitSheet-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/habitSheet-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/habitSheet-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/habitSheet.jpg" class="img-fluid rounded " width="auto" height="auto" title="Use JAX" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Values circled in red are the manually entered habits for the day. EMA in blue. Let's avoid the 'manual' part.</figcaption>

</figure>

<p>Now, if that sounds tedious, I agree! Having to manually input your habits every day, planning exactly what to do to meet your goals, etc. is a right pain. What if we could manage our habits without having to manually input anything? That is one problem we are here to solve.</p>

<h2 id="other-data-collection">Other Data Collection</h2>
<p>If one’s goal is to “be healthy,” one may more concretely define this as maintaining a certain weight, body fat percentage, resting heart rate, or all manner of health-related data. While this section can apply to any manner of metrics (perhaps not even health-related), I’m going to stick with weight.</p>

<p>Now, you’re going to have to actually weigh yourself and punch that in to the spreadsheet. No getting around it. Since our goal is to avoid tedium and pain, let’s be slightly smarter about this. I step on a scale every day, which connects via Bluetooth to my iPhone. An automation app on the phone saves this health data once a day to iCloud, which CoachRL can then access and stick in the spreadsheet for you. All that is required of me is grabbing my phone and stepping on the scale. The enterprising user can consider other ways to automatically collect data that is important to them.</p>
<h2 id="how-to-avoid-manual-habit-entry">How to avoid manual habit entry?</h2>
<p>Get a reinforcement learning algorithm to do it for us. All the pieces required to do this have been explained. Now, let’s put them together.</p>

<p>As you may recall, a reinforcement learning algorithm progresses through a trajectory of time steps. Each time step is composed of an observation, an action, an observation following that action, and a reward associated with the transition. Read <a href="http://www.incompleteideas.net/book/the-book-2nd.html">Sutton and Barto</a> for more details.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RLProblem-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RLProblem-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RLProblem-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/RLProblem.png" class="img-fluid rounded" width="auto" height="auto" title="RL Problem" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
Agents act in (or "send actions to") the environment. The environment progresses one time step based on this action, and responds with an observation and reward.
<d-footnote>Figure from Deepmind Acme paper on arXiv. [arXiv:2006.00979]</d-footnote>
</div>

<p>The observation is a vector of yesterday’s EMAs: one entry for each habit. Depending on the EMA, we will want to take a different action. What if we are falling behind on our running schedule?</p>

<p>The action taken by the actor is today’s planned level of performance for every tracked habit, based on yesterday’s EMA. If we’re behind our running schedule, run today!</p>

<p>The reward is simply 1-mean(abs(1-observation)): see below figure. If all the EMA values yesterday are at 1 (perfectly aligned with the goals of all habits), then the reward is 1. Any deviation will cause the reward to linearly decrease from this max value. We add 1 to make rewards (usually) lie between 0 and 1, just to keep it intuitive.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/rewards-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/rewards-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/rewards-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/rewards.jpg" class="img-fluid rounded " width="auto" height="auto" title="RL Problem" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture><figcaption class="caption">Environment rewards as a function of EMA, averaged across habits.</figcaption>

</figure>

<p>Beyond technical details, this is it! The RL actor performs inference on new observations, probabilistically selects actions for each habit, which is then filled in to the Google spreadsheet via the Google Docs API in Python.</p>

<p>Now, if your actions for any reason disagree with what the RL actor spits out, simply edit those values in the spreadsheet. If they consistently deviate, a little massaging of the output before it is filled into the spreadsheet is in order. For instance, if you are “coached” to run, work out, stretch, and play basketball in the same day, you may think this is not realistic for your schedule. In this particular case, I (in Python) randomly select one of those marked active, and set the rest to 0. Any massaging is also implemented when training the model.</p>

<h2 id="model-and-environment-setup">Model and Environment Setup</h2>
<p>Most of my time was spent creating an environment, grokking iCloud and Google Sheets APIs, and tweaking how actions were processed as I learned about the algorithm’s behavior. A simple N-day rolling average of habit performance was replaced by an exponential moving average halfway through the project. This hugely improved previous erratic behavior: instead of a number dropping out of the dataset entirely once per day, the EMA smoothed this effect out over time.</p>

<p>I did not have a good intuition for how large a network may be required, how many samples, what hyper parameters, etc., were necessary for optimal performance. More often than not, when I tackled strange behavior in the training with creative feature engineering, it ended in more confusion and was ultimately retconned in favor of splitting into multiple actors for the huge action space (10^15 possible combinations for 23 habits with varying action space sizes), one actor for every 5 habits or so.</p>

<p>Finally, the PPO implementation in Acme did not support MultiDiscrete action spaces. That is, 3 possible actions for habit A, 5 possible actions for habit B, and so on. To fix this, given neural network. Taking inspiration from the Stable Baselines codebase, I wrote a similar implementation in JAX, a sample of which is below. <a href="https://github.com/kjabon/coachRL">See the GitHub Repo.</a></p>

<d-code block="" language="python">
  # Compute log probabilities
  def log_prob(params, actions):
    logProb = jnp.stack([dist.log_prob(action) for dist, action in
      zip(getDistribution(params), jaxUnstack(actions, axis=1))], axis=1).sum(axis=1)
    return logProb
  
  # Sample actions
  def sample(params, key: networksLib.PRNGKey):
    samp = jnp.stack([dist.sample(seed=key) for dist in getDistribution(params)], axis=1)
    return samp
</d-code>

<div class="caption">
JAX/Python code for interfacing with neural networks to compute the log probabilities for use in policy gradient, and sampling from the Multi-Categorical probability distributions given a stack of parameters for those distributions.
</div>

<h2 id="simulation-training">Simulation Training</h2>
<p>Before thinking about training in simulation, I made an environment for reading EMAs from Google sheets, inputting this observation into the model, massaging the output a little, and writing the result back into Google sheets. All that was needed to simulate this process was to replace the calls to the Google Docs API with reading and writing from a queue whose elements represented all actions and EMAs for the day, and calculating said EMAs in Python instead.</p>

<h2 id="future-work">Future work</h2>
<p>Why bother with the RL model? Well, you may not know the optimal arrangement of your habits every day, or you might not want to slog through manually crafting hard-coded rules. Furthermore, perhaps there are long-term correlations between habits. Perhaps exercise gives you more energy long-term, causing you to get your work done in less time, leaving room for other habits. In the long run, training the model on real life data from the individual will give the best results. This was the main motivation for using RL in the first place.</p>

<p>However, sample efficiency prevents PPO from accomplishing these tasks. 1-5 million samples may be alright in a simulator with multiple actors in parallel, but most people don’t have time to wait that many days to get a good recommendation. This value may be lower when fine tuning a model from simulation on real life data, but is still substantially too large.</p>

<p>The solution is to use a more sample efficient algorithm. For various projects with costly actions (in particular, <a href="/blog/2023/PRL/">PRL</a>), I reviewed the literature in late 2022 and experimented with different RL algorithms to gain optimal sample efficiency. The two that rose to the top in this regard were MPO and MuZero (and follow-up papers). MuZero in particular has stellar performance in the discrete domain, because of its usage of Monte Carlo Tree Search, and stellar sample efficiency largely because it is able to “<a href="https://www.furidamu.org/blog/2020/12/22/muzero-intuition/">Reanalyse</a>” the data.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/reanalyse.webp-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/reanalyse.webp-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/reanalyse.webp-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/reanalyse.webp" class="img-fluid rounded " width="auto" height="auto" title="EfficientZero" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
Training loop of MuZero Reanalyse. <d-footnote>From a MuZero blog post by the paper's author. [https://www.furidamu.org/blog/2020/12/22/muzero-intuition/]</d-footnote>
</div>

<p><a href="https://arxiv.org/abs/2111.00210">EfficientZero</a> takes this sample efficiency further. (See my implementation and explanation of EfficientZero in blog posts to come!) As one can imagine, this comes at the cost of computational efficiency. However, since the time between steps for our environment is a full day, this is not a dealbreaker for us. While I have moved on to other projects in the meantime, I plan on updating this to use one of these more sample-efficient algorithms to take advantage of this long term “cross pollination” of habit effects. I hope this will result in a more holistic recommendation policy.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/efficientZero-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/efficientZero-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/efficientZero-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/efficientZero.png" class="img-fluid rounded " width="auto" height="auto" title="EfficientZero" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
The gains in sample efficiency in EfficientZero<d-footnote>https://arxiv.org/abs/2111.00210, Figure 1</d-footnote> make one hopeful for the future of RL in the real world.
</div>

<p>Finally, something I found necessary to gain optimal performance for every habit recommendation was to split habits into groups of 5 or so. My use case consists of 23 habits, each with multiple possible actions (up to 20). This is an extremely large action space! This splitting is less than ideal, because it requires more manual work, but particularly because you lose some of the possibility of getting aforementioned “cross-pollination.” It is possible I did not have the patience/compute power to make give optimal recommendations with a single network, or perhaps I simply wasn’t using a large enough network for the task. More reading and experimenting required.</p>

<hr />

<p>Thank you for reading; suggestions are welcome in the comments below! This article was already pretty long, so I skipped over many details; please feel free to ask! <a href="https://github.com/kjabon/coachRL">See the GitHub Repo.</a></p>

<p>See this <a href="/blog/2023/AcmeIssues/">short post</a> for other Acme issues I resolved surrounding multiple GPUs and parallelization.</p>]]></content><author><name>Kenneth Jabon</name></author><category term="habits" /><category term="rl" /><category term="coachrl" /><summary type="html"><![CDATA[Digging in to dirty details]]></summary></entry><entry><title type="html">Organizing Your Day with CoachRL</title><link href="https://kjabon.github.io/blog/2023/CoachRLHighLevel/" rel="alternate" type="text/html" title="Organizing Your Day with CoachRL" /><published>2023-02-10T00:00:00-06:00</published><updated>2023-02-10T00:00:00-06:00</updated><id>https://kjabon.github.io/blog/2023/CoachRLHighLevel</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/CoachRLHighLevel/"><![CDATA[<h2 id="to-summarize-the-previous-post">To Summarize the Previous <a href="/blog/2023/distill/">Post</a></h2>
<p>Suppose you want to form a lot of habits and do it quickly. You need to get organized. You need to track everything, you need to reward yourself for accomplishing tasks/habits, and you need to do all of this in a well-balanced, proportional way.<br />
But that’s hard.</p>

<h2 id="enter--coachrl">Enter- CoachRL</h2>

<p>No more tedium of tracking and decision-making. Automate everything to oblivion! Just a “personal coach” that tells you what to do and gives you a treat for doing it. <a href="https://github.com/kjabon/coachRL">See the GitHub Repo.</a></p>

<p>So what does a typical day look like?</p>

<p>First thing over coffee, you open your spreadsheet on Google Docs to find out what you’re going to be doing today. The row for today has already been filled in, based on the moving average of your “performance” for each habit.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/habitSheet-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/habitSheet-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/habitSheet-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/habitSheet.jpg" class="img-fluid rounded " width="auto" height="auto" title="example" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
In red: today's example activities, suggested by CoachRL. In blue: associated moving average of habit performances.
</div>

<p>Suppose you’re trying to work on a personal project for an hour a day. You were a bit overzealous last weekend and worked on it all weekend, sending your average over the one hour mark. The program will probabilistically output actions from a predefined, categorical distribution, so that over the next few days, your average will come down back to the goal. (You can see each habit can output whatever set of values makes sense - like 1-3 hours worked, or 0-4 “chunks of 30 minutes” practicing piano).<br />
If you differ from this suggestion, at the end of the day you should input the actual behavior, so tomorrow has accurate data for its suggestions.</p>

<h2 id="automation-of-fiddling-and-edits">Automation of Fiddling and Edits</h2>

<p>Typically I like to review my suggestions up front to get a handle on what I’ll be doing today, and make sure they make sense in aggregate. (Over time as I’ve smoothed out the bugs, I simply trust the output is correct and carry on with the following step.) E.g., I’d rather not run, go to the gym, play basketball, and do yoga all in one day - one exercise a day, at most, please! If I notice I’m making this kind of correction a lot, I’ll add it to the code which fills out the row with some simple logic. This further reduces my daily work and mental load. Ahh, the power of automation.</p>

<p>Now that I have today’s numbers in front of me, I open my note taking app on my iPad, and copy my daily schedule template over. I erase the things that have no bearing on today, alter today’s work time, and rearrange things according to decreasing difficulty (see <a href="https://todoist.com/productivity-methods/eat-the-frog">eating the frog</a>), and usually put my exercise in the middle of the day, just before lunch - the perfect midday break from my work day. The template is pre-organized so all of this takes just a minute or two with minimal fiddling. You can see a truncated example of this below. I also like to space out my work with errands/habits, so I’m not forced to work on something for 4 hours straight and get nowhere out of fatigue - but I leave myself the option to keep working indefinitely if I’m in the zone, simply crossing off future work items.</p>

<div class="row">
<div class="col">
</div>
<div class="col-8">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ipadTemplate-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ipadTemplate-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ipadTemplate-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/ipadTemplate.jpg" class="img-fluid rounded " width="auto" height="auto" title="example" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
 <div class="col">
</div>
</div>
<div class="caption">
Example daily template for a to-do list. Please pardon the chicken-scratch.
</div>

<p>Presto, you’re ready to start your day! Perhaps you have a morning routine involving water, spritz of lemon, and a cold shower. Then you can dig straight into your daily habits as the opportunity arises. Time before or after work? Go for a run or plug away at your novel.</p>

<p>Finally, for each item you complete - say 30 minutes of work, practicing piano for 30 minutes -  you request a reward from Telegram, which will tell you what you’ve earned for your hard work. A picture of this interface is in the figure below. Rewards are key, and keeps you trucking through the day. <br />
Pick a reward you can encapsulate and treat yourself with at will, that you find intrinsically rewarding. A YouTube jaunt, good book, or gaming session will do. Enjoy your spoils, and repeat! Add more habits as you feel you can handle the extra load. And that’s all there is to it!</p>

<p>Find the code for the Telegram rewards <a href="https://github.com/kjabon/rewardGenerator">here</a>: it’s very simple, and you should modify it to suit your needs.</p>

<div class="row">
<div class="col">
</div>
<div class="col-8">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/telegramCoach-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/telegramCoach-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/telegramCoach-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/telegramCoach.jpg" class="img-fluid rounded " width="auto" height="auto" title="example" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

</div>
 <div class="col">
</div>
</div>
<div class="caption">
Telegram interface for getting rewards from CoachRL.
</div>

<hr />

<p>See more for how and why to tune rewards, and snowballing good behaviors, in this previous blog <a href="/blog/2023/distill/">post</a>.</p>

<p>For the technical details of how the backend works, see the next <a href="/blog/2023/CoachRLDetails/">post</a>.</p>

<p>If there’s interest (via Github stars or comments below), I can make a post about setting it up for yourself. <a href="https://github.com/kjabon/coachRL">See the GitHub Repo.</a></p>]]></content><author><name>Kenneth Jabon</name></author><category term="habits" /><category term="rl" /><category term="coachrl" /><summary type="html"><![CDATA[Using the project, automate away the tedium of organizing your efforts.]]></summary></entry><entry><title type="html">PRL</title><link href="https://kjabon.github.io/blog/2023/PRL/" rel="alternate" type="text/html" title="PRL" /><published>2023-01-21T00:00:00-06:00</published><updated>2023-01-21T00:00:00-06:00</updated><id>https://kjabon.github.io/blog/2023/PRL</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/PRL/"><![CDATA[<p>Coming Soon!</p>]]></content><author><name>Kenneth Jabon</name></author><category term="photonics" /><category term="rl" /><summary type="html"><![CDATA[Photonic device design by iterative simulations, optimized with RL.]]></summary></entry><entry><title type="html">Game Dev</title><link href="https://kjabon.github.io/blog/2023/gamedev/" rel="alternate" type="text/html" title="Game Dev" /><published>2023-01-21T00:00:00-06:00</published><updated>2023-01-21T00:00:00-06:00</updated><id>https://kjabon.github.io/blog/2023/gamedev</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/gamedev/"><![CDATA[<p>Coming Soon!</p>]]></content><author><name>Kenneth Jabon</name></author><category term="gamedev" /><summary type="html"><![CDATA[Video game environments tuned by RL from human feedback.]]></summary></entry><entry><title type="html">MuZero in Acme</title><link href="https://kjabon.github.io/blog/2023/muzeroAcmeJax/" rel="alternate" type="text/html" title="MuZero in Acme" /><published>2023-01-21T00:00:00-06:00</published><updated>2023-01-21T00:00:00-06:00</updated><id>https://kjabon.github.io/blog/2023/muzeroAcmeJax</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/muzeroAcmeJax/"><![CDATA[<h2 id="why-muzero">Why MuZero?</h2>

<p>Implemented Sampled-MuZero-Reanalyze with sample-efficiency improvements found here, in JAX, in DeepMind’s Acme RL framework, synthesizing these implementations. The goal here was my own understanding, and to build it for my own and others’ general usage, particularly given its outstanding performance in discrete action spaces. (Full understanding of two source implementations complete; I’ve just begun to combine them in JAX/Acme.)</p>

<p>Coming Soon!</p>]]></content><author><name>Kenneth Jabon</name></author><category term="rl" /><summary type="html"><![CDATA[An implementation of DeepMind's breakthrough algorithm, with improvements.]]></summary></entry><entry><title type="html">SPS</title><link href="https://kjabon.github.io/blog/2023/SPS/" rel="alternate" type="text/html" title="SPS" /><published>2023-01-16T00:00:00-06:00</published><updated>2023-01-16T00:00:00-06:00</updated><id>https://kjabon.github.io/blog/2023/SPS</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/SPS/"><![CDATA[<p>Coming soon!</p>]]></content><author><name>Kenneth Jabon</name></author><category term="photonics" /><summary type="html"><![CDATA[A tunable photon pair source for use in a linear-optic quantum computer.]]></summary></entry><entry><title type="html">Habits and Rewards</title><link href="https://kjabon.github.io/blog/2023/distill/" rel="alternate" type="text/html" title="Habits and Rewards" /><published>2023-01-16T00:00:00-06:00</published><updated>2023-01-16T00:00:00-06:00</updated><id>https://kjabon.github.io/blog/2023/distill</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/distill/"><![CDATA[<h2 id="why-habits">Why habits?</h2>

<p>Stephen King is one of the most prolific writers of our time. How has he managed to write more than 60 novels, and sell more than 400 million copies? Consistent and sustainable effort. As he recounted in “On Writing,” the vast majority of his books weren’t the result of 16-hour sprints. He follows one rule: to write for 4 hours a day, every day, no exceptions, like a stream carving out a canyon through hard rock.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/onWriting-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/onWriting-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/onWriting-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/onWriting.jpg" class="img-fluid rounded " width="auto" height="auto" title="example" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<div class="caption">
    I highly recommend Stephen King's "On Writing." The lessons within are not writing-exclusive!
</div>

<p>You likely agree that exercise, healthy eating, a consistent sleep schedule, and reading every day are all things we should be doing, and probably benefit the rest of our activities indirectly.</p>

<h2 id="i-hear-you-counterpoint-tv">I hear you. Counterpoint: TV</h2>

<p>Waking up an hour early to get a run in? Gross. It’s raining. How about some hot chocolate instead? Now add a fuzzy blanket and some X-Files.</p>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/kronk.gif-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/kronk.gif-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/kronk.gif-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/kronk.gif" class="img-fluid rounded " width="auto" height="auto" title="example" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>

<p>If only we had someone around to say hey man, I know it’s raining outside, and you’re not even sure you want to get into this whole cardio thing. But I’m gonna make it worth your while. Go do that run for me, and you can watch as much X-Files as you want.</p>

<p>Ok, sounds pretty good. But what if I’m a good little optimizer, and I want to form as many habits as possible in as short a time as possible, without burning myself out and falling off the wagon of every last thing in an endless loop? And what if I don’t want to run every day: I just want to run two days a week, and go to the gym the other two, and maybe do hot yoga the other two? It sucks keeping track of all this stuff every day! And another thing! If I’m being rewarded with endless X-Files, how am I going to have time for anything beyond exercise and X-Files?</p>

<p>Basically, use rewards (read on), and get organized <a href="/blog/2023/CoachRLHighLevel/">(next post)</a>.</p>

<h2 id="use-rewards">Use rewards</h2>

<p>Now for the juicy part - and I mean that literally in the context of rewarding brain chemicals. For your typical person, running or writing a novel is not intrinsically rewarding, or at least it is less intrinsically rewarding than playing Skyrim.<br />
Until it becomes an automatic habit, you must reward yourself for doing it! This may sound a lot like <a href="https://en.wikipedia.org/wiki/Operant_conditioning">operant conditioning!</a> This can be thought of very similarly to reinforcement learning. Put it to work for yourself and change your life.</p>

<p>Suppose you don’t do this. Your determination and willpower will take a day (or week) off, and sure - maybe you’ll get back on the bus, sooner or later. But relying on your own grit and available mind space is going to take longer, at best. It will leave you with less daily energy to apply to other things! Your work, hobbies, relationships, other habits - everything will suffer just a little bit. At worst, you’ll find yourself in an endless cycle of trying to start running; things go well for a week, and then you burn yourself out with your enthusiasm, and then start all over again.</p>

<h2 id="rewarding-good-behaviors">Rewarding good behaviors</h2>
<p>So, what’s the mechanism we use here?</p>

<p>First, you have to come up with a set of rewards for yourself. What’s your favorite thing to do? This can range from eating sugary treats, playing video games, to reading manga, to watching TV, to reading books, to reading about your favorite hobby, to playing a musical instrument, to cooking, and even to things like running and going to the gym, if that’s a habit you’ve already formed. Whatever it happens to be, set those things aside as rewards.</p>
<div class="row">

<div class="col-sm">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/mineral-480.webp" />
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/mineral-800.webp" />
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/mineral-1400.webp" />
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/mineral.png" class="img-fluid rounded " width="auto" height="auto" onerror="this.onerror=null; $('.responsive-img-srcset').remove();" />

  </picture>

</figure>
 
</div>
</div>
<div class="caption">
    Find the mineral you crave, and use it as a reward.
</div>

<p>Now, every time you clear an item off of your daily to-do list, you reward yourself (more details on automating this process in later posts). These rewards start out small (1-5 minutes). As previously mentioned, you don’t want to spend your whole day watching TV as a nice reward for flossing in the morning.</p>

<p>As more items are completed, the rewards grow. As your willpower is depleted throughout the day, you’ll find yourself making excuses not to tackle the next item on your list. Rewards grow throughout the day to counteract this effect, and keep you forming your habits for longer.</p>

<h2 id="individual-caveats">Individual caveats</h2>

<p>It is important to recognize that every individual has limits, which vary from person to person. Perhaps your work is particularly demanding, or your health has gotten away from you. Taking this into account, you should increase the amount of rewards you get for completing your tasks, and/or decrease the amount you’re trying to accomplish. If you stick to it, habits will become more automatic, requiring less mental energy, and the amount of reward you need to keep it up will decrease over time. Now you have extra energy for the day. If you find yourself at the end of the day with plenty of extra energy, add a new habit, or deepen the goals for an existing one. As I alluded to earlier, perhaps you can instead convert an automatic habit you particularly enjoy (e.g. going for a run) into a reward. One can imagine this can have a compounding effect.</p>

<p>It’s helpful to me to categorize all kinds of habits as how “drug-like” they are. E.g., bingeing TV or video games, alcohol and caffeine intake, etc. These are things that you will necessarily gain (at least) a psychological tolerance to, and withdraw from if you stop. If these are minimized, not only do you have more time for other opportunities, but, anecdotally at least, I find I gain a greater mental acuity. I believe this is due to a greater amount of active <a href="https://pubmed.ncbi.nlm.nih.gov/31905114/">dopamine receptors</a> in the brain. Flood the brain with dopamine via various drug-like activities, and to achieve homeostasis, the brain will reduce the amount of receptors to that particular reward. Tolerance to drug-like things is your brain’s way of normalizing rewards.</p>

<p>If you actively minimize drug-like behaviors, this normalization will happen in the opposite direction, and instead of becoming like the typical drug addict for which normal life holds no interest, normal life will be intensely interesting! In this way, you have an innate motivation to seek rewards, and these rewards feel more rewarding! Conversely, we have “negative drugs,” otherwise known as healthy living: exercise, healthy eating, and the like. Forming such habits will give you this same innate motivation.</p>

<h2 id="minimize-all-fun-not-so-fast">Minimize all fun? Not so fast</h2>

<p>The willful optimizing reader may say: why not completely eliminate anything drug-like, and only live in a completely healthy fashion?</p>

<p>I don’t think the brain is designed to work this way; that is, with no rewards, or constant rewards. The brain’s reward system is greedy. It always seeks to maximize rewards experienced, with as minimal effort as possible - unless there is a system in place to prevent this, e.g. a moral system, a value system, and their physical realizations/extensions. One may argue that these kinds of systems and organizations (personal principles, one’s religion, political beliefs, family, the workplace) are simply different sources of rewards, with social or biological foundations themselves. Aside from a good book waiting for you at the end of the day, your reward may be seeing your values play out in the world, your children succeeding in their endeavors, flourishing relationships, a successful career, etc.</p>

<p>These sorts of things are intrinsic, and typically good and healthy. One may argue that if an individual has found sustainable ways to achieve all of  these kinds of long-term rewards, they’re pretty much set for life. Maybe this is how many define success! But, perhaps you’re not quite there yet to a sustainable, satisfying life. If you were satisfied and sustained, you probably wouldn’t be reading this article. Let’s continue with the assumption that you want to make a change.</p>

<h2 id="balance-in-all-things">Balance in all things</h2>

<p>Often these day-to-day, external, intrinsic rewards compete with other rewards in your life for your attention. It is not easy to encapsulate these kinds of rewards and use them as a treat for going out for a run. Furthermore, if you are not intentionally giving yourself a controllable kind of nugget in exchange for “going for a run,” or whatever your goal habit may be, and doing so in a repeated, organized manner, your goal habits are all but guaranteed to fall by the wayside, and be consumed by more intrinsically rewarding things (drug-like things and base instinct).</p>

<p>Maybe that’s not so bad, depending on what your goals may be. For the purposes of this article, we’ll assume you have a goal that is not intrinsically rewarding up front, as happens to be the case with many worthwhile things.<br />
Going back to our question: why not completely eliminate anything drug-like, and only live in a completely healthy fashion? Because if the nugget waiting for you on the other side of your task or goal isn’t juicy enough, you’re not going to form that habit. You’re not going to train your brain the way you consciously desire, it will be trained by other, external factors. Too juicy, and risk being consumed by your reward. Not juicy enough, and risk being consumed by external factors and making no habitual progress.</p>

<p>Play this balancing act with care; I recommend linearly ramping down rewards (starting high)/ ramping up your habit demands (starting low) until it’s just a little hard - then maintaining this challenge level for awhile, seeing that you don’t burn out, before making further perturbations.</p>

<hr />

<p>See the next article, on using CoachRL to put the above into action, <a href="/blog/2023/CoachRLHighLevel/">here</a>.</p>

<p>See the articles in this series <a href="/blog/tag/habits/">here</a>.</p>]]></content><author><name>Kenneth Jabon</name></author><category term="habits" /><summary type="html"><![CDATA[Forming good habits is a lasting way to change your life.]]></summary></entry><entry><title type="html">Wafer Probing of Silicon Photonics</title><link href="https://kjabon.github.io/blog/2023/waferProber/" rel="alternate" type="text/html" title="Wafer Probing of Silicon Photonics" /><published>2023-01-16T00:00:00-06:00</published><updated>2023-01-16T00:00:00-06:00</updated><id>https://kjabon.github.io/blog/2023/waferProber</id><content type="html" xml:base="https://kjabon.github.io/blog/2023/waferProber/"><![CDATA[<p>Coming Soon!</p>]]></content><author><name>Kenneth Jabon</name></author><category term="photonics" /><summary type="html"><![CDATA[Electro-optic measurements of integrated photonic devices on the wafer scale.]]></summary></entry></feed>