---
layout: distill
title: Vanilla Policy Gradient
description: Theory and intuition behind one of our introductory algorithms
giscus_comments: true
date: 2023-04-14
tags: rl 
authors:
  - name: Kenneth Jabon

  

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: A bird's eye view
  - name: Breaking down the policy gradient
  - name: Learning from rewards
  - name: The value function
  - name: Generalized Advantage Estimation
  - name: Batched training of neural nets

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---



Vanilla policy gradient is one of the simplest reinforcement learning algorithms. It should serve to form the theoretical foundation for many following policy-based, online algorithms. If you're more interested in offline algorithms, I recommend you start with TD-learning and deep Q-learning. See chapter 6 of [Sutton and Barto](http://incompleteideas.net/book/the-book-2nd.html) for that.

This post assumes you're familiar with how Markov Decision Processes work, and how the reinforcement learning problem is set up. For a guide to that, see [here](/blog/2023/RL).



## A bird's eye view

{% include figure.html path="assets/img/eagle.jpg" title="bird" class="img-fluid rounded" %} 

Let's describe what happens in a full iteration of the loop before diving in.

First, we collect a batch of trajectories $$\tau$$ by allowing our current policy $$\pi$$ (represented by a neural network) to unfold in the environment. We collect a full batch because, in general, the policy does not output actions $$a$$, but a probability distribution over actions given the current state $$s$$: $$\pi(a\vert s)$$. To get to the next timestep in our trajectory, we select an action $$a$$ by sampling from this distribution. Randomness may also occur as the result of the environment itself, so all in all we want to get plenty of samples to come up with an accurate, representative set of trajectories $$\tau$$. 

{% details Sampling is great for blind exploration, but... %}
Eventually sampling doesn't do the job any more. At some point we'll want to exploit what we know and make our way to better states to be able to learn from there, so we'll just pick the best known action instead of sampling. The amount of "greedy action selection" will be scheduled to increase over time, to progress from pure exploration of the state space to pure exploitation of the policy.
{% enddetails %}

Now, if a policy gains an above-average sum of rewards in a particular trajectory, we will nudge the policy $$\pi$$ in the direction of the actions $$a$$ (given their respective states, $$\vert s$$) which resulted in this trajectory. Conversely, if a trajectory comes with a below-average sum of rewards, we nudge the policy away from taking those actions. This sum is known as the **return** for a trajectory<d-footnote>This isn't the full return yet, we'll make it more general in a bit.</d-footnote>:

$$
R(\tau)=\sum_{t=0}^{T}r_t
$$

If any of this seems unclear, you'll want to start with the [previous](/blog/2023/RL) post.

To estimate the return at each time step with low variance, we employ generalized advantage estimation (GAE). We train a neural network to represent the value function $$V$$, which is incorporated into our advantage term $$A$$. This will be discussed in the [section on GAE](#generalized-advantage-estimation). 

Now, every time we go through our loop, we have a better estimate of what a good trajectory looks like (by training the value function $$V$$), and a better idea of what actions to take to get good trajectories (by training the policy $$\pi$$). This glosses over some details, which we'll get into in just a bit.

Take a glance over the simplified pseudocode for the algorithm, then let's get cracking!

{% include figure.html path="assets/img/pseudoPseudo.svg" title="Algorithm 0" class="img-fluid rounded" %}


## Breaking down the policy gradient
{% include figure.html path="assets/img/jackhero.jpg" title="jack" class="img-fluid rounded" %} 

The policy gradient is calculated as follows:


$$ \hat{g}_{k}=\frac{1}{\left\vert \mathcal{D}_k \right\vert }\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} \hat{V}_t $$

Let's break this down to gain an understanding of each term, working from the inside out. Let's start with the policy:

$$\pi_\theta (a_t\vert s_t)$$

which is the function which outputs the probability distribution of all possible actions $$a_t$$ at time $$t$$ in the trajectory $$\tau$$, given the state $$s_t$$ at time $$t$$. To bring it back to earth, think of this as $$y = f(x,\theta)$$. 

 {% details O_O %}
If this is making you feel like a deer in headlights, don't worry. Let's be more explicit. $$\theta$$ parameterizes our function $$f$$, and then we evaluate our function on $$x$$. What do I mean? If I had a function $$y = f(x, \theta) = mx + b$$, I parameterize this function with $$m$$ and $$b$$ (represented by the vector $$\theta = [m,b]$$), and then evaluate it on $$x$$. In the case of a neural net, $$\theta$$ is instead a matrix of numbers (which can be flattened into a vector $$[a_1,a_2,a_3,...,a_n]$$) representing its weights and biases.
 {% enddetails %}


- If we have a discrete action space, this is a function which outputs a vector with a value for each possible action (in other words, a categorical probability distribution). 
- If the action space is continuous, this is a function which outputs the mean $$\mu$$ and standard deviation $$\sigma$$ representing the action's probability distribution. <d-footnote> If you're worried about flexibility, it turns out you can also output arbitrary combinations of these, but we won't consider that case here.</d-footnote>



**Take the log** of the policy: 

$$\log\pi_\theta (a_t\vert s_t)$$ 

Remember, all that is happening is we're taking the log of a function, i.e., $$g(x, \theta) = \log(f(x, \theta))$$.
<br>

**Take the gradient** of this with respect to the policy parameters $$\theta$$.

$$\nabla_\theta \log\pi_\theta (a_t\vert s_t)$$  

Simply, the gradient of a function, i.e.  $$\nabla g(x, \theta)$$ with respect to $$\theta$$.
<br>

**Evaluate the policy** given the current policy parameters $$\theta_k$$:

$$\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}$$  

Or, we perform inference with the current model on the current state of the environment. 


 {% details Numerical considerations %}
Let's take a step back. If we are used to working out derivations with pencil and paper, the order in which I presented the last few steps should not start sounding any alarms.<d-footnote>Assuming you made it through Calculus in one piece. If not, don't worry. Go ahead and take or re-take Calculus, and then come back. You can do it, I promise. If that's too much hassle, luckily we have autodiff which means you can summarily forget about this derivation; so just keep reading with a glazed look for a couple more paragraphs. </d-footnote>

Normally in this case one would take the derivative of the symbolic function, then evaluate that to get the derivative at the point of interest, or a similar method of your choice.



However, there are two differences when doing this numerically on your computer; having to do with the derivative and the log. 

### For the derivative:
Instead, we just say "hello, new best friend, the autodiff function <d-footnote>For example, jax.grad()</d-footnote>! I'm going to run the code which evaluates $$\log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}$$ for an entire batch of states in one vectorized/parallelized operation, across whatever computational resources I have available. Can you please give me $$\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}$$?" And our new best friend is happy to oblige. Implementation details can be found in the next [post](/blog/2023/VPGInJax). 

How does our new friend work? I'll mostly defer to good explanations elsewhere, for example [here](https://github.com/HIPS/autograd/blob/master/docs/tutorial.md#whats-going-on-under-the-hood) and [here](https://en.wikipedia.org/wiki/Automatic_differentiation). The gist is that first, it keeps track of the computation graph of whatever functions you want to differentiate, as you execute them, from state $$s$$ and parameters $$\theta$$ all the way to $$\log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}$$. Then, it uses this graph to multiply its way through the chain rule, resulting in the gradient(s) you want. 

### For the log:


**For categorical distributions**, it's very simple.

We're using a neural network, whose job it is to approximate a function's output. Well, why go through the trouble of approximating a function, and then taking the log of the output, when we could just encapsulate the log into the function and approximate that directly? And this exactly what we do. We're interested in the log of the output of a probability distribution (an array of log-likelihoods, one for each action). We just consider the output of the network to be the array: $$\log \left[ P_\theta (s)\right] $$.

**For continuous distributions**, it's a little trickier

Remember we usually represent the probability distribution in the continuous case as a multivariate normal distribution. To keep things simple, we actually use a diagonal covariance matrix, rather than the full covariance matrix, so each dimension of the action can be represented by a single standard deviation. This way, we only need to output one mean and one standard deviation per dimension, and calculating the log of the distribution also becomes much easier. Now, how do we take the log of a (diagonal) normal distribution? 

Like this! 

$$\log\pi_\theta (a\vert s) = -\frac{1}{2}\left(n\log 2\pi+\sum_{i=1}^n\left[\frac{(a_i-\mu_i)^2}{\sigma_i^2}+2\log\sigma_i\right]\right)$$

where n is the dimensionality of the action space. If you're not convinced (I wasn't), [here's a derivation](/blog/2023/ContinuousLogLikelihood/). 

Our neural network spits out a vector each of $$\mu$$ and $$\sigma$$, one element for each action dimension. With these values, we sample from a diagonal MVN distribution, yielding a vector of actions $$a$$. Then, to get the log-likelihoods above equation describes the log likelihoods of that vector of actions.


 {% enddetails %}


**Sum this term** over all time steps in our current trajectory.

$$\sum_{t=0}^{T}\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}$$  

A [brief derivation](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient) results in this full term, the grad log of the current policy. This is the meat of VPG. This term is the gradient of the probability of the current trajectory with respect to the policy parameters, evaluated using the current parameters. For that reason, let's call this the probability gradient. ($$\nabla_\theta P(\tau)$$, where $$\tau$$ is the current trajectory).

In a moment, we're going to add the $$V$$ term we introduced in our bird's eye view. Let's review what we have so far without it: the probability gradient $$\nabla_\theta P(\tau)$$. 

- Without the sum over time steps ($$\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}$$), this tells us how much the probability distributions of our actions change, given a small change in each of our neural network parameters $$\theta$$. 
- With the sum over time steps ($$\sum_{t=0}^{T}\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}$$ ), it instead tells us how much the probability of that sequence of time steps (the trajectory $$\tau$$) changes, again for a small change in our parameters $$\theta$$. 

This is great: if we want to change the probability of a particular trajectory ($$P(\tau)$$), we have the information to do that! ($$\nabla_\theta P(\tau)$$)

The main remaining question is this: do we want our current trajectory $$\tau$$ to happen more or less often? Well, I think we can agree, we want to make good trajectories happen more often, and bad trajectories happen less often. So how good (or bad) is our current trajectory?

## Learning from rewards

The return $$R$$ of a trajectory $$\tau$$ is defined as the discounted sum of rewards obtained in that trajectory. The higher the return, the better the trajectory, and vice-versa. See the [RL post](/blog/2023/RL) for an intro to this concept. 

Following directly from the policy gradient discussion above: we now know not only how to change a trajectory's probability using $$\nabla_\theta P(\tau)$$, but also whether to make it more or less probable: each probability gradient in the sum is weighted by the return $$R(\tau)$$. That is, we have:

$$\hat{g}=\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} R $$

which, again, tells us both how to change the probability of actions leading to the trajectory $$\tau = (s_0, a_0, r_1), (s_1,a_1,r_2), (s_2, a_2, r_3),…$$, and now that we have the weight $$R(\tau)$$, we know whether to make the action $$a_t\vert s_t$$ more or less likely to happen. This summed over all time steps gives us the full policy gradient term.

### Congrats! You've made it to the policy gradient!

If you stopped here, this rudimentary policy gradient would fit into our algorithm above, and that would be able to solve simple environments, albeit a little inefficiently. A golden retriever might eventually be trained to drive a car, but man, is it going to be hard going. And you know that he's going to stop for treats and crash into mailmen. 

{% include figure.html path="assets/img/goldenRetriever.png" title="watered down vanilla ice cream" class="img-fluid rounded" %} 
<div class="caption">
He's trying, give him credit.
<d-footnote>From DALL-E</d-footnote>
</div>

No, we can do better than this. Take a breather, then let's take a step back and think. 

## Future return

If I take an action, what parts of the trajectory will this affect? I'm not currently in possession of a time machine, so any action I take will not affect the past.<d-footnote>History may be written by the victor, but recorded events do not necessarily reflect what actually transpired.</d-footnote> And of course if I take an action, that will carry me to the next time step, so I cannot affect the present either. I can only affect the future. 

Therefore, the part of the trajectory $$\tau$$ (and thus, the rewards $$r_t$$) that any action has bearing on is the **remainder of the trajectory**, from the next time step to the end of the episode. Let's call the discounted sum of *future* rewards "*future return*" $$R^f$$. Our past and present rewards only serve to add noise, or variance, to our estimate of how good the action $$a_t$$ is. This randomness slows down our training at best, and may reduce final performance.

{% details A note on terminology %}
This set of rewards is sometimes known as the "rewards-to-go." This term always struck me as kind of confusing, maybe because "to-go" isn't as precise as I'd like. About half the time I see it, I think "'to-go?' Where are the rewards going? Oh, you mean 'to-go' as in what we have to go, or remaining." Let's avoid any temporary confusion and use "future return."
{% enddetails %}

Let's be as clear as possible. 

Remember our trajectory $$\tau = (s_0, a_0, r_1), (s_1,a_1,r_2), (s_2, a_2, r_3),…$$. Assume $$T$$ time steps $$t$$ in a trajectory $$\tau$$; $$t \in \left[0,T\right)$$, or $$t \in \left[0,T-1\right]$$ if you prefer. Now consider the future return $$R^f_t$$ at various time steps:

- $$R^f_0$$, i.e. $$R^f$$ for the first time step ($$t=0$$) in a trajectory, contains all the rewards in that trajectory. Remember the first reward is $$r_1$$. 
- $$R^f_1$$ contains all rewards, except the first reward: $$r_2$$ onwards.
- $$R^f_{T-1}$$(the final time step) contains no rewards, because there's no future! 
- $$R^f_{T-2}$$ contains only the final reward. 

In other words,

$$R^f_{t} = \sum_{t'=t+1}^{T}\gamma^{t'-t-1}r_{t'}$$

where T is the total number of time steps in an episode (and we've remembered to include the discounting factor $$\gamma$$). 

### Overwrite your pointers and vocabulary? (Y/n)
With the above argument about not being able to affect the past, it turns out there's little reason to ever use the original full trajectory return $R$. So take a moment to internalize this concept. From here on out, we're just going to "overwrite" our previous terminology; any time we say "return" or $R$, we're talking about this new "future return" or $R^f$. It will be *implicit!* Let's modify the policy gradient to use our new understanding of return instead:

$$\hat{g}=\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} R_t $$

It looks the same? Oh yes, but now we understand it differently!

Now, how do we actually calculate $$R_t$$ for a particular time step? It seems obvious: simply take the discounted sum of all the future rewards! Well, it's a bit trickier than that, because I don't have a time machine that goes forward, either. 

The obstacle here is that you never have the future rewards for a time step as you're playing out that step.  You only have immediate access to the rewards you don't care about, those in the past. So you're forced to keep "rolling out" the trajectory to its conclusion, recording all future time steps, and only then you can accurately calculate $$R_t$$ for all time steps.

{% details Yeah ok, but how do you calculate it?? %}
Once you've collected the rewards, $$R_t$$ is easy enough to calculate in one backward-in-time pass with some simple dynamic programming.

Starting from the final time step, each $$R_t$$ equals the reward of the current time step plus the discounted return from the next time step. That is,

$$R_{T} = r_T$$

$$R_{T-1} = r_{T-1} + \gamma r_T$$

$$\vdots$$

$$R_{t} = r_t + \gamma R_{t+1}$$

Now just loop this backward in time.

{% enddetails %}

Whew. That seems a lot more tedious than we were hoping for, doesn't it?<d-footnote>If my episode is infinite, it's technically impossible. I can get around the infinite sum by remembering I'm discounting, and truncate my sum at a reasonable time in the future.</d-footnote> Imagine you're playing soccer for the first time, but the coach says you weren't allowed to correct your mistakes while you're playing! No no, only in the time between full games can you reflect on the error of your ways, and think how to improve for next time. Until then, suck it up and keep tripping over your feet. Intuitively, this seems inefficient, if not patently ridiculous. 

{% include figure.html path="assets/img/effective-soccer-coach.jpg" title="Ponder the error of your ways elsewhere, ye goblins" class="img-fluid rounded" %} 
<div class="caption">
No learning for you!
</div>

{% details Ok, I'm exaggerating a bit %}
Now, modern "offline" algorithms aren't actually this bad. They usually collect short sequences of transitions rather than full trajectories, and store them in a buffer to be sampled and asynchronously learned from. So the "reflection between games" is happening at all times, and the algorithm is "reflecting on all past games," so to speak, rather than the most recent experience. In other words, in "online" algorithms the state space currently being learned from is the same that's currently being explored by the active policy. However in "offline" algorithms the current policy (implicit or explicit) sets the explored state space, but the buffer of historical data encapsulates a larger and potentially different state space. There are pros and cons to each approach.
{% enddetails %}

I would like my algorithm to be "online," meaning I would like to avoid having to collect full episodes before knowing what my return $$R_t$$ is. If I could do that, I would be able to learn from the reward of every time step in real time, and update my policy accordingly. The smaller the time lag in learning, the faster I can gain information about my new policy, and update it again. So, how to access the return now instead of later?

## The value function

The value function $$V$$'s job is to approximate the return $$R$$ at whatever time step $$t$$ you need, and to generalize across states. A window into the future!

This is a lot simpler than it may seem. The value function $$V$$ answers one question: how good is the state $$s$$ to be in? Or, given the state $$s_t$$ at time step $$t$$, what is the best approximation $$V(s_t)$$ of the expected return? With a deep neural network, we can quickly dispatch this question with good old supervised learning. 

We have a trajectory of states $$s_t$$, and at the end of an episode (or the effective time horizon defined by the discounting term $$\gamma$$) we know we're able to calculate the return $$R_t$$ for each time step. We define the loss function for this supervised learning problem as the squared error between the value function $$V$$'s estimate and the actual return:


 $$\text{Loss}_{V_t}=\left(V(s_t)-R_t\right)^2$$

and minimize this Loss by regression with some kind of gradient descent, training our neural network to predict the return.

And by sticking $$V_t$$ in the place of $$R_t$$, we *almost* have the complete policy gradient:

$$\hat{g}=\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} V_t $$

Now for a few caveats.

### Bias and unstable exploration
This approximation may only see a subset of the state space at any given time, and it may *never* see the full state space. We cannot assume it is an *unbiased estimator* unless it has trained uniformly on data from all parts of the state space, and typically that will only be the case once the training is complete, if it happens at all. In parts of the state space which are relatively less explored, it may be *very inaccurate*, which could lead to learning what the value function **says** is an optimal policy, but in **reality** is nonsense. 

One of the best solutions to this problem is seen in the popular RL algorithm [PPO](https://arxiv.org/abs/1707.06347). Once you've understood VPG, you should work your way through [TRPO](https://spinningup.openai.com/en/latest/algorithms/trpo.html), and then [PPO](https://spinningup.openai.com/en/latest/algorithms/ppo.html). These links are decent references, though if you feel this blog post helped you understand VPG, let me know in the comments. If there's enough interest I'll continue these tutorials!

{% details The gist of PPO %}
PPO's goal is to avoid veering too far out into unexplored territory too quickly. It allows the value function time to "acclimate" to its new surroundings and give more accurate estimates, and so the policy is always learning from a "good enough" value function. It accomplishes this by limiting the size of the policy update step by clipping a surrogate loss function - but I won't digress too much on this point, it requires its own post to do it justice.
{% enddetails %}

### Variance

It also turns out that despite removing the noise from past rewards, this simple solution is still quite high-variance in practice. Because of this, it's difficult for the actor to learn a good policy and to do so quickly. Luckily there are many battle-hardened techniques for reducing variance. Read on for one such technique!


## N-step bootstrapping
What is the lowest bias approximation of the return $$R$$ for a state? Well, the return itself! That is, the actual sum of rewards that we calculate. 

Hol' up. Calculating $$R$$ requires collecting a full trajectory $$\tau$$. Aren't we trying to avoid that, to learn faster?

Indeed we are. So perhaps we can combine the two approaches. What if I use the actual reward in the next time step, and add it to the value function's approximation of the discounted return from that point on? Aha, a slightly more accurate estimate!

$$G_{t:t+1} = r_{t+1} + \gamma V(s_{t+1})$$

Perhaps we can do better still by using the next **two** real rewards instead of only one. Or the next **three**? Each addition will require a slightly longer delay between taking an action and being able to learn from it, because we need to collect a longer sequence out of the full trajectory. This is known as the **n-step return** $$G$$.

$$G_{t:t+2} = r_{t+1} + \gamma r_{t+2}+\gamma^2 V(s_{t+2})$$

$$\vdots$$

$$G_{t:t+n} = r_{t+1} + \gamma r_{t+2}+ \cdots +\gamma^{n-1}r_{t+n}+\gamma^n V(s_{t+n})$$

Now the question becomes: what's the optimal tradeoff between accuracy and this learning delay? At this point you just need to experiment and see what works best for your problem, but I will tell you this: the answer is somewhere between 100% accuracy and zero delay, as seen in this figure:

{% include figure.html path="assets/img/nStepReturn.png" title="Bootstrapping" class="img-fluid rounded" %} 
<div class="caption">
The performance of n-step return with various value of n, for a random walk task<d-footnote>Figure 7.2 from Sutton and Barto. [http://incompleteideas.net/book/the-book-2nd.html] Buy their book!</d-footnote>. The learning rate $\alpha$ is on the x axis. You can see a minimum error for n=4 in this particular instance. YMMV.
</div>


It also turns out you can do even better by doing a weighted average of all of these options: one, two, three, etc. real rewards, followed by an approximation, and also experimenting to see what the right weighting is. There is a similar tradeoff between accuracy and computation speed, yielding a chart like the one above. Optimizing these hyperparameters is problem dependent.



## Generalized Advantage Estimation
We have a more accurate approximation with our n-step return $$G$$. We've left out one important piece, though: advantage. Before we add it, let's talk about why we need it.

### Variance reduction

The n-step return handily deals with the bias problem in approximating the value function, but doesn't do much to help the variance. To reduce the variance we need to understand where it comes from. 

In any trajectory, there are several sources of randomness contributing to what direction it takes as it's rolled out: 
- We may have a random initial state $$s_0$$.
- For a given policy $$\pi$$ and state $$s_t$$, in general we **sample** actions $$a_t$$ from a distribution.
- The environment may have probabilistic transitions. For a given state $s$ and action $a$, in general, the resulting transition to the next state $s_{t+1}$ is *also* sampled from a probability distribution, $$p(s' \vert s, a)$$. We swept this one under the rug until now (and will continue to do so for the rest of this post, so don't worry).

Each variation at each time step ultimately leads to the variance of the return $$R$$. The longer the trajectory, the more pronounced this effect will become. Therefore, we want to remove the effect of this future variation.


### Advantage


We know $$G_t$$ is a bit more accurate than $$V(s_t)$$, because it includes a real value based on the action we just took, so this is the expression we'd like a low-variance estimate of. 


Since we're in the business of using "real-time" approximations, our **accurate** estimate of $$R$$ is the n-step return, i.e. something that looks like $$r_{t+1} + \gamma V(s_{t+1})$$. So we go from:

$$A_t = R_t - V(s_t)$$

to

$$A_t = r_{t+1} + \gamma V(s_{t+1}) $$

I'll start with the punchline: it turns out you can take the expression you want (the n-step return), and subtract another similar expression (a baseline) to reduce the variance of your approximation. The most common choice for this baseline is the value function, so the above becomes:

$$A_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)$$

or more generally, 

$$A_t = G_{t:t+n} - V(s_t)$$

Now hold on. This is just the same as we had before. It's the n-step return, but for some reason we're subtracting the value function evaluated on the current state. What's the point?

Intuitively, the advantage at its most *basic* level answers the question: how much better is the *actual* return $$R$$ better than my estimate $$V(s)$$? In a sense, it's a measure of surprise. If taking an action $$a_t$$ given the state $$s_t$$ and transitioning to a state $$s_{t+1}$$ is more rewarding than we think that state is in general ($$V(s)$$), then surely we ought to adjust our policy to do that more often. 

{% details Ok, but why does it *actually* work? %}
The explanation for this is usually swept under the rug or banished to a "see this reference" link, but as it's not perfectly intuitive, I'd like to at least give the basic version.

To begin with, we need to consider at a higher level what we're trying to accomplish. Consider the policy gradient we've derived so far:

$$\hat{g}=\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} (r_{t+1} + \gamma V(s_{t+1})) $$

This term is the sum over time steps in a **single** trajectory $$\tau$$. 

Now for the new part. The above term actually gives us a sample of the optimal policy gradient for **all** possible trajectories. What we would ideally like is the expectation over all trajectories:

$$\mathbb{E}\left[\hat{g}\right]=\mathbb{E}\left[\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} (r_{t+1} + \gamma V(s_{t+1})) \right]$$

$$=\int_{\mathcal{T}} \sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} (r_{t+1} + \gamma V(s_{t+1})) d\tau$$

where $\mathcal{T}$ is the set of all possible trajectories $$\tau$$.

In order to calculate the true optimal policy gradient, we would need to collect every possible trajectory, which of course is intractable for any environment of more than trivial size. Instead, we collect a set $$\mathcal{D}$$ of some batch size (say, 128) trajectories $$\tau$$, and compute a sampled (Monte Carlo) expectation:

$$\approx \frac{1}{\left|\mathcal{D}\right|}\sum_{\tau\in\mathcal{D}}\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} (r_{t+1} + \gamma V(s_{t+1})) $$

Now, our goal is to reduce the variance of this policy gradient estimate. Let's revert to the integral form:

$$\int_{\mathcal{T}} \sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} (r_{t+1} + \gamma V(s_{t+1})) d\tau$$

and encapsulate everything into an integral of one function:

$$\int_{\mathcal{T}}f(\tau)$$

Now add and subtract the integral of a similar function we can evaluate, $$\phi \approx f$$:

$$\int_{\mathcal{T}}f(\tau) = \int_{\mathcal{T}}\left(f(\tau)-\phi (\tau)\right)+\int_{\mathcal{T}}\phi(\tau)$$

Instead of estimating $$\int_{\mathcal{T}}f(\tau)$$ (our original problem), we estimate $$\int_{\mathcal{T}}\left(f(\tau)-\phi (\tau)\right)$$. Now 

$$\text{Var}(f-\phi) = \text{Var}(f) - 2 \text{Cov}(f,\phi)+\text{Var}(\phi)$$

 Since $$\phi\approx f$$, $$\text{Cov}(f,\phi)$$ is positive. If $$- 2 \text{Cov}(f,\phi)+\text{Var}(\phi)$$ is negative, $$\text{Var}(f-\phi) \lt \text{Var}(f)$$, so $$\int_{\mathcal{T}}\left(f(\tau)-\phi (\tau)\right)$$, the new thing we need to estimate, has lower variance. This is exactly what we're looking for!

Are you beginning to notice the similarity to our punchline above? Since it may not be completely obvious, I'll make it painfully explicit. Starting with

$$\int_{\mathcal{T}} \sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} (r_{t+1} + \gamma V(s_{t+1})) d\tau$$

we subtract $$V(s_t)$$:

$$\int_{\mathcal{T}} \sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} (r_{t+1} + \gamma V(s_{t+1})-V(s_t)) d\tau$$

$$=\int_{\mathcal{T}} \sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} (r_{t+1} + \gamma V(s_{t+1})) d\tau - \int_{\mathcal{T}} \sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} (V(s_t)) d\tau$$

$$=\int_{\mathcal{T}} f(\tau) - \int_{\mathcal{T}} \phi(\tau)=\int_{\mathcal{T}} f(\tau) - \phi(\tau)$$

Now just convert all that to the Monte Carlo, sampled-over-trajectories version, and Bob's your uncle. This is why subtracting the value function from the n-step return reduces the variance of the samples from all trajectories.

One last thing. We added $$\int_{\mathcal{T}}\phi(\tau)$$ so that we were still calculating $$\int_{\mathcal{T}}f(\tau)$$, so shouldn't we add $$V(s_t)$$ back in somehow? Turns out you don't have to; subtracting $$\phi$$ has no effect on the bias of the Monte Carlo estimate, i.e. the expectation over trajectories.

I will banish *some* things to a "see this reference" link. If any of this seems suspect, take a look [here](https://www.jmlr.org/papers/volume5/greensmith04a/greensmith04a.pdf). See equation 7 in that reference for why we can subtract a baseline without affecting the bias of the estimate. If you're still not convinced this reduces variance, take a look [here](https://en.wikipedia.org/wiki/Control_variates) and perhaps also at section 5.5 (p.59) in [this](http://www.cs.fsu.edu/~mascagni/Hammersley-Handscomb.pdf) book on Monte Carlo methods.


{% enddetails %}


With the addition of advantage, we have generalized advantage estimation (GAE) in a nutshell.  I've focused on intuition here. For more specifics in how these calculations are done, see the [paper](https://arxiv.org/abs/1707.06347); your mental picture should now be organized to have all those formalisms swiftly fall into place.

We can now convert our $$V_t$$ to $$A_t$$.

$$\hat{g}=\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} A_t $$

Whyarewe ok with subtracting a term? is this legal?

It is worth noting that GAE is not limited to on-policy methods; it can be applied anywhere you use a value function, or even a Q function.


## Batched training of neural nets
{% include figure.html path="assets/img/cookie.jpg" title="kooky" class="img-fluid rounded" %} 
Finally, we sum this term over a batch of collected trajectories, dividing by the number of trajectories to get the sampled mean of the above equation. 

Why? Well, to gain a lower variance estimator of the true policy gradient. This should be quite familiar to you if you've ever done mini-batched stochastic gradient descent. Many trajectories averaged together will smooth out the noise from any one trajectory, and better represent the optimal policy gradient.

### Policy gradient update

Ok, finally, we have the sampled policy gradient!

$$\hat{g}=\frac{1}{\left| \mathcal{D}_k \right|}\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} A_t $$

Where k is the step index in the training loop.
With this term, we can update the parameters of our policy with SGD:

$$ \theta_{k+1} = \theta_{k}+\alpha_k \hat{g}_k $$ 

or otherwise use an optimizer like Adam.

### Value function update

This is the policy learning taken care of. Now we turn to updating the value function. Recalling its loss function:

$$ \text{Loss}_{V_t}=\left(V_\phi(s_t)-\hat{R}_t  \right)^2 $$

We add a sum over all time steps in the trajectory, and a sum over all trajectories in our gathered set of trajectories $$\mathcal{D}$$:

$$ \frac{1}{\left\vert  \mathcal{D}_k \right\vert T}\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\left(V_\phi(s_t)-\hat{R}_t  \right)^2 $$

We divide by T because we're not doing a sum of log likelihoods like for the policy gradient, but instead are calculating mean squared error.

$$ \phi_{k+1}=\arg \min_\phi \frac{1}{\left\vert  \mathcal{D}_k \right\vert T}\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\left(V_\phi(s_t)-\hat{R}_t  \right)^2 $$

Standing in for the argmin, we can use SGD or Adam to compute the update to the value function parameters $$\phi$$.

With each gradient descent step, the value function, on average, better approximates the real sum of rewards. The policy then has a more accurate value function with which to adjust its own actions. Together, they explore the state and action space until a locally optimal policy is reached. Though every step is not guaranteed to improve due to approximation error, even this "vanilla" reinforcement learning algorithm can learn to complete simple tasks. 

Now you can put all these ingredients together. Convince yourself that you understand the full algorithm!

# The main issue I have is there are few visual aids. Think about this. Manim?

***

{% include figure.html path="assets/img/pseudoVPG.svg" title="Pseudocode" class="img-fluid rounded" %} 

Go over it and see that everything lines up for you.

***

Thanks for reading! 

Now we can take a look at an implementation in JAX in the [next post](/blog/2023/VPGInJax).



