---
layout: distill
title: Vanilla Policy Gradient
description: The theory behind one of our introductory algorithms
giscus_comments: true
date: 2023-04-14
tags: rl 
authors:
  - name: Kenneth Jabon

  

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: So, you're interested in RL
  - name: A Simple Environment
  - name: Introducing the Policy
  - name: Imperfect information
  - name: Infinitely long episodes
  - name: Discounted rewards 
  - name: Probabilistic state transitions

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---



This post is about vanilla policy gradient, a simple reinforcement learning algorithm. It should serve to form the theoretical foundation for all following policy-based, online algorithms. If you're more interested in offline algorithms, I recommend you start with TD-learning and deep Q-learning. See chapter 6 of [Sutton and Barto](http://incompleteideas.net/book/RLbook2020.pdf).

This post assumes you're familiar with how Markov Decision Processes work, and how the reinforcement learning problem is set up. For a guide to that, see [here](/blog/2023/RL).

## A bird's eye view

Let's describe what happens in a full iteration of the loop before diving in.

First, we collect a batch of trajectories $$\tau$$ by allowing our current policy $$\pi$$ (represented by a neural network) to unfold in the environment. We collect a full batch because, in general, the policy does not output actions, but a probability distribution over actions given the current state: $$\pi(a\vert s)$$. To get to the next step, we select an action by sampling from this distribution. Randomness may also occur as the result of the environment itself, so all in all we want to get plenty of samples to come up with an accurate representative set of trajectories $$\tau$$. 

{% details Sampling is great for blind exploration, but... %}
Eventually sampling doesn't do the job any more. At some point we'll want to exploit what we know and make our way to better states to be able to learn from there, so we'll just pick the best known action instead of sampling. The amount of "greedy action selection" will be scheduled to increase over time, to progress from pure exploration of the state space to pure exploitation of the policy.
{% enddetails %}

Now, if a policy gains an above-average sum of rewards in a particular trajectory, we will nudge the policy in the direction of the actions (given their respective states) which resulted in this trajectory. Conversely, if a trajectory comes with a lower reward, we nudge the policy away from taking those actions. This sum is known as the **return** for a trajectory<d-footnote>This isn't the full return yet, we'll make it more general in a bit.</d-footnote>:

$$
R(\tau)=\sum_{t=0}^{T}r_t
$$

To estimate the return at each time step with low variance, we employ generalized advantage estimation. We train a neural network to represent the value function V_t, which is incorporated into our advantage term A_t.

Now, loop by loop, we have a better estimate of what a good trajectory looks like (by training the value function $$V$$), and a better idea of what actions to take to get good trajectories (by training the policy $$\pi$$). This glosses over some details, which we'll get into in just a bit.

Take a glance over the pseudo-code for the simplified algorithm, then let's get cracking!




## Breaking down the policy gradient


The policy gradient is calculated as follows:


$$ \hat{g}_{k}=\frac{1}{\left\vert \mathcal{D}_k \right\vert }\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} \hat{A}_t $$ <-actually, just use V_t

Let's break this down to gain an understanding of each term, working from the inside out.

Is the policy, or the function which outputs the probability distribution of all possible actions at time t in the trajectory, given the state at time t. To bring it back to earth, think of this as y = f(x,theta). 

>>If this is making you feel like a deer in headlights, don't worry. Let's be more explicit. Theta parameterizes our function f, and then we evaluate our function on x. What do I mean? If I had a function y = f(x, theta) = mx + b, I parameterize this function with a and b (represented by the vector theta = [m,b]), and then evaluate it on x. In the case of a neural net, theta is instead a matrix of numbers (which can be flattened into a vector [a1,a2,a3,&,an]) representing its weights and biases.<

- If we have a discrete action space, this is a function which outputs a vector with a value for each possible action (in other words, a categorical probability distribution). 
- If the action space is continuous, this is a function which outputs the mean and variance representing the action's probability distribution. Remember when we output the mean and variance from our model earlier?


Take the log of this term. Remember, all that is happening is we're taking the log of a function, i.e., g(x, theta) = log(f(x, theta)).


We take the gradient of this with respect to the policy parameters. Simply, the gradient of a function, i.e.  Del(g(x, theta)) with respect to theta.


We evaluate the policy given the current policy parameters (we perform inference with the current model on the current state of the environment). 

>> Now, let's take a step back. If we are used to working out derivations with pencil and paper, the order in which I presented the last few steps should not start sounding any alarms. Normally in this case one would take the derivative of the symbolic function, then evaluate that to get the derivative at the point of interest, or a similar method of your choice.

[footnote: assuming you made it through Calculus in one piece. If not, don't worry. Go ahead and take or re-take Calculus, and then come back. You can do it, I promise. If that's too much hassle, luckily we have autodiff which means you can summarily forget about this derivation; so just keep reading with a glazed look until we get back to the code.] 

However, that's not exactly what happens on your computer. Instead, we just say hello to our new best friend, the autodiff function (for us, this is jax.grad( ) ). I'm going to run the code which evaluates ___ for an entire batch of states in one vectorized/parallelized operation, across whatever computational resources I have available. Can you please give me ___? And our new best friend is happy to oblige. More on how to do that in a bit. 

How does our new friend work? I'll mostly defer to good explanations elsewhere, for example [here](https://github.com/HIPS/autograd/blob/master/docs/tutorial.md#whats-going-on-under-the-hood) and [here](https://en.wikipedia.org/wiki/Automatic_differentiation). The gist is that first, it keeps track of the computation graph of whatever functions you want to differentiate, from state and theta all the way to ___. Then, it uses this graph to multiply its way through the chain rule, resulting in the gradient(s) you want. And of course, this is done in parallel for all the time steps in your trajectory. Now, back to our regularly scheduled programming. <<






We sum this term over all time steps in our current trajectory. >>A [brief derivation](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient) results in this full term, the grad log of the current policy.<< This is the meat of VPG. This term is the gradient of the probability of the current trajectory with respect to the policy parameters, evaluated at the current parameters. For that reason, let's call this the probability gradient. (Grad_theta P(tau), where tau is the current trajectory).

In a moment, we're going to add the V_t term. Let's review what we have so far without it: the probability gradient Grad_theta P(tau). Without the sum over time steps, this tells us how much the probability distributions of our actions change, given a small change in each of our neural network parameters. Re-adding the sum over time steps, it instead tells us how much the probability of that sequence of time steps (the trajectory) changes, again for a small change in our parameters. This is great: if we want to change the probability of a particular trajectory, we have the information to do that!

The main remaining question is this: do we want our current trajectory to happen more or less often? Well, I think we can agree, we want to make good trajectories happen more often, and bad trajectories happen less often. So how good (or bad) is our current trajectory?

# Learning from rewards

The return of a trajectory is defined as the discounted sum of rewards obtained in that trajectory. The higher the return, the better the trajectory, and vice-versa. See the RL post() for an intro to this concept. Following directly from the policy gradient discussion above: we now know not only how to change a trajectory's probability, but also whether to make it more or less probable: each probability gradient in the sum is weighted by the return!

To be explicit: if an action corresponds to an above-average return, we use that information combined with the probability gradient at that time step to nudge the policy in the direction of the action (given the state it was acting on) which resulted in the future return from that time step. (And vice-versa). This summed over all time steps gives us the full policy gradient term.

Show the equation again, except with R_t instead of V_t

Let's take a step back and think about this. 

If I take an action, what parts of the trajectory will this affect? I'm not currently in possession of a time machine, so any action I take will not affect the past. >Even if history is written by the victor, recorded events do not necessarily reflect what actually transpired.< And of course if I take an action, that will carry me to the next time step, so I cannot affect the present either. I can only affect the future. Therefore, the part of the trajectory (and thus, the rewards) that any action has bearing on is the remainder of the trajectory from the next time step to the end of the episode. I'll call the discounted sum of future rewards "future return" or R^f.
>>This set of rewards is sometimes known as the "rewards-to-go." This term always struck me as kind of confusing, maybe because "to-go" isn't as precise as I'd like. About half the time I see it, I think "to-go?' Where are the rewards going? Oh, you mean to-go' as in what we have to go, or remaining." Let's avoid any temporary confusion and use "future return."

Show the equation again, except with R^f_t instead.

Let's be as clear as possible. Assume T time steps t in a trajectory, t in [0,T), or t in [0,T-1] if you prefer. The future return R^f_t for the first time step (t=0) in a trajectory, R^f_0, contains all the rewards in that trajectory. R^f_1 contains all rewards, except the first reward. R^f_T-1(the final time step) contains no rewards, because there's no future! R^f_T-2 contains only the final reward.

Now, how do we actually calculate R^f_t for a particular time step? It seems obvious: simply take the discounted sum of all the future rewards! Well, it's a bit trickier than that. The obvious obstacle to this is that you never have the future rewards for a time step as you're playing out that step. I don't have a time machine that goes forward, either. You only have immediate access to the rewards you don't care about, those in the past. So you're forced to keep "rolling out" the trajectory to its conclusion, recording all future time steps, and only then you can accurately calculate R^f_t for all time steps. >>R^f_t is easy enough to calculate in one backward-in-time pass. Starting from the final time step, each R_t equals the reward of the current time step plus the sum of the rewards from the next time step.<<

Whew. That seems a lot more tedious than we were hoping for. >>If my episode is infinite, it's technically impossible. I can get around the infinite sum by remembering I'm discounting, and truncate my sum at a reasonable time in the future. << I would like my algorithm to be "online," meaning I would like to avoid having to collect full episodes before knowing what my future return is. If I could do that, I would be able to learn from the reward of every time step in real time, and update my policy accordingly. 

Imagine if you were playing soccer for the first time, but the coach said you weren't allowed to correct your mistakes while you were playing! No no, only in the time between full games can you reflect on the error of your ways, and think how to improve for next time. Until then, suck it up and keep tripping over your feet. Intuitively, this seems inefficient, if not patently ridiculous. 

>>Now, modern "offline" algorithms aren't actually this bad. They usually collect short sequences of transitions rather than full trajectories, and store them in a buffer to be sampled and asynchronously learned from. So the "reflection between games" is happening at all times, and "reflecting on all past games," so to speak, rather than the most recent experience. In other words, in "online" algorithms the state space currently being learned from is the same that's currently being explored by the active policy. However in "offline" algorithms the current policy (implicit or explicit) sets the explored state space, but the buffer of historical data encapsulates a potentially different state space. There are pros and cons to each. <<

The smaller the time lag in learning, the faster I can gain information about my new policy, and update it again. So, how to access the future return now instead of later? Approximate it with the value function V! 

# The value function. 

The value function's (V_t) job is to approximate the return for a particular time step. This is a lot simpler than it may seem. The value function answers one question: how good is this state to be in? Or, given the state at time step t, what is the best approximation of the expected return? With a deep neural network, we can quickly dispatch this question with good old supervised learning. We have a trajectory of states at each time step, and at the end of an episode (or the effective time horizon defined by the discounting term gamma) we're able to calculate the future return for each time step. All that follows is to state the loss function to minimize, and we're off to the races. (Divide this by T)


 


And with that, here is the full policy gradient:
(Math, no sum over trajectories).

A few caveats. 

This is an approximation which may only see a subset of the state space at any given time, and it may never see the full state space. We cannot assume it is unbiased unless it has trained equally as much on data from all parts of the state space, and typically that will only be the case once the training is complete, if it happens at all. In parts of the state space which are relatively less explored, it may even be very inaccurate, which could lead to learning a falsely optimal policy. 

One of the best solutions to this problem is seen in the RL algorithm (PPO)[paper]. >>Its goal is to avoid veering too far out into unexplored territory too quickly. It allows the value function time to "acclimate" to its new surroundings and give more accurate estimates, and so the policy is always learning from a "good enough" value function. It accomplishes this by limiting the size of the policy update step by clipping a surrogate loss function - but I won't digress too much on this point in this post, it requires more time to do it justice.<<

It also turns out that this simple solution is quite high-variance in practice. Because of this, it's difficult for the actor to learn a good policy and to do so quickly. Luckily there are many battle-hardened techniques for reducing variance. 

Take a look at [generalized advantage estimation](gae paper) (GAE) for lower-variance, lower bias estimates of how much to update policy parameters. 

# Let me give a summary of GAE. 
What is the lowest bias approximation of the return for a state? Well, the return itself! That is, the actual sum of rewards that we calculate. "But hang on, I thought we were trying to avoid doing that to get quick feedback?" Indeed we are. So perhaps we can combine the two approaches. What if I use the actual reward in the next time step, and add it to the value function's approximation of the future return from that point on? Aha, a slightly more accurate estimate!

Perhaps we can even do a little better by using the next two real rewards instead of only one. Or the next three? Each addition will require a slightly longer delay between taking an action and being able to learn from it. Now the question becomes: what's the optimal tradeoff between accuracy and this learning delay? At this point you just need to experiment and see what works best for your problem, but I will tell you this: the answer is somewhere between 100% accuracy and zero delay (show the chart here). It also turns out you can do even better by doing a weighted average of all of these options: one, two, three, etc. real rewards, followed by an approximation, and also experimenting to see what the right weighting is. That's GAE in a nutshell. The sum will of course be discounted as usual, so all but the first term have \gamma^t applied to them.

We've left out one important piece, though. This isn't actually "Advantage" yet: we haven't addressed how to lower the variance of our approximation. Okay. We have a more accurate approximation with our "immediate reward r_t plus delayed value function V(s_t+1)," as compared to the non-delayed value function V(s_t) alone. What is the source of the variance? Well, the approximation of the value function, of course. Since the value function at each time step is approximated by the same neural network, we should expect it to contribute similar "noise" or "variance" to our "signal" or "true value." If we subtract "pure approximation" V(s_t) from "slightly more accurate approximation" r_t + V(s_t+1), we remove variance and are mostly left with the true value we care about: the advantage A_t, or how much better is this action than average? Putting all this together gives us GAE: a low variance estimator for the advantage.

I've focused on intuition here. If you're interested in the math of how the weighted averaging and so on works out, go take a look at the paper; your mental picture should now be organized to have all those formalisms swiftly fall into place. Finally we can convert our V_t to A_t.





# Batched training of neural nets
Finally, we sum this term over a batch of collected trajectories, dividing by the number of trajectories to get the sampled mean of the above (the grad log time-step summation). Why? Well, to gain a lower variance estimator of the true policy gradient. This should be quite familiar to you if you've ever done mini-batched stochastic gradient descent. Many trajectories averaged together will smooth out the noise from any one trajectory, and better represent the optimal policy gradient, or the step towards the true optimal policy.

Ok, finally, we have the sampled policy gradient: (show full eqn)



With this term, we can update the parameters of our policy, which in this case is an MLP.

$$ \theta_{k+1} = \theta_{k}+\alpha_k \hat{g}_k $$ 




Refers to the updated parameters,  the parameters to be updated, and the learning rate (a hyper parameter).

This is the policy learning taken care of. Now we turn to the value function, doing the same sum over trajectories, for the same reason:









All we have to do is perform inference with the model at all time steps along the trajectory, for all collected trajectories, and we have all the ingredients we need to perform batched gradient descent of the model to minimize the loss function, as shown below.
$$ \theta_{k+1}=\arg \min_\phi \frac{1}{\left\vert  \mathcal{D}_k \right\vert T}\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\left(V_\phi(s_t)-\hat{R}_t  \right)^2 $$
With each gradient descent step, the value function, on average, better approximates the real sum of rewards. The policy then has a more accurate value function with which to adjust its own actions. Together they explore the state and action space until a quality policy is reached. Though every step is not guaranteed to improve due to approximation error, even this "vanilla" reinforcement learning algorithm can learn to complete simple tasks. 

This previously scary-looking algorithm should now be more approachable. Go over it and see that everything lines up for you, then we can take a look at the code.

