---
layout: distill
title: Vanilla Policy Gradient
description: Theory and intuition behind one of our introductory algorithms
giscus_comments: true
date: 2023-04-14
tags: rl 
authors:
  - name: Kenneth Jabon

  

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: A bird's eye view
  - name: Breaking down the policy gradient
  - name: Learning from rewards
  - name: The value function
  - name: Generalized Advantage Estimation
  - name: Batched training of neural nets

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---



Vanilla policy gradient is one of the simplest reinforcement learning algorithms. It should serve to form the theoretical foundation for all following policy-based, online algorithms. If you're more interested in offline algorithms, I recommend you start with TD-learning and deep Q-learning. See chapter 6 of [Sutton and Barto](http://incompleteideas.net/book/the-book-2nd.html) for that.

This post assumes you're familiar with how Markov Decision Processes work, and how the reinforcement learning problem is set up. For a guide to that, see [here](/blog/2023/RL).

## A bird's eye view

Let's describe what happens in a full iteration of the loop before diving in.

First, we collect a batch of trajectories $$\tau$$ by allowing our current policy $$\pi$$ (represented by a neural network) to unfold in the environment. We collect a full batch because, in general, the policy does not output actions $$a$$, but a probability distribution over actions given the current state $$s$$: $$\pi(a\vert s)$$. To get to the next timestep in our trajectory, we select an action $$a$$ by sampling from this distribution. Randomness may also occur as the result of the environment itself, so all in all we want to get plenty of samples to come up with an accurate, representative set of trajectories $$\tau$$. 

{% details Sampling is great for blind exploration, but... %}
Eventually sampling doesn't do the job any more. At some point we'll want to exploit what we know and make our way to better states to be able to learn from there, so we'll just pick the best known action instead of sampling. The amount of "greedy action selection" will be scheduled to increase over time, to progress from pure exploration of the state space to pure exploitation of the policy.
{% enddetails %}

Now, if a policy gains an above-average sum of rewards in a particular trajectory, we will nudge the policy $$\pi$$ in the direction of the actions $$a$$ (given their respective states, $$\vert s$$) which resulted in this trajectory. Conversely, if a trajectory comes with a lower sum of rewards, we nudge the policy away from taking those actions. This sum is known as the **return** for a trajectory<d-footnote>This isn't the full return yet, we'll make it more general in a bit.</d-footnote>:

$$
R(\tau)=\sum_{t=0}^{T}r_t
$$

If any of this feels poorly defined or explained, you'll want to start with the [previous](/blog/2023/RL) post.

To estimate the return at each time step with low variance, we employ generalized advantage estimation (GAE). We train a neural network to represent the value function $$V$$, which is incorporated into our advantage term $$A$$. This will be discussed in the secion on [GAE](#generalized-advantage-estimation). Also, we actually estimate not the return, but the "future return." More on that in a bit!

Now, every time we go through our loop, we have a better estimate of what a good trajectory looks like (by training the value function $$V$$), and a better idea of what actions to take to get good trajectories (by training the policy $$\pi$$). This glosses over some details, which we'll get into in just a bit.

Take a glance over the simplified pseudocode for the algorithm, then let's get cracking!

{% include figure.html path="assets/img/pseudoPseudoVPG.svg" title="pseudoPseudo" class="img-fluid rounded" %}


## Breaking down the policy gradient


The policy gradient is calculated as follows:


$$ \hat{g}_{k}=\frac{1}{\left\vert \mathcal{D}_k \right\vert }\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} \hat{V}_t $$

Let's break this down to gain an understanding of each term, working from the inside out.

$$\pi_\theta (a_t\vert s_t)$$ is the policy, or the function which outputs the probability distribution of all possible actions $$a_t$$ at time $$t$$ in the trajectory $$\tau$$, given the state $$s_t$$ at time $$t$$. To bring it back to earth, think of this as $$y = f(x,\theta)$$. 

 {% details O_O %}
If this is making you feel like a deer in headlights, don't worry. Let's be more explicit. $$\theta$$ parameterizes our function $$f$$, and then we evaluate our function on $$x$$. What do I mean? If I had a function $$y = f(x, \theta) = mx + b$$, I parameterize this function with $$m$$ and $$b$$ (represented by the vector $$\theta = [m,b]$$), and then evaluate it on $$x$$. In the case of a neural net, $$\theta$$ is instead a matrix of numbers (which can be flattened into a vector $$[a_1,a_2,a_3,...,a_n]$$) representing its weights and biases.
 {% enddetails %}


- If we have a discrete action space, this is a function which outputs a vector with a value for each possible action (in other words, a categorical probability distribution). 
- If the action space is continuous, this is a function which outputs the mean $$\mu$$ and standard deviation $$\sigma$$ representing the action's probability distribution. <d-footnote> If you're worried about flexibility, it turns out you can also output arbitrary combinations of these, but we won't consider that case here.</d-footnote>



**Take the log** of the policy: 

$$\log\pi_\theta (a_t\vert s_t)$$ 

Remember, all that is happening is we're taking the log of a function, i.e., $$g(x, \theta) = \log(f(x, \theta))$$.
<br>

**Take the gradient** of this with respect to the policy parameters $$\theta$$.

$$\nabla_\theta \log\pi_\theta (a_t\vert s_t)$$  

Simply, the gradient of a function, i.e.  $$\nabla g(x, \theta)$$ with respect to $$\theta$$.
<br>

**Evaluate the policy** given the current policy parameters $$\theta_k$$:

$$\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}$$  

Or, we perform inference with the current model on the current state of the environment. 


 {% details Numerical considerations %}
Let's take a step back. If we are used to working out derivations with pencil and paper, the order in which I presented the last few steps should not start sounding any alarms.<d-footnote>Assuming you made it through Calculus in one piece. If not, don't worry. Go ahead and take or re-take Calculus, and then come back. You can do it, I promise. If that's too much hassle, luckily we have autodiff which means you can summarily forget about this derivation; so just keep reading with a glazed look for a couple more paragraphs. </d-footnote>

Normally in this case one would take the derivative of the symbolic function, then evaluate that to get the derivative at the point of interest, or a similar method of your choice.



However, there are two differences when doing this numerically on your computer; having to do with the derivative and the log. 

### For the derivative:
Instead, we just say "hello, new best friend, the autodiff function <d-footnote>For example, jax.grad()</d-footnote>! I'm going to run the code which evaluates $$\log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}$$ for an entire batch of states in one vectorized/parallelized operation, across whatever computational resources I have available. Can you please give me $$\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}$$?" And our new best friend is happy to oblige. Implementation details can be found in the next [post](/blog/2023/VPGInJax). 

How does our new friend work? I'll mostly defer to good explanations elsewhere, for example [here](https://github.com/HIPS/autograd/blob/master/docs/tutorial.md#whats-going-on-under-the-hood) and [here](https://en.wikipedia.org/wiki/Automatic_differentiation). The gist is that first, it keeps track of the computation graph of whatever functions you want to differentiate, as you execute them, from state $$s$$ and parameters $$\theta$$ all the way to $$\log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}$$. Then, it uses this graph to multiply its way through the chain rule, resulting in the gradient(s) you want. 

### For the log:
We're using a neural network, whose job it is to approximate a function's output. Well, why go through the trouble of approximating a function, and then taking the log of the output, when we could just encapsulate the log into the function and approximate that directly? And this exactly what we do. We're interested in the log of the output of a probability distribution (an array of log-likelihoods, one for each action). 

**For categorical distributions**, it's very simple.

We just consider the output of the network to be the array: $$\log \left[ P_\theta (s)\right] $$.

**For continuous distributions**, it's a little trickier

Remember we usually represent the probability distribution in the continuous case as a multivariate normal distribution. To keep things simple, we actually use a diagonal covariance matrix, rather than the full covariance matrix, so each dimension of the action can be represented by a single standard deviation. This way, we only need to output one mean and one standard deviation per dimension, and calculating the log of the distribution also becomes much easier. Now, how do we take the log of a (diagonal) normal distribution? 

Like this! 

$$\log\pi_\theta (a\vert s) = -\frac{1}{2}\left(n\log 2\pi+\sum_{i=1}^n\left[\frac{(a_i-\mu_i)^2}{\sigma_i^2}+2\log\sigma_i\right]\right)$$

where n is the dimensionality of the action space.

[Here's a derivation](/blog/2023/ContinuousLogLikelihood/). Also, take a look [here](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#stochastic-policies) for another take on this issue (I did!).

Remember that our neural network spits out a vector each of $$\mu$$ and $$\sigma$$, one element for each action dimension. With these values, we sample from a diagonal MVN distribution, yielding a vector of actions $$a$$. The above equation describes the log likelihoods of that vector of actions.



Okay, moving on.





 {% enddetails %}


**Sum this term** over all time steps in our current trajectory.

$$\sum_{t=0}^{T}\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}$$  

A [brief derivation](https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient) results in this full term, the grad log of the current policy. This is the meat of VPG. This term is the gradient of the probability of the current trajectory with respect to the policy parameters, evaluated using the current parameters. For that reason, let's call this the probability gradient. ($$\nabla_\theta P(\tau)$$, where $$\tau$$ is the current trajectory).

In a moment, we're going to add the $$V$$ term we introduced in our bird's eye view. Let's review what we have so far without it: the probability gradient $$\nabla_\theta P(\tau)$$. 

- Without the sum over time steps ($$\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}$$), this tells us how much the probability distributions of our actions change, given a small change in each of our neural network parameters $$\theta$$. 
- With the sum over time steps ($$\sum_{t=0}^{T}\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}$$ ), it instead tells us how much the probability of that sequence of time steps (the trajectory $$\tau$$) changes, again for a small change in our parameters $$\theta$$. 

This is great: if we want to change the probability of a particular trajectory ($$P(\tau)$$), we have the information to do that! ($$\nabla_\theta P(\tau)$$)

The main remaining question is this: do we want our current trajectory $$\tau$$ to happen more or less often? Well, I think we can agree, we want to make good trajectories happen more often, and bad trajectories happen less often. So how good (or bad) is our current trajectory?

## Learning from rewards

The return $$R$$ of a trajectory $$\tau$$ is defined as the discounted sum of rewards obtained in that trajectory. The higher the return, the better the trajectory, and vice-versa. See the [RL post](/blog/2023/RL) for an intro to this concept. 

Following directly from the policy gradient discussion above: we now know not only how to change a trajectory's probability using $$\nabla_\theta P(\tau)$$, but also whether to make it more or less probable: each probability gradient in the sum is weighted by the return $$R$$. That is, we have:

$$\hat{g}=\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} R_t $$

which, again, tells us both how to change the probability of actions leading to the trajectory $$\tau = (s_0, a_0, r_1), (s_1,a_1,r_2), (s_2, a_2, r_3),â€¦$$, *and* now that we have the weight $$R_t$$, we know whether to make the action $$a_t\vert s_t$$ more or less likely to happen. This summed over all time steps gives us the full policy gradient term.

### Congrats! You've made it to the policy gradient!

If you stopped here, this rudimentary policy gradient would fit into our algorithm above, and that would be able to solve simple environments, albeit a little inefficiently. A golden retriever might eventually be trained to drive a car, but man, is it going to be hard going. And you know that he's going to stop for treats and crash into mailmen. 

{% include figure.html path="assets/img/goldenRetriever.png" title="watered down vanilla ice cream" class="img-fluid rounded" %} 
<div class="caption">
He's trying, give him credit.
<d-footnote>From DALL-E</d-footnote>
</div>

No, we can do better than this. Take a breather, then let's take a step back and think. 

### Future return

If I take an action, what parts of the trajectory will this affect? I'm not currently in possession of a time machine, so any action I take will not affect the past.<d-footnote>History may be written by the victor, but recorded events do not necessarily reflect what actually transpired.</d-footnote> And of course if I take an action, that will carry me to the next time step, so I cannot affect the present either. I can only affect the future. 

Therefore, the part of the trajectory $$\tau$$ (and thus, the rewards $$r_t$$) that any action has bearing on is the **remainder of the trajectory**, from the next time step to the end of the episode. Let's call the discounted sum of *future* rewards "*future return*" or $$R^f$$. Our past and present rewards $$r_t, t< t_{current}$$ only serve to add noise to our estimate of how good the action 

{% details A note on terminology %}
This set of rewards is sometimes known as the "rewards-to-go." This term always struck me as kind of confusing, maybe because "to-go" isn't as precise as I'd like. About half the time I see it, I think "'to-go?' Where are the rewards going? Oh, you mean 'to-go' as in what we have to go, or remaining." Let's avoid any temporary confusion and use "future return."
{% enddetails %}

Let's modify the policy gradient to use future return instead:

$$\hat{g}=\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} R_t^f $$

Let's be as clear as possible. Assume T time steps t in a trajectory, t in [0,T), or t in [0,T-1] if you prefer. The future return R^f_t for the first time step (t=0) in a trajectory, R^f_0, contains all the rewards in that trajectory. R^f_1 contains all rewards, except the first reward. R^f_T-1(the final time step) contains no rewards, because there's no future! R^f_T-2 contains only the final reward.

Now, how do we actually calculate R^f_t for a particular time step? It seems obvious: simply take the discounted sum of all the future rewards! Well, it's a bit trickier than that. The obvious obstacle to this is that you never have the future rewards for a time step as you're playing out that step. I don't have a time machine that goes forward, either. You only have immediate access to the rewards you don't care about, those in the past. So you're forced to keep "rolling out" the trajectory to its conclusion, recording all future time steps, and only then you can accurately calculate R^f_t for all time steps.<d-footnote>R^f_t is easy enough to calculate in one backward-in-time pass. Starting from the final time step, each R_t equals the reward of the current time step plus the sum of the rewards from the next time step.</d-footnote>

Whew. That seems a lot more tedious than we were hoping for.<d-footnote>If my episode is infinite, it's technically impossible. I can get around the infinite sum by remembering I'm discounting, and truncate my sum at a reasonable time in the future.</d-footnote> I would like my algorithm to be "online," meaning I would like to avoid having to collect full episodes before knowing what my future return is. If I could do that, I would be able to learn from the reward of every time step in real time, and update my policy accordingly. 

Imagine if you were playing soccer for the first time, but the coach said you weren't allowed to correct your mistakes while you were playing! No no, only in the time between full games can you reflect on the error of your ways, and think how to improve for next time. Until then, suck it up and keep tripping over your feet. Intuitively, this seems inefficient, if not patently ridiculous. 
{% details This is kind of a strawman %}
Now, modern "offline" algorithms aren't actually this bad. They usually collect short sequences of transitions rather than full trajectories, and store them in a buffer to be sampled and asynchronously learned from. So the "reflection between games" is happening at all times, and "reflecting on all past games," so to speak, rather than the most recent experience. In other words, in "online" algorithms the state space currently being learned from is the same that's currently being explored by the active policy. However in "offline" algorithms the current policy (implicit or explicit) sets the explored state space, but the buffer of historical data encapsulates a potentially different state space. There are pros and cons to each.
{% enddetails %}

The smaller the time lag in learning, the faster I can gain information about my new policy, and update it again. So, how to access the future return now instead of later? Approximate it with the value function V! 

## The value function

The value function's (V_t) job is to approximate the return for a particular time step. This is a lot simpler than it may seem. The value function answers one question: how good is this state to be in? Or, given the state at time step t, what is the best approximation of the expected return? With a deep neural network, we can quickly dispatch this question with good old supervised learning. We have a trajectory of states at each time step, and at the end of an episode (or the effective time horizon defined by the discounting term gamma) we're able to calculate the future return for each time step. All that follows is to state the loss function to minimize, and we're off to the races. (Divide this by T)


 


And with that, here is the full policy gradient:
(Math, no sum over trajectories).

A few caveats. 

This is an approximation which may only see a subset of the state space at any given time, and it may never see the full state space. We cannot assume it is unbiased unless it has trained equally as much on data from all parts of the state space, and typically that will only be the case once the training is complete, if it happens at all. In parts of the state space which are relatively less explored, it may even be very inaccurate, which could lead to learning a falsely optimal policy. 

One of the best solutions to this problem is seen in the RL algorithm (PPO)[paper]. >>Its goal is to avoid veering too far out into unexplored territory too quickly. It allows the value function time to "acclimate" to its new surroundings and give more accurate estimates, and so the policy is always learning from a "good enough" value function. It accomplishes this by limiting the size of the policy update step by clipping a surrogate loss function - but I won't digress too much on this point in this post, it requires more time to do it justice.<<

It also turns out that this simple solution is quite high-variance in practice. Because of this, it's difficult for the actor to learn a good policy and to do so quickly. Luckily there are many battle-hardened techniques for reducing variance. 

Take a look at [generalized advantage estimation](gae paper) (GAE) for lower-variance, lower bias estimates of how much to update policy parameters. 

## Generalized Advantage Estimation
What is the lowest bias approximation of the return for a state? Well, the return itself! That is, the actual sum of rewards that we calculate. "But hang on, I thought we were trying to avoid doing that to get quick feedback?" Indeed we are. So perhaps we can combine the two approaches. What if I use the actual reward in the next time step, and add it to the value function's approximation of the future return from that point on? Aha, a slightly more accurate estimate!

Perhaps we can even do a little better by using the next two real rewards instead of only one. Or the next three? Each addition will require a slightly longer delay between taking an action and being able to learn from it. Now the question becomes: what's the optimal tradeoff between accuracy and this learning delay? At this point you just need to experiment and see what works best for your problem, but I will tell you this: the answer is somewhere between 100% accuracy and zero delay (show the chart here). It also turns out you can do even better by doing a weighted average of all of these options: one, two, three, etc. real rewards, followed by an approximation, and also experimenting to see what the right weighting is. That's GAE in a nutshell. The sum will of course be discounted as usual, so all but the first term have \gamma^t applied to them.

We've left out one important piece, though. This isn't actually "Advantage" yet: we haven't addressed how to lower the variance of our approximation. Okay. We have a more accurate approximation with our "immediate reward r_t plus delayed value function V(s_t+1)," as compared to the non-delayed value function V(s_t) alone. What is the source of the variance? Well, the approximation of the value function, of course. Since the value function at each time step is approximated by the same neural network, we should expect it to contribute similar "noise" or "variance" to our "signal" or "true value." If we subtract "pure approximation" V(s_t) from "slightly more accurate approximation" r_t + V(s_t+1), we remove variance and are mostly left with the true value we care about: the advantage A_t, or how much better is this action than average? Putting all this together gives us GAE: a low variance estimator for the advantage.

I've focused on intuition here. If you're interested in the math of how the weighted averaging and so on works out, go take a look at the paper; your mental picture should now be organized to have all those formalisms swiftly fall into place. Finally we can convert our $$V_t$$ to $$A_t$$.





## Batched training of neural nets
Finally, we sum this term over a batch of collected trajectories, dividing by the number of trajectories to get the sampled mean of the above (the grad log time-step summation). Why? Well, to gain a lower variance estimator of the true policy gradient. This should be quite familiar to you if you've ever done mini-batched stochastic gradient descent. Many trajectories averaged together will smooth out the noise from any one trajectory, and better represent the optimal policy gradient, or the step towards the true optimal policy.

Ok, finally, we have the sampled policy gradient: (show full eqn)



With this term, we can update the parameters of our policy, which in this case is an MLP.

$$ \theta_{k+1} = \theta_{k}+\alpha_k \hat{g}_k $$ 




Refers to the updated parameters,  the parameters to be updated, and the learning rate (a hyper parameter).

This is the policy learning taken care of. Now we turn to the value function, doing the same sum over trajectories, for the same reason:









All we have to do is perform inference with the model at all time steps along the trajectory, for all collected trajectories, and we have all the ingredients we need to perform batched gradient descent of the model to minimize the loss function, as shown below.

$$ \theta_{k+1}=\arg \min_\phi \frac{1}{\left\vert  \mathcal{D}_k \right\vert T}\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\left(V_\phi(s_t)-\hat{R}_t  \right)^2 $$

With each gradient descent step, the value function, on average, better approximates the real sum of rewards. The policy then has a more accurate value function with which to adjust its own actions. Together they explore the state and action space until a quality policy is reached. Though every step is not guaranteed to improve due to approximation error, even this "vanilla" reinforcement learning algorithm can learn to complete simple tasks. 

This previously scary-looking algorithm should now be more approachable. Go over it and see that everything lines up for you, then we can take a look at the code.

