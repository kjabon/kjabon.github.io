---
layout: distill
title: The Reinforcement Learning Problem
description: Before anything else, define the problem you need to solve.
giscus_comments: true
date: 2023-04-13
tags: rl 
authors:
  - name: Kenneth Jabon

  

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Outline
  - name: Ingredients
  - name: RL Frameworks
  - name: Data Pipelines
  - name: Other Data Collection
  - name: How to avoid manual habit entry?
  - name: Model and Environment Setup 
  - name: Simulation Training
  - name: Future Work

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }

---
# So, you're interested in RL
This post is the place to start. RL has successfully defeated grandmasters in Go, Dota 2, and is responsible for training ChatGPT. 

<div class="row">
<div class="col">
</div>
<div class="col-12">
{% include figure.html path="assets/img/humanfeedbackjump.gif" title="Noodle" class="img-fluid rounded" %}
</div>
 <div class="col">
</div>
</div>
<div class="caption">
As noted by Amid Fish, it can even teach a noodle to do a backflip.
<d-footnote>Learning from human preferences, OpenAI. [https://openai.com/research/learning-from-human-preferences]</d-footnote>
</div>

# Title A
Some text


## Title B
Some text


### Title C
Some text



In the standard setup of the reinforcement learning problem, you have an actor and an environment interacting in a loop. 

{% include figure.html path="assets/img/RLProblem.png" title="RL Problem" class="img-fluid rounded" %} 
<div class="caption">
Agents act in (or "send actions to") the environment. The environment progresses one time step based on this action, and responds with an observation and reward.
<d-footnote>Figure from Deepmind Acme paper on arXiv. [arXiv:2006.00979]</d-footnote>
</div>




## From the environment’s point of view: 
It is always in a well-defined state $$ s $$, and given some action received by the actor, it will transition to a new state $$ s’ $$ corresponding to that action. When this transition happens, the environment will spit out a reward $$ r $$ for transitioning from $$ s $$ to $$s’$$: $$r = R(s,s’)$$. Eventually some special terminal state is reached. We reached our goal or irrevocably failed the task, and our episode ends. At the beginning of each new episode, the environment can be initialized to an initial state s_0 at the beginning of an episode. This environment is a Markov decision process (MDP): go read about those on page 47 of [Sutton and Barto](http://incompleteideas.net/book/RLbook2020.pdf). 



## From the actor’s point of view: 
It will take an observation of the environment’s state, process this information, and output an action to the environment. It will then record the reward r output by the environment, and continue this loop. This reward is the signifier of “good” or “bad” results from the reinforcement learning actor’s actions. 

Following this loop forms a trajectory tau made up of the states, actions, and rewards at each time step. (s0, a0, r1), (s1,a1,r2), (s2, a2, r3),…

{% details “That looks funny,” I hear you say. %}
 The reward time step is offset by one for a few reasons. We don’t get any reward for the first time step, i.e. for initializing the state. Why? We haven’t taken any action yet. Also, the reward is associated with both state s and s’, which live in separate time steps. We have to pick one formalism arbitrarily, so we assign the reward to the latter time step, because the environment returns it at the same time as the next state s’. However, we group it with the former tuple in our trajectory, because it’s associated with the action we took at that time step. This tuple organization follows an intuitive loop from the actor’s point of view: “observe, act, get a reward,” then repeat.
 
 {% details “But wait,” I hear you say. %}
 It would make more sense to have a “null” reward r0 at the beginning which we spit out but don’t do anything with, and form our trajectory like so: 
(r0, s0, a0) (r1, s1,a1),(r2,s2, a2), …. 
The subscripts look nicer, but alas, it’s not the standard formalism. Also, this doesn’t really end the tuples at natural stopping points. Feel free to consider further, but try not to get hung up on this point. Ultimately, we’re representing the same thing, and sometime very soon you will abstract away the whole process. The following example should clear things up. 
{% enddetails %}
{% enddetails %}


# 1D Grid World, a Simple Example

Let’s consider a 1D grid world, where the goal is simply for the actor to be as near as it can to a particular grid space, and t is the target grid space.

t = 4	  [    |    |    |    | t |    |    ]
State     [ 0 | 1 | 2 | 3 | 4 | 5 | 6 ]

Since our goal is to be near to the target space, let’s define a reward function: R(s,s’) = |t-s’| + 4. Remember s’ is the state we end up in after taking an action. We see each grid space take on a reward following this function:
>(I add the 4 to keep the numbers positive, i.e. a little cleaner, but offsetting the reward function makes no difference to the RL algorithm. I could add or subtract 10,000, and in principle it will still work, especially if you are standardizing the inputs to your neural network.)

t = 4	  [    |    |    |    | t |    |    ]
State     [ 0 | 1 | 2 | 3 | 4 | 5 | 6 ]
Reward [ 0 | 1 | 2 | 3 | 4 | 3 | 2 ]

Now, our actor may start at a random location, but let’s suppose it starts at 0: [ x |  |  |  | t |  |  ]
X is the location of our actor. Notice here the origin state doesn’t actually change the reward, because the initialization of the environment state doesn’t spit out a reward. > In RL, we don’t get a reward just for showing up, we get rewards for participation!

Suppose we have 3 actions available to us at a given time step. We can move left, right, or stay put. Encode these actions as -1, 1, and 0 respectively. Suppose a deterministic state transition, i.e. a left action will always move us left one grid space, and so on. If we bop into a wall, then we stay put; this is still deterministic. 

When we transition to the new state based on our action, we obtain the reward associated with that transition from old to new state: r = R(s, s’). The goal in the RL problem is defined as maximizing the “return,” or the sum of rewards r in a trajectory tau. (Math) The actor maintains a policy pi(a|s). This function outputs the probability distribution of all possible actions at time t in the trajectory, given the state at time t. We can see the optimal policy pi(a|s) which achieves this goal immediately:

t = 4	  [    |    |    |    | t |    |    ]
State.    [ 0 | 1 | 2 | 3 | 4 | 5 | 6 ]
Reward [ 0 | 1 | 2 | 3 | 4 | 3 | 2 ]
Policy    [ 1 | 1 | 1 | 1| 0 |-1 |-1 ] (1, 0, -1 = right, stay, left)

That is, step towards the target, and if you’re on the target, stay put. At the risk of being obvious, let’s show the optimal trajectory following this policy for our actor starting at position 0. Remember a trajectory \tau follows the form (S0, a0, r1), (s1,a1,r2),(s2, a2, r3),….

[ x|  |  |  | t |  |  ] s0=0, a0=1 (initial state; no reward)
[   |x|  |  | t |  |  ] r1 = 1, s1=1, a1=1
[   |  |x|  | t |  |  ] r2 = 2, s2=2, a2=1
[   |  |  |x| t |  |  ] r3 = 3, s3=3, a3=1,
[   |  |  |  | x|  |  ] r4 = 4, s4=4

At this point the actor receives the final reward and state, and notices it has reached the goal/terminal state. No further actions are taken and the episode ends. Our return, or sum of rewards, is r1+r2+r3+r4 = 10. >At this point it is worth mentioning: a trajectory is any contiguous subsequence of an episode, while an episode is the full sequence from initial state to terminal state, if there is one.

We also could have put “pitfalls” at each end, such that the actor would receive a large negative reward, and the episode would end then, as well. Clearly an optimal policy would involve avoiding these “bad” spaces.

We’ll leave representing and learning the policy, which can be handled by all manner of RL algorithms, to future posts(policy gradient) and external resources. See [here](https://spinningup.openai.com/en/latest/user/algorithms.html) for starters.

Let’s make this picture more general so it can describe any environment interaction. 

# Imperfect information
Earlier I said the actor “takes an observation” rather than “records the state.” This is because in general, the observation recorded by the actor may be an imperfect representation of the well-defined environment state. 

Suppose our actor is Paul Revere, and he is deciding whether to hang one or two lanterns at the Old North Church (action: 0, 1, or 2, encoding a signal to the militia: “don’t know”, “by land” and “by sea” respectively). There is an advancing British force coming in ships off the coast (state: 15,000 troops coming by sea). 

However, Mr. Revere can only see a boat or two off the coast, and similarly a few carriages shuttling around on land. The British force is concealed by the fog and dark of night. His observation is an imperfect representation of the environment state (observation: ~ 0, or maybe -10 (a few more people on land) or 10 (a few more at sea)).



# Infinitely long episodes
Next, what if our loop has no foreseeable end? In general this will be the case. Some environments go on forever, and there is nothing in our MDP picture which prevents that from happening. Suppose our actor is a bipedal robot. Its task is to push an AWS server to the top of Mount Everest, because there are excellent ambient temperatures for computing up there. Unfortunately for the robot, every time it gets near the top, its servos freeze over, it loses control of the server rack, and it rolls all the way back to the bottom. And so it will try again until the end of time, or at least the end of its Amazonian overlords. All is well for the robot, who has no shortage of energy or enthusiasm.

How do we support infinite episodes? We simply mandate that every state must accept a set of valid actions, and that such actions result in a state transition. There is nowhere to “end,” and the MDP goes on forever. 

Does this picture still support finite episodes? Notice that a state s can transition to itself (s’ = s). To mark a state as terminal, we only allow it to transition to itself. Technically this is still an infinite MDP: our picture hasn’t changed, it will transition to itself forever. But if we reach a particular state or set of states, we can decide to stop traversing and end the infinite episode prematurely.

# Discounted rewards 
Now, let me ask you one last question. You’ve won a million dollars. Congrats. Would you like your prize now, or in 30 years? Everyone can agree on the answer. If you have it now, you can improve your life, or others lives, now. If you’re worried about self control, stick it in a trust and only touch the dividends. What use is there in waiting?

In a more general sense, our actor’s goal is to maximize the discounted sum of rewards. In other words, reward now is better than reward later, else you’re just wasting time. What’s more, remember our trajectory is infinitely long in general. Ultimately need to do calculations with the sum to learn from it, and we can’t do that with infinite numbers. We add the discounting term so that our return converges to some finite value. Let’s formally describe this value: R(\tau)=\sum_{t=0}^{T}\gamma^tr_t. We can see that as the time step gets further into the future, the discounting factor \gamma will make reward term decay to 0, and assuming no one reward is infinite, the sum will never be infinity.

# Probabilistic state transitions
What if a particular action from a particular observation doesn’t always result in the same state transition? Our robot from earlier is halfway up the mountain and tries to push forward one more step, but randomly a gust of wind causes him to lose his grip on the AWS server, rolling back down. This is supported by probabilistic state transitions in the MDP. A definite action is taken, then state s will transition to s’, but s’=0 (the bottom of the mountain) with 5% probability, and s’=s+1 the rest of the time. In a simulated environment this can be represented by a transition function p(s’| s, a), with a vector of probabilities for all reachable s’ from s, for a particular action a.

That’s our MDP picture and the RL problem; not so bad, is it? Of course, we haven’t done any learning yet! See the next post on VPG for how to learn from interacting with the environment.
***
I put this together without referencing other material except where stated. That said, my understanding of the subject comes from David Silver’s excellent lectures, Open AI’s spinning up, the 2017 Berkeley Deep RL Bootcamp, Pieter Abbeel’s and Sergey Levine’s own lectures, and Sutton and Barto’s “Reinforcement Learning” (which was referenced by the others). These are excellent resources, and I recommend you check them out.

{% include figure.html path="assets/img/Markov_Decision_Process.svg" title="MDP" class="img-fluid rounded" %} 
<div class="caption">
An example of an MDP. This encapsulates states (green nodes), actions (red nodes), rewards upon state transition (emitted squiggles), and nondeterministic transitions (arrows from red nodes).
<d-footnote>From the wikipedia page for MDPs.</d-footnote>
</div>


