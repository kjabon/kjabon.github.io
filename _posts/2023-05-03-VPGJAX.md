---
layout: distill
title: Vanilla Policy Gradient In JAX
description: A simple implementation using Acme
giscus_comments: true
date: 2023-05-03
tags: rl 
authors:
  - name: Kenneth Jabon

  

# Optionally, you can add a table of contents to your post.
# NOTES:
#   - make sure that TOC names match the actual section names
#     for hyperlinks within the post to work correctly.
#   - we may want to automate TOC generation in the future using
#     jekyll-toc plugin (https://github.com/toshimaru/jekyll-toc).
toc:
  - name: Future return

# Below is an example of injecting additional post-specific styles.
# If you use this post as a template, delete this _styles block.
_styles: >
  .fake-img {
    background: #bbb;
    border: 1px solid rgba(0, 0, 0, 0.1);
    box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
    margin-bottom: 12px;
  }
  .fake-img p {
    font-family: monospace;
    color: white;
    text-align: left;
    margin: 12px 0;
    text-align: center;
    font-size: 16px;
  }
---


#### The code
For this blog post, I’m going to implement vanilla policy gradient in JAX. We’ll not only get into a VPG implementation, but perhaps more interestingly, discuss the how and why of using JAX. Note this post focuses on implementation; the theory of VPG is broken down in [this post](/blog/2023/VPG). Feel free to take a look at that first if you find yourself asking “why?”

To begin with, let’s take a look at the pseudocode for the VPG algorithm, courtesy of OpenAI’s [Spinning Up](https://spinningup.openai.com/en/latest/algorithms/vpg.html#pseudocode). Just do a once-over, we’re going to implement it piece by piece.

{% include figure.html path="assets/img/pseudoVPG.svg" title="Pseudocode" class="img-fluid rounded" zoomable=true %} 

Ok, so maybe that’s a lot to take in all at once. Let’s just take a look at the first line.

THE FIRST LINE
We need a policy and a value function. Both will be represented by neural networks (of course). I have found that for most starter Gym [environments](https://gymnasium.farama.org/), two layers with 128 weights each do the job just fine (and often even less than this is needed). 

The below code is adapted from the Acme [implementation](https://github.com/deepmind/acme) of PPO. Kudos to DeepMind for providing excellent baseline implementations of many RL algorithms.

Before we get started, let’s define some convenience classes which will allow us to organize our neural network and all the functions related to it. This will also serve to give us a roadmap for building and initializing our neural networks.

<d-code block language="python">
class VPGNetworks:
    network:   networks_lib.FeedForwardNetwork
    sample:    networks_lib.SampleFn    # = Callable[[NetworkOutput, PRNGKey], Action]
    log_prob:  networks_lib.LogProbFn   # = Callable[[NetworkOutput, Action], LogProb]
</d-code>


This is pretty straightforward: for RL we need a network, a way to sample from the outputs of that network, and a way to compute the log probabilities of those samples (i.e., the actions). If this is not straightforward, see the previous [post](/blog/2023/VPG)

``sample()`` will take in the network output (including info encoding the probability distribution) and a random number key, and return a sampled action. 

``log_prob()`` will take in the network output (again, including the probability distribution) and an action, and return the log probability of that action.

In general we’ll have a vector of actions to contend with, and operate on batches of them.

Let’s take a look at the ``FeedForwardNetwork`` class.
<d-code block language="python">
class FeedForwardNetwork:
  # A pure function: ``params = init(rng, *a, **k)`` 
  # Initializes and returns the networks parameters.
  init

  # A pure function: ``out = apply(params, rng, *a, **k)`` 
  # Computes and returns the outputs of a forward pass.
  apply
  
</d-code>

So, we need a way to initialize and apply our neural network, and for JAX, we need to ensure these are pure functions. Luckily, the haiku library takes care of most of the heavy lifting here, as we’ll see. For clarity, I’ve swept some type checking under the rug.

Let’s get cracking on the network!


The following MLP and Linear functions allow starting with various well-known initializations, falling under [variance scaling](https://jax.readthedocs.io/en/latest/_autosummary/jax.nn.initializers.variance_scaling.html#jax.nn.initializers.variance_scaling). For this post we’ll just use the defaults without specifying those arguments.

Starting with the value function:
<d-code block language="python">
import haiku as hk
import jax.numpy as jnp 
# jax.numpy has the same interface as numpy, but uses XLA backend for hardware acceleration!

layer_sizes = (128,128)
V = hk.nets.MLP(layer_sizes, activate_final=True)
V = hk.Linear(1)(V)
</d-code>


Now, we almost have an MLP which can serve as our value function, with a single output: the value given the observation! However we’re missing two things: proper input and output dimensions.

We need to tell the model what input dimensions (from the batch of observations) we expect. We’ll leave this as a dummy ``obs`` variable for now - we’ll come back to it in a minute. We would like to flatten the input, except for the batch dimension (assuming only one batch dimension). That is, for batch size n, we want n observation vectors. We define the following function to do so:

<d-code block language="python">
def batch_flatten(obs):
	if obs.ndim > 0: 
		return jnp.reshape(obs, [obs.shape[0], -1])
	return input

</d-code>

…and prepend a call to this function to our code like so:

<d-code block language="python">
V = batch_flatten(obs)
V = hk.nets.MLP(layer_sizes, activate_final=True)(V)
V = hk.Linear(1)(V)
</d-code>

Finally, we’d like to make sure the output of our value function is a batch-sized vector of values if it isn’t already, so we squeeze the output results. Putting all of this into a haiku Sequential model yields:

<d-code block language="python">
V = hk.Sequential([
	batch_flatten,
	hk.nets.MLP(value_layer_sizes, activate_final=True),
	hk.Linear(1),
	lambda x: jnp.squeeze(x, axis=-1)
])
</d-code>

This completes our value network!


Now, our policy model is slightly different. First, because we are assuming a continuous action space, we need to output a distribution over actions, from which the actual action for some observation will be sampled. The policy model will output mean and variance to describe this multivariate normal distribution. Two fully connected layers are branched from the torso, to be able to learn mean and variance from the same embedding of the observation. For this reason, we don't use ``hk.Sequential`` [Stick in a drawing].


Here is the torso: 

<d-code block language="python">
h = utils.batch_concat(obs)
h = hk.nets.MLP(policy_layer_sizes, activate_final=True)(h)
</d-code>

Now we add a branch each for the mean and variance.

<d-code block language="python">
min_scale = 1e-3
num_dimensions = np.prod(environment_spec.actions.shape, dtype=int)
mean_layer = hk.Linear(num_dimensions)
var_layer = hk.Linear(num_dimensions)

mean_out = mean_layer(h)
var_out = var_layer(h)
</d-code>

Constrain to a positive scale, as variance must be positive. 

<d-code block language="python">
var_out = jax.nn.softplus(var_out) + 1e-3 #some epsilon; avoid div-by-zero
</d-code>

{% details Why softplus? %}
ReLU will compute faster, and may be a choice worth considering if compute power/training speed is a concern. However, if the user has problems with ReLU “dying” in the zero region (when $$x < 0$$, $$y = 0$$), we cannot use leaky ReLU, the usual first solution to this problem, because the output must be positive. Softplus is a better option in this case. **(ReLU vs soft plus image)**
{% enddetails %}

We’re not quite done just yet, because of the vagaries of JAX. We need to wrap and transform the models we’ve just written.

First, we encapsulate our policy code into a function:
<d-code block language="python">
def _policy_network(obs):
	h = utils.batch_concat(obs)
	#…previous code defining policy model…
	return (mean_out, var_out)
</d-code>

Then, we wrap both the policy and value functions into one forward function, outputting everything we infer from the observation from this function.
<d-code block language="python">
def forward_fn(inputs: networks_lib.Observation):
	inputs = jnp.array(inputs, dtype=jnp.float32) #ensure we are working with JAX NumPy

	#….previous code defining policy and value functions….

	policy_output = _policy_network(inputs)
	value = value_network(inputs)
	return (policy_output, value)
</d-code>

Finally, we use the haiku ``transform()`` function. 

{% details An aside on ``transform()``, JAX, and pure functions %}
If you don’t care about why we use transform, so much as that it makes deep learning fast, feel free to skip this section.

From the haiku [documentation](https://dm-haiku.readthedocs.io/en/latest/notebooks/basics.html):
>…Haiku modules are Python objects that hold references to their own parameters, other modules, and methods that apply functions on user inputs. On the other hand, since JAX operates on pure function transformations, Haiku modules cannot be instantiated verbatim. Rather, the modules need to be wrapped into pure function transformations.
>Haiku provides a simple function transformation, hk.transform, that turns functions that use these object-oriented, functionally “impure” modules into pure functions that can be used with JAX.

See [here](https://en.wikipedia.org/wiki/Pure_function) for pure functions, and [here](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html#pure-functions) for why pure functions are required. 

In short, JAX uses cached compilations of functions to speed everything up. 

As you, a computer scientist, probably know, an optimizing compiler (here, XLA) converts your code into a form that is both lower level (closer to machine instructions), and able to make certain assumptions to run your computations in fewer steps. Oh yeah, compilation is great!

But of course, anyone who’s written anything moderately sized in C/C++ knows that repeated compilation will send your productivity straight in the trash, right next to the greasy pizza boxes and used Kleenex, never to be seen again. Can we avoid unnecessary compilation?

By mandating pure functions, JAX knows the function’s behavior will never change, and so not only will it never need to be recompiled after the first time it’s used, it will be able to make more assumptions to improve its computational efficiency all the more so!
{% enddetails %}

So, without further ado, let’s transform our model!

<d-code block language="python">
forward = hk.without_apply_rng(hk.transform(forward_fn))
</d-code>

Now that we’ve transformed (“purified/JAX-ified”) our model, we’ve exposed a pure interface to our model we can use without worry: that is, ``forward.init()``, and ``forward.apply()``. **say a word on ``without_apply_rng``**

Let’s initialize our model by creating a dummy observation with the same dimensions as the real inputs to the model, and passing this to ``init()``. The model will then be configured to handle the correct input and batch dimensions.

<d-code block language="python">
dummy_obs = utils.zeros_like(environment_spec.observations)
dummy_obs = utils.add_batch_dim(dummy_obs)  # Dummy 'sequence' dim.
network = networks_lib.FeedForwardNetwork(
	lambda rng: forward.init(rng, dummy_obs), forward_fn.apply)
</d-code>

``FeedForwardNetwork`` simply wraps ``haiku.transform``’s outputs (``init`` and ``apply``) into a standard JAX container. 


{% details Why haiku? %}
By the way, you may be wondering “why haiku instead of flax?” Well, the immediate reason is that personally, I’m more used to it. They’re very similar, application-wise, so you should just stick with what works for you. 

Highly subjective opinion warning: the feeling I get is that the collection of the Deepmind repositories are less the result of a "wild-west, every small team publishing for themself” environment. Rather, the collection of repos has a sense of cohesion and coordination. I could be wrong about this. Anyway, Deepmind has done a good job [open sourcing](https://github.com/deepmind?q=reinforcement&type=all&language=&sort=) a ton of their software stack related to reinforcement learning and deep learning, not limited to haiku. Check it out!
{% enddetails %}

{% details Why rng key? %}

JAX shuttles around these explicit (pseudo-)random number generator parameters in the functions that require them. In this way, you can reproduce experiments without worrying that a different seed is causing you uncertainty in your results behind the scenes. All in the name of good science! 

Why is this explicitness necessary? Normally, Python will, from an initial seed, maintain a global key which functions will then access, generating random numbers. This is fine and dandy if there is a single thread of execution. For example, consider the following code.<d-footnote> This is effectively the same example as the JAX documentation (https://jax.readthedocs.io/en/latest/jax-101/05-random-numbers.html#random-numbers-in-jax), read that for further consideration of the implications and solutions.</d-footnote>

<d-code block language="python">
import random

random.seed(42)

def funA(): return random.random( )
def funB(): return random.random( )

def randomSum( ): return funA( ) + 2 * funB( )

print(randomSum( ))
</d-code>

Python will always run ``randomSum()`` the same way, according to a well-defined [operator precedence](https://docs.python.org/3/reference/expressions.html#operator-precedence). ``funA()`` is called (first random number from seed 42), then ``2*funB()`` is calculated, at which time ``funB()`` is called (second random number from seed 42), then they are summed. If ``funB`` ran before ``funA`` on occasion, the final output would change. This never happens. The output of ``randomSum()`` will always be the same, i.e. reproducible! 

{% include figure.html path="assets/img/obama.gif" class="img-fluid rounded" %}  

Feel free to add some print statements and run the code to satisfy any doubts.

If we are parallelizing our code (the whole point of using JAX!), we *don’t* know what order functions will be called in, due to how your OS handles [process scheduling](https://en.wikipedia.org/wiki/Scheduling_(computing)#Process_scheduler). <d-footnote>These adverse effects are the same for threads and processes assuming every concurrent unit has access to the global key, so we use them interchangeably here.</d-footnote> If we delegated the calling of ``funA()`` and ``funB()`` above to different processes, sometimes funB( ) might run first. As a result, using a global random key causes us to sacrifice our beloved reproducibility. Thus, we must explicitly give each process its own specific key, which it can then use in its own good time, regardless of execution order, in a reproducible, well-defined fashion.
A process executes precisely when it means to.

{% enddetails %}

So, we have our network defined and transformed. Taking a look at our container class:

<d-code block language="python">
class VPGNetworks:
    network:   networks_lib.FeedForwardNetwork
    sample:    networks_lib.SampleFn    # = Callable[[NetworkOutput, PRNGKey], Action]
    log_prob:  networks_lib.LogProbFn   # = Callable[[NetworkOutput, Action], LogProb]
</d-code>

All that remains is a little book-keeping to expose ``sample()`` and ``log_prob()`` functions.



<d-code block language="python">
def log_prob(params, action):
	return tfd.MultivariateNormalDiag(
	loc=params.loc, scale_diag=params.scale_diag).log_prob(action)


def sample(params, key):
	return tfd.MultivariateNormalDiag(
	loc=params.loc, scale_diag=params.scale_diag).sample(seed=key)

vpgNetwork = VPGNetworks(
	network=network,
	log_prob=log_prob
	sample=sample
)
</d-code>

Whew! We made it past the first line! Surprisingly, most of the hard code is behind us, and we get to jump into the algorithm proper. [This is where the fun begins].

THE SECOND LINE

Ok, not too bad. 




