<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Imitation Learning with PWIL | Kenneth  Jabon</title>
    <meta name="author" content="Kenneth  Jabon">
    <meta name="description" content="An exercise on the Acrobot Swingup task">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://kjabon.github.io/blog/2022/ImitationLearningPWILAcrobot/">
    
    <!-- Dark Mode -->
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <!-- Page/Post style -->
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
} iframe {
  display: block;
  border-style:none;
}

    </style>
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "Imitation Learning with PWIL",
      "description": "An exercise on the Acrobot Swingup task",
      "published": "July 18, 2022",
      "authors": [
        {
          "author": "Kenneth Jabon",
          "authorURL": ""
          
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Kenneth </span>Jabon</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repos</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>Imitation Learning with PWIL</h1>
        <p>An exercise on the Acrobot Swingup task</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#future-return">Future return</a></div>
            
          </nav>
        </d-contents>

        <p>If you’re unfamiliar with Acrobot, it’s a classic control problem in which the “elbow” joint can be actuated, the “shoulder” joint swings freely, and the goal is (typically) to swing to a vertical, upwards position, starting from a vertical downwards position. Sometimes the goal is to also maintain this upright position, which we’ll leave for another time.</p>

<p>This may come as a shock, but there exist pretty good solutions to this problem that don’t involve deep reinforcement learning. (Brief ILQR explanation.)</p>

<p>The basic question of this post is: can we learn a classical ILQR controller’s policy for solving acrobot-swingup with imitation learning?</p>

<div class="row">
<div class="col">
</div>
<div class="col-8">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/ilqr_swingup.gif-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/ilqr_swingup.gif-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/ilqr_swingup.gif-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/ilqr_swingup.gif" class="img-fluid rounded" width="auto" height="auto" title="ilqr" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">Acrobot swingup with classical ILQR controller.</figcaption>

</figure>
 
</div>
 <div class="col">
</div>
</div>

<p>The deeper question of this post is: what can we do with imitation learning that we <em>can’t</em> do with plain RL? Well, there are two main situations in which it can help us out. First, if we’re unable to adequately define a reward function for the task we have in mind, or if the reward function is known, but too sparse to learn efficiently (in which case we have failed to adequately define a reward function, so this is really a special case of the first).</p>

<p>One symptom of this may be that despite being able to learn a policy which technically achieves the goal, like swingup in acrobot or running for humanoid robots, these learned policies may be “idiosyncratic.” Perhaps they are too jerky, or do things that look obviously goofy or energy-inefficient. Defining rewards to tamp down on undesired behaviors can become a tedious and never-ending game of whack-a-mole.</p>

<hr>

<p>Youtube video:</p>
<center><iframe width="560" height="315" src="https://www.youtube.com/embed/hx_bgoTF7bs?start=88" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen=""></iframe></center>

<hr>

<p>In this video, we see RL-trained agents successfully navigating environments, with somewhat goofy behavior. The “arm pumping” is serving some kind of angular-momentum counterbalancing purpose, but we know intuitively that this behavior is not ideal. Yet, it’s not clear how one would go about defining a reward function to tamp down on this.</p>

<p>Another example: suppose I was training a robotic hand to grasp and sort objects of various size, material, and shape, certainly I would be able to specify distance-based measures of reward: how far is the object from the target? But the reward for a “stable grasp” of an object is not so easily mathematically defined.</p>

<p>Certainly we could get into the realm of feature engineering… perhaps I could define a stable grasp to be three points of contact at which I am applying a normal force, the sum of these forces being as close to zero as possible, all while relying on friction and torque as little as possible. Add in a feature for whether the center of gravity is below all of my contact points, and we’re looking pretty good. But this sort of feature engineering tends to be object class-specific. The features for grasping a glass of water will look very different from those for grasping a raspberry, a sheet of paper, a block of agar, or a chicken egg. This could also end up needing quite a few sensors than we have available rather than the usual image and/or robot state observation.</p>

<p>So, instead, we could have people directly controlling the robot to perform the desired tasks, record the trajectories including control actions by the people, and do imitation learning on that. For the humanoid example, perhaps we could have people navigate obstacle courses while wearing motion capture gear.</p>

<p>There are of course other methods which we could use on this problem, such as RLHF, and more advanced methods which build upon what’s discussed in this post (see google soccer paper), but let’s stick to imitation learning for this post.</p>

<p>Explain ILQR controller, and RL training.<br>
Replace all plots with matplotlib/plotly</p>

<p>We don’t use the standard gym/mujoco environments for this post, because they don’t lend themselves as straightforwardly to using classical controllers via access to their dynamics and kinematics (it is possible, I just haven’t implemented it).</p>

<p>Basically, we want to perform the following tasks.</p>

<p>Do RL with D4PG on the task, no imitation. This is our baseline.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/acrobot_d4pg_10M-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/acrobot_d4pg_10M-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/acrobot_d4pg_10M-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/acrobot_d4pg_10M.png" class="img-fluid rounded" width="auto" height="auto" title="D4PG" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">Episode return vs. time steps; acrobot run with D4PG.</figcaption>

</figure>

<div class="row">
<div class="col">
</div>
<div class="col-8">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/d4pg_swingup.gif-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/d4pg_swingup.gif-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/d4pg_swingup.gif-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/d4pg_swingup.gif" class="img-fluid rounded" width="auto" height="auto" title="D4PG" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">Acrobot run after 10M training steps with D4PG. </figcaption>

</figure>
 
</div>
 <div class="col">
</div>
</div>

<p>During training, upon reaching the top, the episode would end and the environment would reset. In the above figure, after turning this reset off, the policy is still able to swing back up to position, and surprisingly maintains its position.</p>

<p>Collect trajectories from an ilqr controller for swingup, stopping and resetting above the threshold line.<br>
These should be noisy.<br>
Now, we also train PWIL on ILQR data for 1M time steps. Let’s take a look at the results…</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/acrobot_pwil_1M-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/acrobot_pwil_1M-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/acrobot_pwil_1M-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/acrobot_pwil_1M.png" class="img-fluid rounded" width="auto" height="auto" title="pwil" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">Episode return vs time steps; acrobot run for 1M timesteps with PWIL.</figcaption>

</figure>

<p>We can see that PWIL works! By learning to imitate the classical controller’s actions given the state, this learned controller shows signs of life in its training curve. Because it isn’t training to maximize rewards directly (the loss function doesn’t even contain the environment reward), as expected, the final average return isn’t quite so high as something that’s optimizing for it.</p>

<p>Let’s see what happens when we visualize a rollout of the PWIL-learned policy.</p>

<div class="row">
<div class="col">
</div>
<div class="col-8">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pwil_swingup.gif-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pwil_swingup.gif-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pwil_swingup.gif-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pwil_swingup.gif" class="img-fluid rounded" width="auto" height="auto" title="pwilSwing" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">Rollout of PWIL policy, learned from expert ILQR demonstrations.</figcaption>

</figure>
 
</div>
 <div class="col">
</div>
</div>

<p>Not exactly the same as the PWIL controller, but pretty close! Keep in mind that this was trained with two sources of noise: the ILQR controller itself had its actions and observations perturbed, and PWIL additionally perturbs actions.</p>

<p>Now at this point it’s worth discussing the reward and termination scheme. The environment gives a reward of +10 if the end of the pendulum is within some radius of the target (i.e. where the pendulum is completely vertical and pointing up; the angle of the first joint is \(\pi\) and the angle of the second joint is 0). The radius threshold is somewhat large: 0.2 (compare to the length of the pendulum: 0.5). In all cases it will receive a penalty proportional to its distance from the target, and penalties proportional to the absolute position and velocity, to avoid wild spinning.</p>

<p>Finally, the environment resets either after 10 seconds (5000 time steps each 0.002s long), or if the end of the pendulum is both within the aforementioned radius from the target, and its y position is within 0.05 of the target. This y condition is most of the story, but the radius necessitates it to be nearer to the center.</p>

<p>Do you see the potential issue? In the D4PG gif above, we see some interesting behavior suggesting an incorrect environment setup.</p>

<p>The first loop goes to the target as hoped for; the second loop maintains position away from the target, below the y position. That’s strange… if we look more closely at the first loop, and notice that the position of the middle joint does <strong>not</strong> go to the top, we can see that the reset condition wasn’t actually reached! And indeed, looking at the episode length chart from training the D4PG agent, it is more or less always at the full 5000 time steps. Clearly the agent has learned to <strong>not</strong> terminate the episode, but get as many +10’s as it can while staying inside the target radius threshold, but below the y position threshold.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/d4pg_episode_length-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/d4pg_episode_length-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/d4pg_episode_length-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/d4pg_episode_length.png" class="img-fluid rounded" width="auto" height="auto" title="d4pgLen" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">The D4PG agent learns to not terminate episodes; the episode length is 5000.</figcaption>

</figure>

<p>This is a perfect example of why visualization is so important.</p>

<p>So, what to do? Well, let’s first fix the reward, by adding the y condition to be able to get a +10. Let’s re-train both D4PG and PWIL with the fix. This should tamp down on the strange “hold near target” behavior, and the max return should be near 0. PWIL doesn’t use this reward to train, so we’re merely reflecting how the training on the PWIL reward affects progression towards the environment reward. We expect the D4PG curve to end with a higher return, since it’s optimizing for the reward.</p>

<p>[Show D4PG and PWIL training curves].</p>

<p>However, because of the termination condition, it is possible that both policies without resets will get up to the top and then start exhibiting strange behavior.</p>

<p>[Show gifs]</p>

<p>So what exactly is the benefit of imitation learning? For the most part, D4PG seemed to win out here.</p>

<p>[Robot hand pic]</p>

<p>If human demonstrations are provided, we could then learn from that experience with much less sparse rewards than the end result. That intermediary policy could then be fine-tuned with RL on the sparse reward. If the robot hand has an approximately good policy, we can then improve upon it without so much random exploration.</p>

<p>Can we fine tune on this result with RL? Let’s give it a shot.</p>

<p>[D4PG fine tuned from PWIL]</p>

<p>One last topic of discussion. Is the policy enacted by the ILQR controller even learnable by the neural network? In theory it should be, but notice how the ILQR controller swings back and forth several times before building up the momentum to make a final attack. Many of the states in this initial swing-up sequence are pretty similar, and maybe the corresponding actions are not-so-similar. I’m not going to try this here, but if this is the case, we could instead try using the history as the input to our policy instead of its current state to try to resolve this issue. This could either involve frame-stacking, an RNN/LSTM policy and value function, or perhaps even an attention mechanism.</p>

<p>An easy thing to check is whether the D4PG policy, already represented by the same neural network, is learnable by PWIL. <br>
Do some roll-outs (they’ll probably all be pretty similar; check this first). Learn them with PWIL.</p>

<p>[Show the curve.]</p>

<p>We expect this to perform quite a bit better.</p>

<p>Discussion/ conclusions, possible applications. Negative IP, takeaways.</p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "kjabon/kjabon.github.io",
        "data-repo-id": "R_kgDOIuhwgg",
        "data-category": "Comments",
        "data-category-id": "DIC_kwDOIuhwgs4CVKPS",
        "data-mapping": "url",
        "data-strict": "0",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "top",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        © Copyright 2023 Kenneth  Jabon. Powered by al-folio.<br>Last updated: July 26, 2023.
      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', '');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
