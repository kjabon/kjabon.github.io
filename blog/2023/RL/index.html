<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>The Reinforcement Learning Problem | Kenneth  Jabon</title>
    <meta name="author" content="Kenneth  Jabon">
    <meta name="description" content="Before anything else, define the problem you need to solve.">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://kjabon.github.io/blog/2023/RL/">
    
    <!-- Dark Mode -->
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <!-- Page/Post style -->
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "The Reinforcement Learning Problem",
      "description": "Before anything else, define the problem you need to solve.",
      "published": "April 13, 2023",
      "authors": [
        {
          "author": "Kenneth Jabon",
          "authorURL": ""
          
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Kenneth </span>Jabon</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repos</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>The Reinforcement Learning Problem</h1>
        <p>Before anything else, define the problem you need to solve.</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#outline">Outline</a></div>
            <div><a href="#ingredients">Ingredients</a></div>
            <div><a href="#rl-frameworks">RL Frameworks</a></div>
            <div><a href="#data-pipelines">Data Pipelines</a></div>
            <div><a href="#other-data-collection">Other Data Collection</a></div>
            <div><a href="#how-to-avoid-manual-habit-entry">How to avoid manual habit entry?</a></div>
            <div><a href="#model-and-environment-setup">Model and Environment Setup</a></div>
            <div><a href="#simulation-training">Simulation Training</a></div>
            <div><a href="#future-work">Future Work</a></div>
            
          </nav>
        </d-contents>

        <h1 id="so-youre-interested-in-rl">So, you’re interested in RL</h1>
<p>This post is the place to start. RL has successfully defeated grandmasters in Go, Dota 2, and is responsible for training ChatGPT. Here we will lay out the reinforcement learning problem, and in future posts I’ll lay out how various algorithms go about solving it.</p>

<div class="row">
<div class="col">
</div>
<div class="col-8">
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/humanfeedbackjump.gif-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/humanfeedbackjump.gif-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/humanfeedbackjump.gif-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/humanfeedbackjump.gif" class="img-fluid rounded" width="auto" height="auto" title="Noodle" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

</div>
 <div class="col">
</div>
</div>

<div class="caption">
As duly noted by Amid Fish, it can even teach a noodle to do a backflip.
<d-footnote>Learning from human preferences, OpenAI. [https://openai.com/research/learning-from-human-preferences]</d-footnote>
</div>

<p>Well, let’s jump right in. In the standard setup of the reinforcement learning problem, you have an actor and an environment interacting in a loop.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/RLProblem-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/RLProblem-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/RLProblem-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/RLProblem.png" class="img-fluid rounded" width="auto" height="auto" title="RL Problem" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

<div class="caption">
Agents act in (or "send actions to") the environment. The environment progresses one time step based on this action, and responds with an observation and reward.
<d-footnote>Figure from Deepmind Acme paper on arXiv. [arXiv:2006.00979]</d-footnote>
</div>

<h3 id="from-the-environments-point-of-view">From the environment’s point of view…</h3>
<p>The environment is always in a well-defined state \(s\), and given some action received by the actor, it will transition to a new state \(s’\) corresponding to that action. When this transition happens, the environment will spit out a reward \(r\) for transitioning from \(s\) to \(s’\): \(r = R(s,a,s’)\). Eventually some special terminal state is reached. We reached our goal or irrevocably failed the task, and our episode ends. At the beginning of each new episode, the environment can be initialized to an initial state \(s_0\).</p>

<p>This environment is a Markov decision process (MDP): go read about those on page 47 of <a href="http://incompleteideas.net/book/the-book-2nd.html" rel="external nofollow noopener" target="_blank">Sutton and Barto</a>.</p>

<h3 id="from-the-actors-point-of-view">From the actor’s point of view…</h3>
<p>The actor will take an observation of the environment’s state, process this information, and output an action to the environment. It will then record the reward \(r\) output by the environment, and continue this loop. This reward is the signifier of “good” or “bad” results from the reinforcement learning actor’s actions.</p>

<p>Following this loop forms a trajectory \(\tau\) made up of the states \(s_t\), actions \(a_t\), and rewards \(r_t\) at each time step \(t\). \((s_0, a_0, r_1), (s_1,a_1,r_2), (s_2, a_2, r_3),…\)</p>

<details><summary>“That looks funny,” I hear you say.</summary>
<p>The reward time step is offset by one for a few reasons. We don’t get any reward for the first time step \(t=0\), i.e. for initializing the state \(s_0\). Why? We haven’t taken any action yet. Also, the reward \(r\) is associated with both state \(s\) and \(s’\), which live in separate time steps. We have to pick a formalism, so we assign the reward to the latter time step \(t+1\), because the environment returns it at the same time as the next state \(s’\). However, we group it with the former tuple in our trajectory \(\tau\), because it’s associated with the action \(a_t\) we took at that time step \(t\). This tuple organization follows an intuitive loop from the actor’s point of view: “observe, act, get a reward,” then repeat.</p>

<details><summary>“But wait,” I hear you say.</summary>
<p>It would make more sense to have a “null” reward \(r_0\) at the beginning which we spit out but don’t do anything with, and form our trajectory like so: <br>
\((r_0, s_0, a_0) (r_1, s_1, a_1),(r_2, s_2, a_2), …\)<br>
The subscripts look nicer, but alas, it’s not the standard formalism. Also, this doesn’t really end the tuples at natural stopping points. Feel free to consider further, but try not to get hung up on this point. Ultimately, we’re representing the same thing, and sometime very soon you will abstract away the whole process. The following example should clear things up.</p>
</details>
</details>
<p><br></p>

<h1 id="1d-grid-world">1D Grid World</h1>
<h2 id="a-simple-environment">A Simple Environment</h2>

<p>Let’s consider a 1D grid world, where the goal is simply for the actor to be as near as it can to a particular grid space, and x is the target grid space.</p>

\[\begin{array}{|l||c|c|c|c|c|c|c|} 
\hline
\text{x=4} &amp;  &amp; &amp; &amp; &amp; x &amp; &amp; \\ 
\hline  
\text{State} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;5&amp;6\\ 
\hline 
\end{array}\]

<p>Since our goal is to be near to the target space \(x=4\), let’s define a reward function: \(R(s’) = |4-s’| + 4\). Remember \(s’\) is the state we end up in after taking an action.</p>
<details><summary>You dropped this: \(s,a\)</summary>
<p>Well well, you’re a quick learner. Yes in general, the reward is a function of \((s,a,s')\). This simple example only depends on \(s'\).</p>

</details>
<details><summary>Why \(+ 4\)?</summary>
<p>I add the 4 to keep the numbers positive, i.e. a little cleaner, but offsetting the reward function makes no difference to the RL algorithm. I could add or subtract 10,000, and in principle it will still work, especially if you are standardizing the inputs to your neural network.</p>
</details>
<p>We see each grid space take on a reward following this function:</p>

\[\begin{array}{|l||c|c|c|c|c|c|c|} 
\hline
\text{x=4} &amp;  &amp; &amp; &amp; &amp; x &amp; &amp; \\ 
\hline  
\text{State} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;5&amp;6\\ 
\hline 
\text{Reward} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;3&amp;2\\ 
\hline 
\end{array}\]

<p>Now, our actor may start at a random location, but let’s suppose it starts at 0:</p>

\[\begin{array}{|l||c|c|c|c|c|c|c|} 
\hline
\text{x=4}  &amp; o &amp; &amp; &amp; &amp; x &amp; &amp; \\ 
\hline  
\text{State} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;5&amp;6\\ 
\hline 
\text{Reward} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;3&amp;2\\ 
\hline 
\end{array}\]

<p>where o is the location of our actor. Notice here the initialization of the environment state doesn’t spit out a reward. <d-footnote>In RL, we don’t get a reward just for showing up, we get rewards for participation!</d-footnote></p>

<p>Suppose we have 3 actions available to us at a given time step. We can move left, right, or stay put. Encode these actions as -1, 1, and 0 respectively. This environment follows a deterministic state transition, i.e. a left action will always move us left one grid space, and so on. If we bop into a wall, then we stay put.</p>

<p>When we transition to the new state, we obtain the associated reward: \(r = R(s’)\). The goal in the RL problem is defined as maximizing the “return,” or the sum of rewards \(r\) in a trajectory \(\tau\). If you prefer, the trajectory’s return \(R(\tau)=\sum_{t=0}^{T}r_t\).</p>
<h2 id="introducing-the-policy">Introducing the Policy</h2>
<p>The actor maintains a policy \(\pi(a\vert s)\). For a given time step \(t\) in the trajectory \(\tau\), this function outputs the probability distribution of all possible actions \(a_t\), given the state \(s_t\). We can see the optimal policy \(\pi(a\vert s)\) which achieves this goal immediately:</p>

\[\begin{array}{|l||c|c|c|c|c|c|c|} 
\hline
\text{x=4}  &amp;  &amp; &amp; &amp; &amp; x &amp; &amp; \\ 
\hline  
\text{State} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;5&amp;6\\ 
\hline 
\text{Reward} &amp;0 &amp; 1 &amp;2&amp;3&amp;4&amp;3&amp;2\\ 
\hline 
\text{Policy} &amp;1 &amp; 1 &amp;1&amp;1&amp;0&amp;-1&amp;-1&amp;\text{1, 0, -1 = right, stay, left}\\ 
\hline 
\end{array}\]

<p>That is, step towards the target, and if you’re on the target, stay put. At the risk of being obvious, let’s show the optimal trajectory following this policy for our actor starting at position 0. Remember a trajectory \(\tau\) follows the form \((s_0, a_0, r_1), (s_1,a_1,r_2), (s_2, a_2, r_3),…\)</p>

\[\begin{array}{|c|c|c|c|c|c|c|l|} 
\hline
 o &amp; &amp; &amp; &amp; x &amp; &amp; &amp;s_0=0, a_0=1 &amp;\text{ initial state; no reward}\\ 
\hline  
  &amp;o &amp; &amp; &amp; x &amp; &amp; &amp;r_1 = 1, s_1=1, a_1=1&amp;\text{ move right}\\ 
\hline  
  &amp; &amp;o &amp; &amp; x &amp; &amp; &amp;r_2 = 2, s_2=2, a_2=1&amp;\text{ move right}\\ 
\hline  
  &amp; &amp; &amp;o&amp; x &amp; &amp; &amp;r_3 = 3, s_3=3, a_3=1&amp;\text{ move right}\\ 
\hline  
 &amp; &amp; &amp; &amp; o &amp; &amp; &amp;r_4 = 4, s_4=4&amp;\text{ terminal state; no action}\\ 
\hline  
\end{array}\]

<p>At this point the actor receives the final reward and state, and notices it has reached the goal/terminal state. No further actions are taken and the episode ends. Our return, or sum of rewards, is \(R(\tau)=\sum_{t=0}^{T}r_t = r_1+r_2+r_3+r_4 = 10\).</p>

<details><summary>Trajectory? Episode?</summary>
<p>For the purposes of this post, the trajectory is just the full episode. In general, a trajectory is any contiguous subsequence of an episode, while an episode is the full sequence from initial state \(s_0\) to terminal state \(s_{T-1}\) for an episode of length \(T\), if it ends at all.</p>

</details>

<details><summary>What if: bad grid spaces?</summary>
<p>We also could have put “pitfalls” at each end, such that the actor would receive a large negative reward, and the episode could end then, as well. Clearly an optimal policy would involve avoiding these “bad” spaces.</p>

</details>

<p>We’ll leave representing and learning the policy \(\pi\), which can be handled by all manner of RL algorithms, to <a href="/blog/2023/VPG">future posts</a> and external <a href="https://spinningup.openai.com/en/latest/user/algorithms.html" rel="external nofollow noopener" target="_blank">resources</a>. This post’s purpose is merely to lay out the problem to be solved.</p>

<p>Let’s make this picture more general so it can describe any environment interaction. <br>
<br></p>
<h1 id="extending-the-simple-picture">Extending the Simple Picture</h1>

<p>By the way, if this gets to be a bit much, there is a handy picture of an MDP at the bottom.</p>

<h2 id="imperfect-information">Imperfect information</h2>
<p>Earlier I said the actor “takes an observation” rather than “records the state.” This is because in general, the observation \(o\) recorded by the actor may be an imperfect representation of the well-defined environment state \(s\).</p>

<p>Suppose our actor is Paul Revere, and he is deciding whether to hang one or two lanterns at the Old North Church (action \(a\): 0, 1, or 2, encoding a signal to the militia: “don’t know”, “by land” and “by sea” respectively). There is an advancing British force coming in ships off the coast (state \(s\): 15,000 troops coming by sea).</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/revereride-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/revereride-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/revereride-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/revereride.jpg" class="img-fluid rounded" width="auto" height="auto" title="American dog is fully prepared to fight invaders, 1774." onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">The British are coming! Maybe.</figcaption>

</figure>

<p>However, Mr. Revere can only see a boat or two off the coast, and similarly a few carriages shuttling around on land. The British force is concealed by the fog and dark of night <d-footnote>Or worse, a sepia tone</d-footnote>. His observation \(o\) is an imperfect representation of the environment state \(s\) (observation \(o\): ~ 0, or maybe -10 (a few more people on land) or 10 (a few more at sea)).</p>

<h2 id="infinitely-long-episodes">Infinitely long episodes</h2>
<p>Next, what if our loop has no foreseeable end? In general this will be the case. Some environments go on forever, and there is nothing in our MDP picture which prevents that from happening.</p>

<p>Suppose our actor is a bipedal robot. Its task is to push an AWS server to the top of Mount Everest, because there are excellent ambient temperatures for computing up there. Unfortunately for the robot, every time it gets near the top, its servos freeze over, it loses control of the server rack, and it rolls all the way back to the bottom. And so it will try again until the end of time, or at least the end of its Amazonian overlords. All is well for the robot, who has no shortage of energy or enthusiasm.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Mount-Everest.webp-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Mount-Everest.webp-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Mount-Everest.webp-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Mount-Everest.webp" class="img-fluid rounded" width="auto" height="auto" title="Stay frosty" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">You got this, chief</figcaption>

</figure>

<p>How do we support infinite episodes? We simply mandate that every state \(s\) must accept a set of valid actions, and that such actions result in a state transition \((s,s')\). There is nowhere to “end,” and the MDP goes on forever.</p>

<p>Does this picture still support finite episodes? Notice that a state \(s\) can transition to itself \((s’ = s)\). To mark a state as terminal, we only allow it to transition to itself. Technically this is still an infinite MDP: our picture hasn’t changed, it will transition to itself forever. But if we reach a particular state or set of states, we can decide to stop traversing and end the infinite episode prematurely.</p>

<h2 id="discounted-rewards">Discounted rewards</h2>
<p>Now, let me ask you a question. You’ve won a million dollars. Congrats. Would you like your prize now, or in 30 years? Everyone can agree on the answer. If you have it now, you can improve your life, or others lives, now. If you’re worried about self control, stick it in a trust and only touch the dividends. What use is there in waiting?</p>

<p>In other words, reward now is better than reward later, else you’re just wasting time. What’s more, remember our trajectory is infinitely long in general. Ultimately we need to do calculations with the sum to learn from it, and we can’t do that with infinite numbers. To handle this, our actor’s more generalized goal is to maximize the discounted sum of rewards. <br>
\(R(\tau)=\sum_{t=0}^{T}\gamma^tr_t\)<br>
\(\gamma\) will usually be set to some number very close to 1, like 0.98. We add the discounting term so that our return converges to some finite value. We can see that as the time step gets further into the future, the discounting factor \(\gamma\) will make reward term decay to 0, and assuming no one reward is infinite, the sum will never be infinity.</p>
<details><summary>We have to go back…</summary>
<p>Notice we can recover our original un-discounted picture simply by setting \(\gamma=1\).</p>

</details>

<h2 id="probabilistic-state-transitions">Probabilistic state transitions</h2>
<p>Our robot from earlier is halfway up the mountain and tries to push forward one more step, but randomly a gust of wind causes him to lose his grip on the AWS server, rolling back down.</p>

<p>What if a particular action \(a\) from a particular observation \(o\) doesn’t always result in the same state transition \((s,s')\)? This is supported by probabilistic state transitions in the MDP. A definite action is taken, then state \(s\) will transition to \(s’\), but \(s’=0\) (the bottom of the mountain) with 5% probability, and \(s’=s+1\) the rest of the time.</p>

<p>In a simulated environment this can be represented by a transition function \(p(s’\vert s, a)\), with a vector of probabilities for all reachable \(s’\) from \(s\), for a particular action \(a\).</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Markov_Decision_Process.svg-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Markov_Decision_Process.svg-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Markov_Decision_Process.svg-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Markov_Decision_Process.svg.png" class="img-fluid rounded" width="auto" height="auto" title="MDP" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

<div class="caption">
An example of the full MDP. This encapsulates states (green nodes), actions (red nodes), rewards upon state transition (emitted squiggles), and nondeterministic transitions (arrows from red nodes).
<d-footnote>From the wikipedia page for MDPs.</d-footnote>
</div>

<p>That’s our MDP picture and the RL problem; not so bad, is it? Of course, we haven’t done any learning yet! See the next post on <a href="/blog/2023/VPG">VPG</a> for how to learn from interacting with the environment.</p>

<hr>
<p>My understanding of the subject comes from David Silver’s excellent <a href="https://www.deepmind.com/learning-resources/introduction-to-reinforcement-learning-with-david-silver" rel="external nofollow noopener" target="_blank">lectures</a>, Open AI’s <a href="https://spinningup.openai.com/en/latest/" rel="external nofollow noopener" target="_blank">spinning up</a>, the 2017 Berkeley <a href="https://sites.google.com/view/deep-rl-bootcamp/home" rel="external nofollow noopener" target="_blank">Deep RL Bootcamp</a>, Pieter Abbeel’s and Sergey Levine’s various lectures on YouTube, and Sutton and Barto’s “<a href="http://incompleteideas.net/book/the-book-2nd.html" rel="external nofollow noopener" target="_blank">Reinforcement Learning</a>” (which was referenced by the others). These are excellent resources, and I recommend you check them out.</p>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "kjabon/kjabon.github.io",
        "data-repo-id": "R_kgDOIuhwgg",
        "data-category": "Comments",
        "data-category-id": "DIC_kwDOIuhwgs4CVKPS",
        "data-mapping": "url",
        "data-strict": "0",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "top",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        © Copyright 2023 Kenneth  Jabon. Powered by al-folio.<br>Last updated: April 14, 2023.
      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', '');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
