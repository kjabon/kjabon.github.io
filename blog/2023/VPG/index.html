<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Intro to Vanilla Policy Gradient | Kenneth  Jabon</title>
    <meta name="author" content="Kenneth  Jabon">
    <meta name="description" content="Theory and intuition behind one of our introductory algorithms">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://kjabon.github.io/blog/2023/VPG/">
    
    <!-- Dark Mode -->
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <!-- Page/Post style -->
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "Intro to Vanilla Policy Gradient",
      "description": "Theory and intuition behind one of our introductory algorithms",
      "published": "April 14, 2023",
      "authors": [
        {
          "author": "Kenneth Jabon",
          "authorURL": ""
          
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Kenneth </span>Jabon</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repos</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>Intro to Vanilla Policy Gradient</h1>
        <p>Theory and intuition behind one of our introductory algorithms</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#a-bird-s-eye-view">A bird's eye view</a></div>
            <div><a href="#breaking-down-the-policy-gradient">Breaking down the policy gradient</a></div>
            <div><a href="#learning-from-rewards">Learning from rewards</a></div>
            
          </nav>
        </d-contents>

        <p>Vanilla policy gradient is one of the simplest reinforcement learning algorithms. It should serve to form the theoretical foundation for many following policy-based, online algorithms. If you’re more interested in offline algorithms, I recommend you start with TD-learning and deep Q-learning. See chapter 6 of <a href="http://incompleteideas.net/book/the-book-2nd.html" rel="external nofollow noopener" target="_blank">Sutton and Barto</a> for that.</p>

<p>This post assumes you’re familiar with how Markov Decision Processes work, and how the reinforcement learning problem is set up. For a guide to that, see <a href="/blog/2023/RL">here</a>.</p>

<h2 id="a-birds-eye-view">A bird’s eye view</h2>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/eagle-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/eagle-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/eagle-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/eagle.jpg" class="img-fluid rounded" width="auto" height="auto" title="bird" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

<p>Let’s describe what happens in a full iteration of the loop before diving in.</p>

<p>First, we collect a batch of trajectories \(\tau\) by allowing our current policy \(\pi\) (represented by a neural network) to unfold in the environment. We collect a full batch because, in general, the policy does not output actions \(a\), but a probability distribution over actions given the current state \(s\): \(\pi(a\vert s)\). To get to the next timestep in our trajectory, we select an action \(a\) by sampling from this distribution. Randomness may also occur as the result of the environment itself, so all in all we want to get plenty of samples to come up with an accurate, representative set of trajectories \(\tau\).</p>

<details><summary>Sampling is great for blind exploration, but…</summary>
<p>Eventually sampling doesn’t do the job any more. At some point we’ll want to exploit what we know and make our way to better states to be able to learn from there, so we’ll just pick the best known action instead of sampling. The amount of “greedy action selection” will be scheduled to increase over time, to progress from pure exploration of the state space to pure exploitation of the policy.</p>
</details>

<p>Now, if a policy gains an above-average sum of rewards in a particular trajectory, we will nudge the policy \(\pi\) in the direction of the actions \(a\) (given their respective states, \(\vert s\)) which resulted in this trajectory. Conversely, if a trajectory comes with a below-average sum of rewards, we nudge the policy away from taking those actions. This sum is known as the <strong>return</strong> for a trajectory<d-footnote>This isn't the full return yet, we'll make it more general in a bit.</d-footnote>:</p>

\[R(\tau)=\sum_{t=0}^{T}r_t\]

<p>If any of this seems unclear, you’ll want to start with the <a href="/blog/2023/RL">previous</a> post.</p>

<p>To estimate the return at each time step with low variance, we employ generalized advantage estimation (GAE). We train a neural network to represent the value function \(V\), which is incorporated into our advantage term \(A\). This will be discussed in the <a href="#generalized-advantage-estimation">section on GAE</a>.</p>

<p>Now, every time we go through our loop, we have a better estimate of what a good trajectory looks like (by training the value function \(V\)), and a better idea of what actions to take to get good trajectories (by training the policy \(\pi\)). This glosses over some details, which we’ll get into in just a bit.</p>

<p>Take a glance over the simplified pseudocode for the algorithm, then let’s get cracking!</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pseudoPseudo.svg-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pseudoPseudo.svg-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pseudoPseudo.svg-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pseudoPseudo.svg" class="img-fluid rounded" width="auto" height="auto" title="Algorithm 0" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

<h2 id="breaking-down-the-policy-gradient">Breaking down the policy gradient</h2>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/jackhero-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/jackhero-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/jackhero-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/jackhero.jpg" class="img-fluid rounded" width="auto" height="auto" title="jack" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

<p>The policy gradient is calculated as follows:</p>

\[\hat{g}_{k}=\frac{1}{\left\vert \mathcal{D}_k \right\vert }\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} \hat{V}_t\]

<p>Let’s break this down to gain an understanding of each term, working from the inside out. Let’s start with the policy:</p>

\[\pi_\theta (a_t\vert s_t)\]

<p>which is the function which outputs the probability distribution of all possible actions \(a_t\) at time \(t\) in the trajectory \(\tau\), given the state \(s_t\) at time \(t\). To bring it back to earth, think of this as \(y = f(x,\theta)\).</p>

<details><summary>O_O</summary>
<p>If this is making you feel like a deer in headlights, don’t worry. Let’s be more explicit. \(\theta\) parameterizes our function \(f\), and then we evaluate our function on \(x\). What do I mean? If I had a function \(y = f(x, \theta) = mx + b\), I parameterize this function with \(m\) and \(b\) (represented by the vector \(\theta = [m,b]\)), and then evaluate it on \(x\). In the case of a neural net, \(\theta\) is instead a matrix of numbers (which can be flattened into a vector \([a_1,a_2,a_3,...,a_n]\)) representing its weights and biases.</p>

</details>

<ul>
  <li>If we have a discrete action space, this is a function which outputs a vector with a logit for each possible action (in other words, a categorical probability distribution).</li>
  <li>If the action space is continuous, this is a function which outputs the mean \(\mu\) and standard deviation \(\sigma\) representing the action’s probability distribution. <d-footnote> If you're worried about flexibility, it turns out you can also output arbitrary combinations of these, but we won't consider that case here.</d-footnote>
</li>
</ul>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/Fig0_ManimCE_v0.17.3-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/Fig0_ManimCE_v0.17.3-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/Fig0_ManimCE_v0.17.3-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/Fig0_ManimCE_v0.17.3.png" class="img-fluid rounded" width="auto" height="auto" title="nn outputs" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

<p><strong>Take the log</strong> of the policy:</p>

\[\log\pi_\theta (a_t\vert s_t)\]

<p>Remember, all that is happening is we’re taking the log of a function, i.e., \(g(x, \theta) = \log(f(x, \theta))\).<br>
<br></p>

<p><strong>Take the gradient</strong> of this with respect to the policy parameters \(\theta\).</p>

\[\nabla_\theta \log\pi_\theta (a_t\vert s_t)\]

<p>Simply, the gradient of a function, i.e.  \(\nabla g(x, \theta)\) with respect to \(\theta\).<br>
<br></p>

<p><strong>Evaluate the policy</strong> given the current policy parameters \(\theta_k\):</p>

\[\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\]

<p>Or, we perform inference with the current model on the current state of the environment.</p>

<details><summary>Numerical considerations</summary>
<p>Let’s take a step back. If we are used to working out derivations with pencil and paper, the order in which I presented the last few steps should not start sounding any alarms.<d-footnote>Assuming you made it through Calculus in one piece. If not, don't worry. Go ahead and take or re-take Calculus, and then come back. You can do it, I promise. If that's too much hassle, luckily we have autodiff which means you can summarily forget about this derivation; so just keep reading with a glazed look for a couple more paragraphs. </d-footnote></p>

<p>Normally in this case one would take the derivative of the symbolic function, then evaluate that to get the derivative at the point of interest, or a similar method of your choice.</p>

<p>However, there are two differences when doing this numerically on your computer; having to do with the derivative and the log.</p>

<h3 id="for-the-derivative">For the derivative:</h3>
<p>Instead, we just say “hello, new best friend, the autodiff function <d-footnote>For example, jax.grad()</d-footnote>! I’m going to run the code which evaluates \(\log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\) for an entire batch of states in one vectorized/parallelized operation, across whatever computational resources I have available. Can you please give me \(\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\)?” And our new best friend is happy to oblige.</p>

<p>How does our new friend work? I’ll mostly defer to good explanations elsewhere, for example <a href="https://github.com/HIPS/autograd/blob/master/docs/tutorial.md#whats-going-on-under-the-hood" rel="external nofollow noopener" target="_blank">here</a> and <a href="https://en.wikipedia.org/wiki/Automatic_differentiation" rel="external nofollow noopener" target="_blank">here</a>. The gist is that first, it keeps track of the computation graph of whatever functions you want to differentiate, as you execute them, from state \(s\) and parameters \(\theta\) all the way to \(\log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\). Then, it uses this graph to multiply its way through the chain rule, resulting in the gradient(s) you want.</p>

<h3 id="for-the-log">For the log:</h3>

<p><strong>For categorical distributions</strong>, it’s very simple.</p>

<p>We configure our neural network output to be logits, and later follow this with a softmax to convert to probabilities. We then simply compute the log of these values: \(\log \left[ P_\theta (s)\right]\). Since this is a vector, to get the log probability for the action of interest \(a\), (\(a\) will be an integer from 0 to n-1, n being the dimensionality of the actions), we simply grab the \(a\)‘th value from this vector. In other words, we compute \(\log \left[ P_\theta (s)\right]_a\). <d-footnote>Thank you to ALPH2A on the RL discord for pointing out a discrepancy here.</d-footnote><br>
&lt;/div&gt;</p>

<p><strong>For continuous distributions</strong>, it’s a little trickier</p>

<p>Remember we usually represent the probability distribution in the continuous case as a multivariate normal distribution. To keep things simple, we actually use a diagonal covariance matrix, rather than the full covariance matrix, so each dimension of the action can be represented by a single standard deviation. This way, we only need to output one mean and one standard deviation per dimension, and calculating the log of the distribution also becomes much easier. Now, how do we take the log of a (diagonal) normal distribution?</p>

<p>Like this!</p>

\[\log\pi_\theta (a\vert s) = -\frac{1}{2}\left(n\log 2\pi+\sum_{i=1}^n\left[\frac{(a_i-\mu_i)^2}{\sigma_i^2}+2\log\sigma_i\right]\right)\]

<p>where n is the dimensionality of the action space. If you’re not convinced (I wasn’t), <a href="/blog/2023/ContinuousLogLikelihood/">here’s a derivation</a>.</p>

<p>Our neural network spits out a vector each of \(\mu\) and \(\sigma\), one element for each action dimension. With these values, we sample from a diagonal MVN distribution, yielding a vector of actions \(a\). Then, the above equation describes the log likelihoods of that vector of actions.</p>

<p>See <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#stochastic-policies" rel="external nofollow noopener" target="_blank">here</a> for reference.</p>

</details>

<p>Finally, <strong>summing this term</strong> over all time steps in our current trajectory gives us…</p>

<h3 id="the-meat-of-vpg">The meat of VPG</h3>

\[\sum_{t=0}^{T}\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\]

<p>A <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient" rel="external nofollow noopener" target="_blank">brief derivation</a> results in this full term, the grad log of the current policy. This term is the gradient of the probability of the current trajectory with respect to the policy parameters, evaluated using the current parameters. For that reason, let’s call this the probability gradient. (\(\nabla_\theta P(\tau)\), where \(\tau\) is the current trajectory).</p>

<p>In a moment, we’re going to add the \(V\) term we introduced in our bird’s eye view. Let’s review what we have so far without it: the probability gradient \(\nabla_\theta P(\tau)\).</p>

<ul>
  <li>Without the sum over time steps (\(\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\)), this tells us how much the probability distributions of our actions change, given a small change in each of our neural network parameters \(\theta\).</li>
  <li>With the sum over time steps (\(\sum_{t=0}^{T}\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\) ), it instead tells us how much the probability of that sequence of time steps (the trajectory \(\tau\)) changes, again for a small change in our parameters \(\theta\).</li>
</ul>

<p>This is great: if we want to change the probability of a particular trajectory (\(P(\tau)\)), we have the information to do that! (\(\nabla_\theta P(\tau)\))</p>

<p>The main remaining question is this: do we want our current trajectory \(\tau\) to happen more or less often? Well, I think we can agree, we want to make good trajectories happen more often, and bad trajectories happen less often. So how good (or bad) is our current trajectory?</p>

<h2 id="learning-from-rewards">Learning from rewards</h2>

<p>The return \(R\) of a trajectory \(\tau\) is defined as the discounted sum of rewards obtained in that trajectory. The higher the return, the better the trajectory, and vice-versa. See the <a href="/blog/2023/RL">RL post</a> for an intro to this concept.</p>

<p>Following directly from the policy gradient discussion above: we now know not only how to change a trajectory’s probability using \(\nabla_\theta P(\tau)\), but also whether to make it more or less probable: each probability gradient in the sum is weighted by the return \(R(\tau)\). That is, we have:</p>

\[\hat{g}=\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} R\]

<p>which, again, tells us both how to change the probability of actions leading to the trajectory \(\tau = (s_0, a_0, r_1), (s_1,a_1,r_2), (s_2, a_2, r_3),…\), and now that we have the weight \(R(\tau)\), we know whether to make the action \(a_t\vert s_t\) more or less likely to happen. This summed over all time steps gives us the full policy gradient term.</p>

<h3 id="congrats-youve-made-it-to-the-policy-gradient">Congrats! You’ve made it to the policy gradient!</h3>

<p>If you stopped here, this rudimentary policy gradient would fit into our algorithm above, and that would be able to solve simple environments, albeit a little inefficiently. A golden retriever might eventually be trained to drive a car, but man, is it going to be hard going. And you know that he’s going to stop for treats and crash into mailmen.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/goldenRetriever-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/goldenRetriever-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/goldenRetriever-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/goldenRetriever.png" class="img-fluid rounded" width="auto" height="auto" title="watered down vanilla ice cream" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

<div class="caption">
He's trying, give him credit.
<d-footnote>From DALL-E</d-footnote>
</div>

<p>No, we can do better than this. Take a breather, then let’s extend this picture.</p>

<hr>

<p>Well, this is about a 15 minute read (or more) already.</p>

<p>Let’s wrap this up in the <a href="/blog/2023/VPG2">next post</a>!</p>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "kjabon/kjabon.github.io",
        "data-repo-id": "R_kgDOIuhwgg",
        "data-category": "Comments",
        "data-category-id": "DIC_kwDOIuhwgs4CVKPS",
        "data-mapping": "url",
        "data-strict": "0",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "top",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        © Copyright 2023 Kenneth  Jabon. Powered by al-folio.<br>Last updated: July 20, 2023.
      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', '');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
