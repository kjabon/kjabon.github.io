<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Vanilla Policy Gradient | Kenneth  Jabon</title>
    <meta name="author" content="Kenneth  Jabon">
    <meta name="description" content="The theory behind one of our introductory algorithms">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://kjabon.github.io/blog/2023/VPG/">
    
    <!-- Dark Mode -->
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <!-- Page/Post style -->
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "Vanilla Policy Gradient",
      "description": "The theory behind one of our introductory algorithms",
      "published": "April 14, 2023",
      "authors": [
        {
          "author": "Kenneth Jabon",
          "authorURL": ""
          
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Kenneth </span>Jabon</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repos</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>Vanilla Policy Gradient</h1>
        <p>The theory behind one of our introductory algorithms</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#a-bird-s-eye-view">A bird's eye view</a></div>
            <div><a href="#breaking-down-the-policy-gradient">Breaking down the policy gradient</a></div>
            <div><a href="#learning-from-rewards">Learning from rewards</a></div>
            <div><a href="#the-value-function">The value function</a></div>
            <div><a href="#generalized-advantage-estimation">Generalized Advantage Estimation</a></div>
            <div><a href="#batched-training-of-neural-nets">Batched training of neural nets</a></div>
            
          </nav>
        </d-contents>

        <p>Vanilla policy gradient is one of the simplest reinforcement learning algorithms. It should serve to form the theoretical foundation for all following policy-based, online algorithms. If you’re more interested in offline algorithms, I recommend you start with TD-learning and deep Q-learning. See chapter 6 of <a href="http://incompleteideas.net/book/the-book-2nd.html" rel="external nofollow noopener" target="_blank">Sutton and Barto</a> for that.</p>

<p>This post assumes you’re familiar with how Markov Decision Processes work, and how the reinforcement learning problem is set up. For a guide to that, see <a href="/blog/2023/RL">here</a>.</p>

<h2 id="a-birds-eye-view">A bird’s eye view</h2>

<p>Let’s describe what happens in a full iteration of the loop before diving in.</p>

<p>First, we collect a batch of trajectories \(\tau\) by allowing our current policy \(\pi\) (represented by a neural network) to unfold in the environment. We collect a full batch because, in general, the policy does not output actions \(a\), but a probability distribution over actions given the current state \(s\): \(\pi(a\vert s)\). To get to the next timestep in our trajectory, we select an action \(a\) by sampling from this distribution. Randomness may also occur as the result of the environment itself, so all in all we want to get plenty of samples to come up with an accurate, representative set of trajectories \(\tau\).</p>

<details><summary>Sampling is great for blind exploration, but…</summary>
<p>Eventually sampling doesn’t do the job any more. At some point we’ll want to exploit what we know and make our way to better states to be able to learn from there, so we’ll just pick the best known action instead of sampling. The amount of “greedy action selection” will be scheduled to increase over time, to progress from pure exploration of the state space to pure exploitation of the policy.</p>
</details>

<p>Now, if a policy gains an above-average sum of rewards in a particular trajectory, we will nudge the policy \(\pi\) in the direction of the actions \(a\) (given their respective states, \(\vert s\)) which resulted in this trajectory. Conversely, if a trajectory comes with a lower sum of rewards, we nudge the policy away from taking those actions. This sum is known as the <strong>return</strong> for a trajectory<d-footnote>This isn't the full return yet, we'll make it more general in a bit.</d-footnote>:</p>

\[R(\tau)=\sum_{t=0}^{T}r_t\]

<p>If any of this feels poorly defined or explained, you’ll want to start with the <a href="/blog/2023/RL">previous</a> post.</p>

<p>To estimate the return at each time step with low variance, we employ generalized advantage estimation (GAE). We train a neural network to represent the value function \(V\), which is incorporated into our advantage term \(A\). This will be discussed in the secion on <a href="#generalized-advantage-estimation">GAE</a>.</p>

<p>Now, every time we go through our loop, we have a better estimate of what a good trajectory looks like (by training the value function \(V\)), and a better idea of what actions to take to get good trajectories (by training the policy \(\pi\)). This glosses over some details, which we’ll get into in just a bit.</p>

<p>Take a glance over the simplified pseudocode for the algorithm, then let’s get cracking!</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/pdf/pseudoPseudoVPG.pdf-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/pdf/pseudoPseudoVPG.pdf-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/pdf/pseudoPseudoVPG.pdf-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/pdf/pseudoPseudoVPG.pdf" class="img-fluid rounded" width="auto" height="auto" title="pseudoPseudo" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

<h2 id="breaking-down-the-policy-gradient">Breaking down the policy gradient</h2>

<p>The policy gradient is calculated as follows:</p>

\[\hat{g}_{k}=\frac{1}{\left\vert \mathcal{D}_k \right\vert }\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} \hat{V}_t\]

<p>Let’s break this down to gain an understanding of each term, working from the inside out.</p>

<p>\(\pi_\theta (a_t\vert s_t)\) is the policy, or the function which outputs the probability distribution of all possible actions at time \(t\) in the trajectory \(\tau\), given the state at time \(t\). To bring it back to earth, think of this as \(y = f(x,\theta)\).</p>

<details><summary>Huh?</summary>
<p>If this is making you feel like a deer in headlights, don’t worry. Let’s be more explicit. \(\theta\) parameterizes our function \(f\), and then we evaluate our function on \(x\). What do I mean? If I had a function \(y = f(x, \theta) = mx + b\), I parameterize this function with \(m\) and \(b\) (represented by the vector \(\theta = [m,b]\)), and then evaluate it on \(x\). In the case of a neural net, \(\theta\) is instead a matrix of numbers (which can be flattened into a vector \([a_1,a_2,a_3,...,a_n]\)) representing its weights and biases.</p>

</details>

<ul>
  <li>If we have a discrete action space, this is a function which outputs a vector with a value for each possible action (in other words, a categorical probability distribution).</li>
  <li>If the action space is continuous, this is a function which outputs the mean \(\mu\) and standard deviation \(\sigma\) representing the action’s probability distribution.</li>
</ul>

<p>Take the log of this term:</p>

\[\log\pi_\theta (a_t\vert s_t)\]

<p>Remember, all that is happening is we’re taking the log of a function, i.e., \(g(x, \theta) = log(f(x, \theta))\).<br>
<br></p>

<p>We take the gradient of this with respect to the policy parameters \(\theta\).</p>

\[\nabla_\theta \log\pi_\theta (a_t\vert s_t)\]

<p>Simply, the gradient of a function, i.e.  \(\nabla g(x, \theta)\) with respect to \(\theta\).<br>
<br></p>

<p>We evaluate the policy given the current policy parameters \(\theta_k\):</p>

\[\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\]

<p>Or, we perform inference with the current model on the current state of the environment.</p>

<details><summary>Numerical considerations</summary>
<p>Let’s take a step back. If we are used to working out derivations with pencil and paper, the order in which I presented the last few steps should not start sounding any alarms.<d-footnote>Assuming you made it through Calculus in one piece. If not, don't worry. Go ahead and take or re-take Calculus, and then come back. You can do it, I promise. If that's too much hassle, luckily we have autodiff which means you can summarily forget about this derivation; so just keep reading with a glazed look for a couple more paragraphs. </d-footnote> Normally in this case one would take the derivative of the symbolic function, then evaluate that to get the derivative at the point of interest, or a similar method of your choice.</p>

<p>However, there are two differences when doing this numerically on your computer; having to do with the log and the derivative.</p>

<h3 id="for-the-log">For the log:</h3>
<p>We’re using a neural network, whose job it is to approximate a function’s output. Well, why go through the trouble of approximating a function, and then taking the log of the output, when we could just encapsulate the log into the function and approximate that directly? And this exactly what we do: we output log standard deviations \(\log \sigma_\theta\).</p>

<h3 id="for-the-derivative">For the derivative:</h3>
<p>Instead, we just say hello to our new best friend, the autodiff function <d-footnote>For example, jax.grad()</d-footnote>. I’m going to run the code which evaluates \(\log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\) for an entire batch of states in one vectorized/parallelized operation, across whatever computational resources I have available. Can you please give me \(\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\)? And our new best friend is happy to oblige. Implementation details can be found in the next <a href="/blog/2023/VPGInJax">post</a>.</p>

<p>How does our new friend work? I’ll mostly defer to good explanations elsewhere, for example <a href="https://github.com/HIPS/autograd/blob/master/docs/tutorial.md#whats-going-on-under-the-hood" rel="external nofollow noopener" target="_blank">here</a> and <a href="https://en.wikipedia.org/wiki/Automatic_differentiation" rel="external nofollow noopener" target="_blank">here</a>. The gist is that first, it keeps track of the computation graph of whatever functions you want to differentiate, from state \(s\) and parameters \(\theta\) all the way to \(\log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\). Then, it uses this graph to multiply its way through the chain rule, resulting in the gradient(s) you want. And of course, this is done in parallel for all the time steps in your trajectory.</p>

</details>

<p>\(\sum_{t=0}^{T}\nabla_\theta \log\pi_\theta (a_t\vert s_t)\vert_{\theta_k}\) We sum this term over all time steps in our current trajectory. A <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient" rel="external nofollow noopener" target="_blank">brief derivation</a> results in this full term, the grad log of the current policy. This is the meat of VPG. This term is the gradient of the probability of the current trajectory with respect to the policy parameters, evaluated at the current parameters. For that reason, let’s call this the probability gradient. (\(\nabla_\theta P(\tau)\), where \(\tau\) is the current trajectory).</p>

<p>In a moment, we’re going to add the V_t term. Let’s review what we have so far without it: the probability gradient Grad_theta P(tau). Without the sum over time steps, this tells us how much the probability distributions of our actions change, given a small change in each of our neural network parameters. Re-adding the sum over time steps, it instead tells us how much the probability of that sequence of time steps (the trajectory) changes, again for a small change in our parameters. This is great: if we want to change the probability of a particular trajectory, we have the information to do that!</p>

<p>The main remaining question is this: do we want our current trajectory to happen more or less often? Well, I think we can agree, we want to make good trajectories happen more often, and bad trajectories happen less often. So how good (or bad) is our current trajectory?</p>

<h2 id="learning-from-rewards">Learning from rewards</h2>

<p>The return of a trajectory is defined as the discounted sum of rewards obtained in that trajectory. The higher the return, the better the trajectory, and vice-versa. See the RL post() for an intro to this concept. Following directly from the policy gradient discussion above: we now know not only how to change a trajectory’s probability, but also whether to make it more or less probable: each probability gradient in the sum is weighted by the return!</p>

<p>To be explicit: if an action corresponds to an above-average return, we use that information combined with the probability gradient at that time step to nudge the policy in the direction of the action (given the state it was acting on) which resulted in the future return from that time step. (And vice-versa). This summed over all time steps gives us the full policy gradient term.</p>

<p>Show the equation again, except with R_t instead of V_t</p>

<p>Let’s take a step back and think about this.</p>

<p>If I take an action, what parts of the trajectory will this affect? I’m not currently in possession of a time machine, so any action I take will not affect the past. &gt;Even if history is written by the victor, recorded events do not necessarily reflect what actually transpired.&lt; And of course if I take an action, that will carry me to the next time step, so I cannot affect the present either. I can only affect the future. Therefore, the part of the trajectory (and thus, the rewards) that any action has bearing on is the remainder of the trajectory from the next time step to the end of the episode. I’ll call the discounted sum of future rewards “future return” or R^f.</p>
<blockquote>
  <blockquote>
    <p>This set of rewards is sometimes known as the “rewards-to-go.” This term always struck me as kind of confusing, maybe because “to-go” isn’t as precise as I’d like. About half the time I see it, I think “to-go?’ Where are the rewards going? Oh, you mean to-go’ as in what we have to go, or remaining.” Let’s avoid any temporary confusion and use “future return.”</p>
  </blockquote>
</blockquote>

<p>Show the equation again, except with R^f_t instead.</p>

<p>Let’s be as clear as possible. Assume T time steps t in a trajectory, t in [0,T), or t in [0,T-1] if you prefer. The future return R^f_t for the first time step (t=0) in a trajectory, R^f_0, contains all the rewards in that trajectory. R^f_1 contains all rewards, except the first reward. R^f_T-1(the final time step) contains no rewards, because there’s no future! R^f_T-2 contains only the final reward.</p>

<p>Now, how do we actually calculate R^f_t for a particular time step? It seems obvious: simply take the discounted sum of all the future rewards! Well, it’s a bit trickier than that. The obvious obstacle to this is that you never have the future rewards for a time step as you’re playing out that step. I don’t have a time machine that goes forward, either. You only have immediate access to the rewards you don’t care about, those in the past. So you’re forced to keep “rolling out” the trajectory to its conclusion, recording all future time steps, and only then you can accurately calculate R^f_t for all time steps. »R^f_t is easy enough to calculate in one backward-in-time pass. Starting from the final time step, each R_t equals the reward of the current time step plus the sum of the rewards from the next time step.«</p>

<p>Whew. That seems a lot more tedious than we were hoping for. »If my episode is infinite, it’s technically impossible. I can get around the infinite sum by remembering I’m discounting, and truncate my sum at a reasonable time in the future. « I would like my algorithm to be “online,” meaning I would like to avoid having to collect full episodes before knowing what my future return is. If I could do that, I would be able to learn from the reward of every time step in real time, and update my policy accordingly.</p>

<p>Imagine if you were playing soccer for the first time, but the coach said you weren’t allowed to correct your mistakes while you were playing! No no, only in the time between full games can you reflect on the error of your ways, and think how to improve for next time. Until then, suck it up and keep tripping over your feet. Intuitively, this seems inefficient, if not patently ridiculous.</p>

<blockquote>
  <blockquote>
    <p>Now, modern “offline” algorithms aren’t actually this bad. They usually collect short sequences of transitions rather than full trajectories, and store them in a buffer to be sampled and asynchronously learned from. So the “reflection between games” is happening at all times, and “reflecting on all past games,” so to speak, rather than the most recent experience. In other words, in “online” algorithms the state space currently being learned from is the same that’s currently being explored by the active policy. However in “offline” algorithms the current policy (implicit or explicit) sets the explored state space, but the buffer of historical data encapsulates a potentially different state space. There are pros and cons to each. «</p>
  </blockquote>
</blockquote>

<p>The smaller the time lag in learning, the faster I can gain information about my new policy, and update it again. So, how to access the future return now instead of later? Approximate it with the value function V!</p>

<h2 id="the-value-function">The value function</h2>

<p>The value function’s (V_t) job is to approximate the return for a particular time step. This is a lot simpler than it may seem. The value function answers one question: how good is this state to be in? Or, given the state at time step t, what is the best approximation of the expected return? With a deep neural network, we can quickly dispatch this question with good old supervised learning. We have a trajectory of states at each time step, and at the end of an episode (or the effective time horizon defined by the discounting term gamma) we’re able to calculate the future return for each time step. All that follows is to state the loss function to minimize, and we’re off to the races. (Divide this by T)</p>

<p>And with that, here is the full policy gradient:<br>
(Math, no sum over trajectories).</p>

<p>A few caveats.</p>

<p>This is an approximation which may only see a subset of the state space at any given time, and it may never see the full state space. We cannot assume it is unbiased unless it has trained equally as much on data from all parts of the state space, and typically that will only be the case once the training is complete, if it happens at all. In parts of the state space which are relatively less explored, it may even be very inaccurate, which could lead to learning a falsely optimal policy.</p>

<p>One of the best solutions to this problem is seen in the RL algorithm (PPO)[paper]. »Its goal is to avoid veering too far out into unexplored territory too quickly. It allows the value function time to “acclimate” to its new surroundings and give more accurate estimates, and so the policy is always learning from a “good enough” value function. It accomplishes this by limiting the size of the policy update step by clipping a surrogate loss function - but I won’t digress too much on this point in this post, it requires more time to do it justice.«</p>

<p>It also turns out that this simple solution is quite high-variance in practice. Because of this, it’s difficult for the actor to learn a good policy and to do so quickly. Luckily there are many battle-hardened techniques for reducing variance.</p>

<p>Take a look at <a href="gae%20paper">generalized advantage estimation</a> (GAE) for lower-variance, lower bias estimates of how much to update policy parameters.</p>

<h2 id="generalized-advantage-estimation">Generalized Advantage Estimation</h2>
<p>What is the lowest bias approximation of the return for a state? Well, the return itself! That is, the actual sum of rewards that we calculate. “But hang on, I thought we were trying to avoid doing that to get quick feedback?” Indeed we are. So perhaps we can combine the two approaches. What if I use the actual reward in the next time step, and add it to the value function’s approximation of the future return from that point on? Aha, a slightly more accurate estimate!</p>

<p>Perhaps we can even do a little better by using the next two real rewards instead of only one. Or the next three? Each addition will require a slightly longer delay between taking an action and being able to learn from it. Now the question becomes: what’s the optimal tradeoff between accuracy and this learning delay? At this point you just need to experiment and see what works best for your problem, but I will tell you this: the answer is somewhere between 100% accuracy and zero delay (show the chart here). It also turns out you can do even better by doing a weighted average of all of these options: one, two, three, etc. real rewards, followed by an approximation, and also experimenting to see what the right weighting is. That’s GAE in a nutshell. The sum will of course be discounted as usual, so all but the first term have \gamma^t applied to them.</p>

<p>We’ve left out one important piece, though. This isn’t actually “Advantage” yet: we haven’t addressed how to lower the variance of our approximation. Okay. We have a more accurate approximation with our “immediate reward r_t plus delayed value function V(s_t+1),” as compared to the non-delayed value function V(s_t) alone. What is the source of the variance? Well, the approximation of the value function, of course. Since the value function at each time step is approximated by the same neural network, we should expect it to contribute similar “noise” or “variance” to our “signal” or “true value.” If we subtract “pure approximation” V(s_t) from “slightly more accurate approximation” r_t + V(s_t+1), we remove variance and are mostly left with the true value we care about: the advantage A_t, or how much better is this action than average? Putting all this together gives us GAE: a low variance estimator for the advantage.</p>

<p>I’ve focused on intuition here. If you’re interested in the math of how the weighted averaging and so on works out, go take a look at the paper; your mental picture should now be organized to have all those formalisms swiftly fall into place. Finally we can convert our \(V_t\) to \(A_t\).</p>

<h2 id="batched-training-of-neural-nets">Batched training of neural nets</h2>
<p>Finally, we sum this term over a batch of collected trajectories, dividing by the number of trajectories to get the sampled mean of the above (the grad log time-step summation). Why? Well, to gain a lower variance estimator of the true policy gradient. This should be quite familiar to you if you’ve ever done mini-batched stochastic gradient descent. Many trajectories averaged together will smooth out the noise from any one trajectory, and better represent the optimal policy gradient, or the step towards the true optimal policy.</p>

<p>Ok, finally, we have the sampled policy gradient: (show full eqn)</p>

<p>With this term, we can update the parameters of our policy, which in this case is an MLP.</p>

\[\theta_{k+1} = \theta_{k}+\alpha_k \hat{g}_k\]

<p>Refers to the updated parameters,  the parameters to be updated, and the learning rate (a hyper parameter).</p>

<p>This is the policy learning taken care of. Now we turn to the value function, doing the same sum over trajectories, for the same reason:</p>

<p>All we have to do is perform inference with the model at all time steps along the trajectory, for all collected trajectories, and we have all the ingredients we need to perform batched gradient descent of the model to minimize the loss function, as shown below.<br>
\(\theta_{k+1}=\arg \min_\phi \frac{1}{\left\vert  \mathcal{D}_k \right\vert T}\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\left(V_\phi(s_t)-\hat{R}_t  \right)^2\)<br>
With each gradient descent step, the value function, on average, better approximates the real sum of rewards. The policy then has a more accurate value function with which to adjust its own actions. Together they explore the state and action space until a quality policy is reached. Though every step is not guaranteed to improve due to approximation error, even this “vanilla” reinforcement learning algorithm can learn to complete simple tasks.</p>

<p>This previously scary-looking algorithm should now be more approachable. Go over it and see that everything lines up for you, then we can take a look at the code.</p>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "kjabon/kjabon.github.io",
        "data-repo-id": "R_kgDOIuhwgg",
        "data-category": "Comments",
        "data-category-id": "DIC_kwDOIuhwgs4CVKPS",
        "data-mapping": "url",
        "data-strict": "0",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "top",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        © Copyright 2023 Kenneth  Jabon. Powered by al-folio.<br>Last updated: April 22, 2023.
      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', '');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
