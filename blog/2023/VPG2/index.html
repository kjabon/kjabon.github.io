<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Intro to Vanilla Policy Gradient, continued... | Kenneth  Jabon</title>
    <meta name="author" content="Kenneth  Jabon">
    <meta name="description" content="Extending the Theory and intuition behind one of our introductory algorithms">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://kjabon.github.io/blog/2023/VPG2/">
    
    <!-- Dark Mode -->
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <!-- Page/Post style -->
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "Intro to Vanilla Policy Gradient, continued...",
      "description": "Extending the Theory and intuition behind one of our introductory algorithms",
      "published": "April 14, 2023",
      "authors": [
        {
          "author": "Kenneth Jabon",
          "authorURL": ""
          
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Kenneth </span>Jabon</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repos</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>Intro to Vanilla Policy Gradient, continued...</h1>
        <p>Extending the Theory and intuition behind one of our introductory algorithms</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#future-return">Future return</a></div>
            <div><a href="#the-value-function">The value function</a></div>
            <div><a href="#n-step-return">N-step return</a></div>
            <div><a href="#generalized-advantage-estimation">Generalized Advantage Estimation</a></div>
            <div><a href="#batched-training-of-neural-nets">Batched training of neural nets</a></div>
            
          </nav>
        </d-contents>

        <p>This post is a direct continuation of <a href="/blog/2023/VPG">part 1</a> of introducing vanilla policy gradient. Start there!!</p>

<p>From the most basic possible working version of the algorithm introduced in that post, we now extend VPG with many of the foundational “tricks” in RL to make it into a useable algorithm.</p>

<hr>

<p>…Now, we can do better than the <em>very</em> simple algorithm presented in the last post. Take a breather, then let’s take a step back and think.</p>

<h2 id="future-return">Future return</h2>

<p>If I take an action, what parts of the trajectory will this affect? I’m not currently in possession of a time machine, so any action I take will not affect the past.<d-footnote>History may be written by the victor, but recorded events do not necessarily reflect what actually transpired.</d-footnote> And of course if I take an action, that will carry me to the next time step, so I cannot affect the present either. I can only affect the future.</p>

<p>Therefore, the part of the trajectory \(\tau\) (and thus, the rewards \(r_t\)) that any action has bearing on is the <strong>remainder of the trajectory</strong>, from the next time step to the end of the episode. Let’s call the discounted sum of <em>future</em> rewards “<em>future return</em>” \(R^f\). Our past and present rewards only serve to add noise, or variance, to our estimate of how good the action \(a_t\) is. This randomness slows down our training at best, and may reduce final performance.</p>

<details><summary>A note on terminology</summary>
<p>This set of rewards is sometimes known as the “rewards-to-go.” This term always struck me as kind of confusing, maybe because “to-go” isn’t as precise as I’d like. About half the time I see it, I think “‘to-go?’ Where are the rewards going? Oh, you mean ‘to-go’ as in what we have to go, or remaining.” Let’s avoid any temporary confusion and use “future return.”</p>
</details>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/FigFutureReward_ManimCE_v0.17.3-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/FigFutureReward_ManimCE_v0.17.3-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/FigFutureReward_ManimCE_v0.17.3-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/FigFutureReward_ManimCE_v0.17.3.png" class="img-fluid rounded" width="auto" height="auto" title="future reward" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">Rewards r (in red) are emitted at each time step, upon transitioning to a new state s. Future return R for a time step is the (discounted) sum of all future rewards.</figcaption>

</figure>

<p>Let’s be as explicit about this as possible.</p>

<p>Remember our trajectory \(\tau = (s_0, a_0, r_1), (s_1,a_1,r_2), (s_2, a_2, r_3),…\). Assume \(T\) time steps \(t\) in a trajectory \(\tau\); \(t \in \left[0,T\right)\), or \(t \in \left[0,T-1\right]\) if you prefer. Now consider the future return \(R^f_t\) at various time steps:</p>

<ul>
  <li>\(R^f_0\), i.e. \(R^f\) for the first time step (\(t=0\)) in a trajectory, contains all the rewards in that trajectory. Remember the first reward is \(r_1\).</li>
  <li>\(R^f_1\) contains all rewards, except the first reward: \(r_2\) onwards.</li>
  <li>\(R^f_{T-1}\)(the final time step) contains no rewards, because there’s no future!</li>
  <li>\(R^f_{T-2}\) contains only the final reward.</li>
</ul>

<p>In other words,</p>

\[R^f_{t} = \sum_{t'=t+1}^{T}\gamma^{t'-t-1}r_{t'}\]

<p>where T is the total number of time steps in an episode (and we’ve remembered to include the discounting factor \(\gamma\)).</p>

<h3 id="overwrite-your-pointers-and-vocabulary-yn">Overwrite your pointers and vocabulary? (Y/n)</h3>
<p>With the above argument about not being able to affect the past, it turns out there’s little reason to ever use the original full trajectory return $R$. So take a moment to internalize this concept. From here on out, we’re just going to “overwrite” our previous terminology; any time we say “return” or $R$, we’re talking about this new “future return” or $R^f$. It will be <em>implicit!</em> Let’s modify the policy gradient to use our new understanding of return instead:</p>

\[\hat{g}=\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} R_t\]

<p>It looks the same? Oh yes, but now we understand it differently!</p>

<p>Now, how do we actually calculate \(R_t\) for a particular time step? It seems obvious: simply take the discounted sum of all the future rewards! Well, it’s a bit trickier than that, because I don’t have a time machine that goes forward, either.</p>

<p>The obstacle here is that you never have the future rewards for a time step as you’re playing out that step.  You only have immediate access to the rewards you don’t care about, those in the past. So you’re forced to keep “rolling out” the trajectory to its conclusion, recording all future time steps, and only then you can accurately calculate \(R_t\) for all time steps.</p>

<details><summary>Yeah ok, but how do you calculate it??</summary>
<p>Once you’ve collected the rewards, \(R_t\) is easy enough to calculate in one backward-in-time pass with some simple dynamic programming.</p>

<p>Starting from the final time step, each \(R_t\) equals the reward of the current time step plus the discounted return from the next time step. That is,</p>

\[R_{T} = r_T\]

\[R_{T-1} = r_{T-1} + \gamma r_T\]

\[\vdots\]

\[R_{t} = r_t + \gamma R_{t+1}\]

<p>Now just loop this backward in time.</p>

</details>

<p>Whew. That seems a lot more tedious than we were hoping for, doesn’t it?<d-footnote>If my episode is infinite, it's technically impossible. I can get around the infinite sum by remembering I'm discounting, and truncate my sum at a reasonable time in the future.</d-footnote> Imagine you’re playing soccer for the first time, but the coach says you weren’t allowed to correct your mistakes while you’re playing! No no, only in the time between full games can you reflect on the error of your ways, and think how to improve for next time. Until then, suck it up and keep tripping over your feet. Intuitively, this seems inefficient, if not patently ridiculous.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/effective-soccer-coach-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/effective-soccer-coach-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/effective-soccer-coach-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/effective-soccer-coach.jpg" class="img-fluid rounded" width="auto" height="auto" title="Ponder the error of your ways elsewhere, ye goblins" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">No learning for you!</figcaption>

</figure>

<details><summary>Ok, I’m exaggerating a bit</summary>
<p>Now, modern “offline” algorithms aren’t actually this bad. They usually collect short sequences of transitions rather than full trajectories, and store them in a buffer to be sampled and asynchronously learned from. So the “reflection between games” is happening at all times, and the algorithm is “reflecting on all past games,” so to speak, rather than the most recent experience. In other words, in “online” algorithms the state space currently being learned from is the same that’s currently being explored by the active policy. However in “offline” algorithms the current policy (implicit or explicit) sets the explored state space, but the buffer of historical data encapsulates a larger and potentially different state space. There are pros and cons to each approach.</p>
</details>

<p>I would like my algorithm to be “online,” meaning I would like to avoid having to collect full episodes before knowing what my return \(R_t\) is. If I could do that, I would be able to learn from the reward of every time step in real time, and update my policy accordingly. The smaller the time lag in learning, the faster I can gain information about my new policy, and update it again. So, how to access the return now instead of later?</p>

<h2 id="the-value-function">The value function</h2>

<p>The value function \(V\)’s job is to approximate the return \(R\) at whatever time step \(t\) you need, and to generalize across states. A window into the future!</p>

<p>This is a lot simpler than it may seem. The value function \(V\) answers one question: how good is the state \(s\) to be in? Or, given the state \(s_t\) at time step \(t\), what is the best approximation \(V(s_t)\) of the expected return? With a deep neural network, we can quickly dispatch this question with good old supervised learning.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/FigValueFunction_ManimCE_v0.17.3-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/FigValueFunction_ManimCE_v0.17.3-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/FigValueFunction_ManimCE_v0.17.3-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/FigValueFunction_ManimCE_v0.17.3.png" class="img-fluid rounded" width="auto" height="auto" title="value function" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">Input state s, output return R. Do supervised learning on pairs calculated from trajectories.</figcaption>

</figure>

<p>We have a trajectory of states \(s_t\), and at the end of an episode (or the effective time horizon defined by the discounting term \(\gamma\)) we know we’re able to calculate the return \(R_t\) for each time step. We define the loss function for this supervised learning problem as the squared error between the value function \(V\)’s estimate and the actual return:</p>

\[\text{Loss}_{V_t}=\left(V(s_t)-R_t\right)^2\]

<p>and minimize this Loss by regression with some kind of gradient descent, training our neural network to predict the return.</p>

<p>And by sticking \(V_t\) in the place of \(R_t\), we <em>almost</em> have the complete policy gradient:</p>

\[\hat{g}=\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} V_t\]

<p>Now for a few caveats.</p>

<h3 id="bias-and-unstable-exploration">Bias and unstable exploration</h3>
<p>This approximation may only see a subset of the state space at any given time, and it may <em>never</em> see the full state space. We cannot assume it is an <em>unbiased estimator</em> unless it has trained uniformly on data from all parts of the state space, and typically that will only be the case once the training is complete, if it happens at all. In parts of the state space which are relatively less explored, it may be <em>very inaccurate</em>, which could lead to learning what the value function <strong>says</strong> is an optimal policy, but in <strong>reality</strong> is nonsense.</p>

<p>One of the best solutions to this problem is seen in the popular RL algorithm <a href="https://arxiv.org/abs/1707.06347" rel="external nofollow noopener" target="_blank">PPO</a>. Once you’ve understood VPG, you should work your way through <a href="https://spinningup.openai.com/en/latest/algorithms/trpo.html" rel="external nofollow noopener" target="_blank">TRPO</a>, and then <a href="https://spinningup.openai.com/en/latest/algorithms/ppo.html" rel="external nofollow noopener" target="_blank">PPO</a>. These links are decent references, though if you feel this blog post helped you understand VPG, let me know in the comments. If there’s enough interest I’ll continue these tutorials!</p>

<details><summary>The gist of PPO</summary>
<p>PPO’s goal is to avoid veering too far out into unexplored territory too quickly. It allows the value function time to “acclimate” to its new surroundings and give more accurate estimates, and so the policy is always learning from a “good enough” value function. It accomplishes this by limiting the size of the policy update step by clipping a surrogate loss function - but I won’t digress too much on this point, it requires its own post to do it justice.</p>
</details>

<h3 id="variance">Variance</h3>

<p>It also turns out that despite removing the noise from past rewards, this simple solution is still quite high-variance in practice. Because of this, it’s difficult for the actor to learn a good policy and to do so quickly. Luckily there are many battle-hardened techniques for reducing variance. Read on for one such technique!</p>

<h2 id="n-step-return">N-step return</h2>
<p>What is the lowest bias approximation of the return \(R\) for a state? Well, the return itself! That is, the actual sum of rewards that we calculate.</p>

<p>Hol’ up. Calculating \(R\) requires collecting a full trajectory \(\tau\). Aren’t we trying to avoid that, to learn faster?</p>

<p>Indeed we are. So perhaps we can combine the two approaches. What if I use the actual reward in the next time step, and add it to the value function’s approximation of the discounted return from that point on? Aha, a slightly more accurate estimate! This one step return is shown in the following figure and equation:</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/FigNStepReturn_ManimCE_v0.17.3-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/FigNStepReturn_ManimCE_v0.17.3-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/FigNStepReturn_ManimCE_v0.17.3-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/FigNStepReturn_ManimCE_v0.17.3.png" class="img-fluid rounded" width="auto" height="auto" title="one-step return" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">A step towards lower bias. Calculate the 1-step return for a time step from the following reward, and the value function evaluated on the following state.</figcaption>

</figure>

\[G_{t:t+1} = r_{t+1} + \gamma V(s_{t+1})\]

<p>Perhaps we can do better still by using the next <strong>two</strong> real rewards instead of only one. Or the next <strong>three</strong>? Each addition will require a slightly longer delay between taking an action and being able to learn from it, because we need to collect a longer sequence out of the full trajectory. This is known as the <strong>n-step return</strong> \(G\).</p>

\[G_{t:t+2} = r_{t+1} + \gamma r_{t+2}+\gamma^2 V(s_{t+2})\]

\[\vdots\]

\[G_{t:t+n} = r_{t+1} + \gamma r_{t+2}+ \cdots +\gamma^{n-1}r_{t+n}+\gamma^n V(s_{t+n})\]

<p>Now the question becomes: what’s the optimal tradeoff between accuracy and this learning delay? At this point you just need to experiment and see what works best for your problem, but I will tell you this: the answer is somewhere between 100% accuracy and zero delay, as seen in this figure:</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/nStepReturn-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/nStepReturn-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/nStepReturn-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/nStepReturn.png" class="img-fluid rounded" width="auto" height="auto" title="Bootstrapping" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

<div class="caption">
The performance of n-step return with various value of n, for a random walk task<d-footnote>Figure 7.2 from Sutton and Barto. [http://incompleteideas.net/book/the-book-2nd.html] Buy their book!</d-footnote>. The learning rate $\alpha$ is on the x axis. You can see a minimum error for n=4 in this particular instance. YMMV.
</div>

<p>It also turns out you can do even better by doing a weighted average of all of these options: one, two, three, etc. real rewards, followed by an approximation, and also experimenting to see what the right weighting is. There is a similar tradeoff between accuracy and computation speed, yielding a chart like the one above. Optimizing these hyperparameters is problem dependent.</p>

<h2 id="generalized-advantage-estimation">Generalized Advantage Estimation</h2>
<p>We have a more accurate approximation with our n-step return \(G\). We’ve left out one important piece, though: advantage. Before we add it, let’s talk about why we need it.</p>

<h3 id="variance-reduction">Variance reduction</h3>

<p>The n-step return handily deals with the bias problem in approximating the value function, but doesn’t do much to help the variance. To reduce the variance we need to understand where it comes from.</p>

<p>In any trajectory, there are several sources of randomness contributing to what direction it takes as it’s rolled out:</p>
<ul>
  <li>We may have a random initial state \(s_0\).</li>
  <li>For a given policy \(\pi\) and state \(s_t\), in general we <strong>sample</strong> actions \(a_t\) from a distribution.</li>
  <li>The environment may have probabilistic transitions. For a given state $s$ and action $a$, in general, the resulting transition to the next state $s_{t+1}$ is <em>also</em> sampled from a probability distribution, \(p(s' \vert s, a)\). We swept this one under the rug until now (and will continue to do so for the rest of this post, so don’t worry).</li>
</ul>

<p>Each variation at each time step ultimately leads to the variance of the return \(R\). The longer the trajectory, the more pronounced this effect will become. Therefore, we want to remove the effect of this future variation.</p>

<h3 id="advantage">Advantage</h3>

<p>I’ll start with the punchline: it turns out you can take the expression you want (the n-step return), and subtract another similar expression (a baseline) to reduce the variance of your approximation. Take the following figure as a simple example.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/FigAdvantage_ManimCE_v0.17.3-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/FigAdvantage_ManimCE_v0.17.3-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/FigAdvantage_ManimCE_v0.17.3-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/FigAdvantage_ManimCE_v0.17.3.png" class="img-fluid rounded" width="auto" height="auto" title="baseline" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">Left plot shows a cubic function plus noise. Subtracting the cubic function yields the plot on the right, which reduces the variance of the possible values of the function,</figcaption>

</figure>

<p>We’re in the business of using “real-time” approximations, so the n_step return is what we’d like a low-variance estimate of. Consider the 1-step return.</p>

\[r_{t+1} + \gamma V(s_{t+1})\]

<p>The most common choice for a baseline is the value function, so the above becomes:</p>

\[A_t = r_{t+1} + \gamma V(s_{t+1}) - V(s_t)\]

<p>or more generally,</p>

\[A_t = G_{t:t+n} - V(s_t)\]

<p>Now hold on. This is just the same as we had before. It’s the n-step return, but for some reason we’re subtracting the value function evaluated on the current state. What’s the point?</p>

<p>Intuitively, the advantage at its most <em>basic</em> level answers the question: how much better is the <em>actual</em> return \(R\) better than my estimate \(V(s)\)? In a sense, it’s a measure of surprise. If taking an action \(a_t\) given the state \(s_t\) and transitioning to a state \(s_{t+1}\) is more rewarding than we think that state is in general (\(V(s)\)), then surely we ought to adjust our policy to do that more often.</p>

<p>With the addition of advantage, we have generalized advantage estimation (GAE) in a nutshell.  I’ve focused on intuition here. For more specifics in how these calculations are done, see the <a href="https://arxiv.org/abs/1707.06347" rel="external nofollow noopener" target="_blank">paper</a>; your mental picture should now be organized to have all those formalisms swiftly fall into place.</p>

<p>We can now convert our \(V_t\) to \(A_t\).</p>

\[\hat{g}=\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} A_t\]

<p>It is worth noting that GAE is not limited to on-policy methods; it can be applied anywhere you use a value function, or even a Q function.</p>

<details><summary>Ok, but why does it <em>actually</em> work?</summary>
<p>If the intuition isn’t cutting it for you, see <a href="/blog/2023/Baseline">this brief post</a> about why subtracting a baseline reduces variance.</p>

</details>

<h2 id="batched-training-of-neural-nets">Batched training of neural nets</h2>
<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/cookie-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/cookie-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/cookie-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/cookie.jpg" class="img-fluid rounded" width="auto" height="auto" title="kooky" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

<p>Finally, we sum the policy gradient over a set \(\mathcal{D}\) of collected trajectories \(\tau\), dividing by the number of trajectories \(\vert \mathcal{D} \vert\) to get the sampled mean of the above equation.</p>

<p>Why? Well, to gain a lower variance estimator of the true policy gradient. This should be quite familiar to you if you’ve ever done mini-batched stochastic gradient descent. Many trajectories averaged together will smooth out the noise from any one trajectory, and better represent the optimal policy gradient.</p>

<h3 id="policy-gradient-update">Policy gradient update</h3>

<p>With this, we finally have the full policy gradient!</p>

\[\hat{g}=\frac{1}{\left| \mathcal{D}_k \right|}\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\nabla _{\theta}\log\pi_\theta (a_t\vert s_t)_{\theta _k} A_t\]

<p>Where \(k\) is the step index in the training loop.<br>
With this term, we can update the parameters of our policy with SGD:</p>

\[\theta_{k+1} = \theta_{k}+\alpha_k \hat{g}_k\]

<p>or otherwise use an optimizer like Adam.</p>

<h3 id="value-function-update">Value function update</h3>

<p>This is the policy learning taken care of. Now we turn to updating the value function. Recalling its loss function:</p>

\[\text{Loss}_{V_t}=\left(V_\phi(s_t)-\hat{R}_t  \right)^2\]

<p>We add a sum over all time steps in the trajectory, and a sum over all trajectories in our gathered set of trajectories \(\mathcal{D}\):</p>

\[\frac{1}{\left\vert  \mathcal{D}_k \right\vert T}\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\left(V_\phi(s_t)-\hat{R}_t  \right)^2\]

<p>We divide by \(T\) because we’re not doing a sum of log likelihoods like for the policy gradient, but instead are calculating mean squared error.</p>

\[\phi_{k+1}=\arg \min_\phi \frac{1}{\left\vert  \mathcal{D}_k \right\vert T}\sum_{\tau\in \mathcal{D}_k}\sum_{t=0}^{T}\left(V_\phi(s_t)-\hat{R}_t  \right)^2\]

<p>Standing in for the \(\arg \min\), we can use SGD or Adam to compute the update to the value function parameters \(\phi\).</p>

<p>With each gradient descent step, the value function, on average, better approximates the real sum of rewards. The policy then has a more accurate value function with which to adjust its own actions. Together, they explore the state and action space until a locally optimal policy is reached. Though every step is not guaranteed to improve due to approximation error, even this “vanilla” reinforcement learning algorithm can learn to complete simple tasks.</p>

<p>Now you can put all these ingredients together. Convince yourself that you understand the full algorithm!</p>

<hr>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/pseudoVPG.svg-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/pseudoVPG.svg-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/pseudoVPG.svg-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/pseudoVPG.svg" class="img-fluid rounded" width="auto" height="auto" title="Pseudocode" data-zoomable="" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture>

</figure>

<p>Go over it and see that everything lines up for you.</p>

<hr>

<p>Thanks for reading!</p>

<p>Now we can take a look at an implementation in JAX in the <a href="/blog/2023/VPGInJax">next post</a>.</p>


      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "kjabon/kjabon.github.io",
        "data-repo-id": "R_kgDOIuhwgg",
        "data-category": "Comments",
        "data-category-id": "DIC_kwDOIuhwgs4CVKPS",
        "data-mapping": "url",
        "data-strict": "0",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "top",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        © Copyright 2023 Kenneth  Jabon. Powered by al-folio.<br>Last updated: May 03, 2023.
      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', '');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
