<!DOCTYPE html>
<!-- _layouts/distill.html --><html>
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Image Regression Lessons Learned (in JAX) | Kenneth  Jabon</title>
    <meta name="author" content="Kenneth  Jabon">
    <meta name="description" content="An exercise in image processing">
    <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700%7CRoboto+Slab:100,300,400,500,700%7CMaterial+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    <!-- Styles -->
    
    <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;">
    
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://kjabon.github.io/blog/2023/ImageRegressionJax/">
    
    <!-- Dark Mode -->
    


    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    <!-- Distill js -->
    <script src="/assets/js/distillpub/template.v2.js"></script>
    <script src="/assets/js/distillpub/transforms.v2.js"></script>
    <script src="/assets/js/distillpub/overrides.js"></script>
    
    <!-- Page/Post style -->
    <style type="text/css">
      .fake-img {
  background: #bbb;
  border: 1px solid rgba(0, 0, 0, 0.1);
  box-shadow: 0 0px 4px rgba(0, 0, 0, 0.1);
  margin-bottom: 12px;
} .fake-img p {
  font-family: monospace;
  color: white;
  text-align: left;
  margin: 12px 0;
  text-align: center;
  font-size: 16px;
}

    </style>
  </head>

  <body>
<d-front-matter>
    <script async type="text/json">{
      "title": "Image Regression Lessons Learned (in JAX)",
      "description": "An exercise in image processing",
      "published": "July 10, 2023",
      "authors": [
        {
          "author": "Kenneth Jabon",
          "authorURL": ""
          
        }
        
      ],
      "katex": {
        "delimiters": [
          {
            "left": "$",
            "right": "$",
            "display": false
          },
          {
            "left": "$$",
            "right": "$$",
            "display": true
          }
        ]
      }
    }</script>
  </d-front-matter>

  

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Kenneth </span>Jabon</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">about</a>
              </li>
              
              <!-- Blog -->
              <li class="nav-item active">
                <a class="nav-link" href="/blog/">blog<span class="sr-only">(current)</span></a>
              </li>

              <!-- Other pages -->
              <li class="nav-item ">
                <a class="nav-link" href="/projects/">projects</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/publications/">publications</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">repos</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">cv</a>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="post distill">

      <d-title>
        <h1>Image Regression Lessons Learned (in JAX)</h1>
        <p>An exercise in image processing</p>
      </d-title>

      <d-byline></d-byline>

      <d-article>
        <d-contents>
          <nav class="l-text figcaption">
          <h3>Contents</h3>
            <div><a href="#the-problem">The problem</a></div>
            <div><a href="#get-signs-of-life-and-starter-tips">Get signs of life, and starter tips</a></div>
            <div><a href="#overfit-on-purpose">Overfit on purpose</a></div>
            <div><a href="#regularize">Regularize</a></div>
            <div><a href="#speed-is-the-key">Speed is the key</a></div>
            <div><a href="#further-optimization">Further optimization</a></div>
            
          </nav>
        </d-contents>

        <p>To get up to speed with image processing, and doing so in JAX, I decided to try something not typically found in your starter image classification notebook. This project was (heavily) modified from the ResNet example found on the <a href="https://github.com/deepmind/dm-haiku/tree/main/examples/imagenet" rel="external nofollow noopener" target="_blank">haiku</a> GitHub page. Find my code on <a href="https://github.com/kjabon/image_regression" rel="external nofollow noopener" target="_blank">GitHub</a>.</p>

<p>In this post, partly inspired by a <a href="http://karpathy.github.io/2019/04/25/recipe/" rel="external nofollow noopener" target="_blank">list</a> of NN training tips by Andrej Karpathy, I’ll walk through the process of trying to train with low loss on the following dataset.</p>

<h2 id="the-problem">The problem</h2>
<ul>
  <li>~100k samples: x: X-ray images of lungs, y: age label (this is a <a href="https://www.kaggle.com/datasets/nih-chest-xrays/data" rel="external nofollow noopener" target="_blank">dataset</a> found on kaggle)</li>
  <li>Perform regression from image to age.</li>
</ul>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/xray1-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/xray1-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/xray1-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/xray1.png" class="img-fluid rounded " width="auto" height="auto" title="X-ray 1" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">Example x-ray image.</figcaption>

</figure>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/xray2-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/xray2-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/xray2-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/xray2.png" class="img-fluid rounded " width="auto" height="auto" title="X-ray 2" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">Example x-ray image.</figcaption>

</figure>

<h2 id="get-signs-of-life-and-starter-tips">Get signs of life, and starter tips</h2>

<p>First, you want to make sure your training pipeline actually works and is bug-free. Does your training loss curve reliably decrease and converge to some reasonable (in the context of your dataset) minimum? If it is merely oscillating in place with no sign of training on the dataset, or forever increasing, many things could be going wrong. A bug in your code should always be your first guess. In deep learning, things have a sneaky tendency to fail silently. Without correct code, you could easily spend days chasing barely perceptible gains by tweaking architecture, hyperparameters, trying different optimizers, etc., when it turned out you were computing your loss incorrectly, or simply forgot to apply your gradient.</p>

<h3 id="start-simple">Start simple</h3>

<p>Anything is on the table as far as bugs go, and for this reason it’s important to just get <em>something that works</em> as soon as possible. Start with a relatively low-complexity model, like a basic CNN. If you’re at the point where this seems menacing, just start with a basic MLP, or even linear regression. Start simpler and work your way up in complexity later. Prove to yourself that your training loop is computing loss and gradients and applying them correctly. Prove to yourself that your dataset is actually labelled correctly by going through it manually. Better yet, start from a known working dataset online to prove that your training pipeline works, then swap in the dataset of interest.</p>

<h3 id="be-one-with-the-dataset">Be one with the dataset</h3>

<p>Even with correct code, your dataset may have little to no real relationships to be learned at all! For this reason it’s important to attune yourself to the dataset. Really immerse yourself in the relationships you expect and the mechanisms behind them, if any. Any deep learning practitioner needs some data scientist tools in their belt. Visualize, normalize, and clean your dataset.</p>

<p>Once you are satisfied the code is correct, you may still not be training effectively. Your learning rate could be too low, or too high (this is typically the first thing to check). Your model may not have enough capacity to capture the relationships in the data.</p>

<p>Once you’re getting off the ground…</p>

<h2 id="overfit-on-purpose">Overfit on purpose</h2>

<p>As with any supervised learning problem, your first goal is to overfit the problem. I.e., while paying no mind to the validation loss, see that your model is actually capable of learning the dataset, whether or not it is generalizing well or overfitting. Put another way, make sure you have enough model capacity to accurately map from x to y.</p>

<p>We’ll start with a Resnet with as few layers as we can, and start adding blocks until we get a desirable training error. In this case, let’s consider ourselves satisfied once our training error is less than 1 (estimating age within +/- 1 years). In theory we could go arbitrarily low, effectively memorizing the dataset.</p>

<p>In the figure below, we’ve run each of these Resnet models with early stopping on the train loss.  Clearly, increasing the number of blocks (parameters) allows it to more accurately model the dataset, but we see diminishing returns, possibly suggesting that we are nearing the actual relationship that the data actually shows. It’s also possible that <em>not</em> using early stopping and continuing to train these networks would decrease the training loss further. If this weren’t a demo problem, this would be the next thing to check (we’ll skip it in this post for the sake of time).</p>

<p>Here we standardize the inputs to the dataset mean and variance which we’ve calculated separately. We also use Batchnorm for training stability.</p>

<details><summary>Standardize inputs</summary>
<d-code block="" language="python">
# ...get the next batch from the dataset...
# extract the images
images = batch['image']


# Example mean and stddev for an RGB dataset (not used here)
# MEAN_RGB = jnp.array([88.94542, 75.03732, 71.47706])
# STDDEV_RGB = jnp.array([71.67816, 66.773605, 65.78593])
stddev_greyscale = jnp.array([42.48881573])
mean_greyscale = jnp.array([126.52482573])
images -= mean_greyscale
images /= stddev_greyscale
</d-code>
</details>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/imageRegressionOverfit-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/imageRegressionOverfit-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/imageRegressionOverfit-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/imageRegressionOverfit.png" class="img-fluid rounded " width="auto" height="auto" title="Overfit" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">Training loss as we increase model size, for as long as it helps, then a little longer.</figcaption>

</figure>

<h3 id="maybe-even-go-a-little-bigger">Maybe even go a little bigger</h3>

<p>It’s often a good idea to leave some wiggle room, rather than using a model that is just barely good enough. It helps to allow the model multiple avenues to approximate the dataset, rather than there being one perfect set of trained weights. As it turns out, having a model that is just barely capable of accurately representing the dataset may actually be detrimental to performance, so it’s best to give it a little extra capacity once you think you’ve hit a minimum. See <a href="https://arxiv.org/pdf/1912.02292.pdf" rel="external nofollow noopener" target="_blank">this paper</a> for thoughts on why this may be the case. Once we start regularizing, a little more capacity won’t hurt either: e.g. because we use only a fraction of the network during training during dropout. We’ll worry about increasing the capacity later if need be. Let’s use ResNet34 from here on.</p>

<h2 id="regularize">Regularize</h2>

<p>At this point, it’s practically guaranteed we are overfitting the data, particularly as our model size increases. Let’s take a look at the validation error curves from the previous training run.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/imRegValOverfit-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/imRegValOverfit-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/imRegValOverfit-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/imRegValOverfit.png" class="img-fluid rounded " width="auto" height="auto" title="Overfit" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">Validation loss as we increase model size (same runs as previous figure).</figcaption>

</figure>

<p>Yes, so we can see that our validation error is quite high.</p>

<p>As a side note, the generalization error is the difference between validation and training errors. If generalization error is higher than training error, (as it is here), this indicates we need to regularize our model. Since this is already the case even at the minimum of the above curves, early stopping will not help us here.</p>

<p>So, our next step is to drive down this validation error with some kind of regularization. This will allow our model to generalize to the dataset’s distribution, rather than overfitting to the training data.</p>

<h3 id="data-augmentation">Data augmentation</h3>

<h4 id="its-a-kind-of-regularization">It’s a kind of regularization.</h4>

<p>Often we overfit to particular details in a dataset when, in actuality, more general features are the only ones that apply. Suppose that patients over 60 getting x-rayed are recommended to a particular hospital or clinic for their specialty in older patients. Dependent on the x-ray machine or the post processing of its outputs, perhaps all the x-rays to come out of this clinic have a grey tint, or some form of aberration or postmark around the edges of the image. The neural network will learn (incorrectly, for the global dataset) that grey-tinted x-rays correspond to older patients. Perhaps some similar fluke has patients under 30 have their x-rays mirrored along the y axis.</p>

<p>Enter data augmentation. Images are randomly transformed to be slightly different from the original each time they’re passed through the network, so that incidental details are not learned to be the cause for particular labels. Put another way, we add noise to artificially increase the size of the dataset. (If getting more data is a relatively cheap option, you should absolutely do that first).</p>

<p><a href="https://arxiv.org/pdf/1909.13719.pdf" rel="external nofollow noopener" target="_blank">RandAugment</a> is the name of the game at the time of this writing, appearing to have the best performance out of comprehensive image augmentation techniques. It blends many different kinds of classic image augmentations, from contrast to translation.</p>

<p>However, dependent on the type of image to be classified or regressed, don’t blindly use augmentations that would obscure necessary info to you. Use common sense here.</p>

<details><summary>Here’s the preprocessing code implementing RandAugment</summary>
<d-code block="" language="python">
# Each of these layers is implemented as a function in JAX
# in the same file, preprocessing.py

randomLayers = [
    flipX,
    translate,
    color_degen, 
    auto_contrast,
    equalize,
    random_contrast, 
    brightness,
    cutout,
    rotate,
]

def randomAugmentJax(images, rng, policy, augmentations_per_image = 3, rate = 10/11):
    # For each image, we're going to want a different rng, so we need to split
    keys = jax.random.split(rng, images.shape[0])

    def apply_rand_layer(image, layer_select_key, layer_key):
        randIndex = jax.random.randint(layer_select_key, (), 0, len(randomLayers)-1)
        for i in range(len(randomLayers)):
            image = jnp.where(randIndex == i, randomLayers[i](image, layer_key, policy), image)
        return image

    # Now, we need to define a function we'll vmap to our images
    def augment(image, loop_key):

        #within this function, we just copy the rand_augment apply
        original_image = image

        for _ in range(augmentations_per_image):
            # get a skip uniform random number
            loop_key, skip_key, layer_select_key, layer_key = jax.random.split(loop_key,4)
            skip_augment = jax.random.uniform(skip_key)
            #skip based on rng above (identity)
            #else, apply a random layer to the input
            image = jnp.where(skip_augment &gt; rate,
                              original_image,
                              apply_rand_layer(image, layer_select_key, layer_key))
        return image

    return jax.vmap(augment)(images, keys)
</d-code>
</details>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/resnet34wAug-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/resnet34wAug-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/resnet34wAug-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/resnet34wAug.png" class="img-fluid rounded " width="auto" height="auto" title="With data augmentation" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">Training and validation loss upon adding data augmentation.</figcaption>

</figure>

<p>The clear difference here is our validation loss tracks the training loss curve. It isn’t a one-to-one comparison, as data augmentations aren’t applied during eval, so we expect the training loss to be artificially higher. However, as the training loss decreases, the validation loss does as well; they don’t split apart early on as before, with an increasing generalization error. This gives us confidence that we can continue training for a long time without overfitting. We see our validation loss drops from 4.4 to 3.4. Now we’re seeing some results!</p>

<p>I used early stopping here, but it is likely the validation loss would decrease even further. Additionally, a run I did before writing this post with the same parameters, only adding dropout to the final MLP after the ResNet blocks, got down to 2.91 years mean absolute error when taken to 80k training steps. (Dropout rate was set to 0.25). With that in mind, let’s turn on dropout for the rest of this post.</p>

<figure>

  <picture>
    
    <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/resnet34wAugAndDropout-480.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/resnet34wAugAndDropout-800.webp"></source>
    <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/resnet34wAugAndDropout-1400.webp"></source>
    

    <!-- Fallback to the original file -->
    <img src="/assets/img/resnet34wAugAndDropout.png" class="img-fluid rounded " width="auto" height="auto" title="With dropout" onerror="this.onerror=null; $('.responsive-img-srcset').remove();">

  </picture><figcaption class="caption">Training and validation loss upon adding dropout.</figcaption>

</figure>

<h2 id="speed-is-the-key">Speed is the key</h2>

<p>While doing all these tasks, your concern should also be speed of training for a low iteration time.</p>

<p>I recommend you don’t use the python multiprocessing library for data augmentation: inter-process communication is too slow because it writes to disk, and you’ll be moving a lot of data around. There are python libraries out there which purport to use RAM for communication, but I did not find them to be particularly plug-and-play.<br>
Instead, learn to use tfds for your data pipeline and dataset creation via their examples and documentation. It may seem like a big hurdle at first, but it <em>will</em> pay off in speed, and it’s applicable to any supervised learning problem you can imagine. (If you’re using pytorch, there is a similar dataloader for you to learn).</p>

<details><summary>Here’s the tfds code preparing the dataset pipeline including some preprocessing</summary>
<d-code block="" language="python">
import tensorflow_datasets as tfds
def load_dataset(split):
    # Maximize batch size for the available VRAM
    # Bigger images &amp; bigger networks = smaller batch size
    imgSize = imgSizes[FLAGS.img_size_num]
    resNetRelSize = resnetSizes[FLAGS.resnet_num] / 50
    batchSize = nearest2(int(FLAGS.batch_size/resNetRelSize*(128/imgSize)**2))
    
    train_dataset = tfds.load(datasetName, split=split)
    num_datapoints = train_dataset.cardinality().numpy()
    train_dataset = train_dataset.repeat().shuffle(1024)

    train_dataset = train_dataset.map(lambda x: (preprocess(x, True)), num_parallel_calls=-1)

    train_dataset = train_dataset.batch(batchSize).batch(jax.local_device_count()).prefetch(-1)

    train_dataset = tfds.as_numpy(train_dataset)

    return train_dataset, num_datapoints
    
# Found in preprocessing.py
def preprocess(x):
    # This is a little barebones; before moving preprocessing to GPU, 
    # everything was done here via tfds on cpu.
    x,y = deconstruct(x)
    if x.get_shape()[2] == 1:
        x = rgb(x) # convert greyscale to 3 channels for uniform architecture.
    x = cast_fn(x)
    return reconstruct(x, y)
</d-code>
</details>

<p>This said, if you’re doing all your data preprocessing (e.g. image augmentation) in tfds, you might find you’re CPU-bottlenecked; this was the case for me. With access to highly optimized libraries which can easily execute batched operations on the GPU, or better yet, multiple GPUs (JAX!), you would be remiss to ignore that possibility for your data augmentation. GPUs are not just for neural networks! When in doubt, see if you can implement your preprocessing as part of your network, with a flag for training/eval. This enabled me to go from 20 to near-100% utilization across 3 GPUs.</p>

<details><summary>This code defines the network, but importantly includes preprocessing steps on the GPU in JAX!</summary>
<d-code block="" language="python">
def _forward(
        batch,
        is_training: bool,
        apply_rng=None
) -&gt; jax.Array:
    """Forward application of the resnet."""
    images = batch['image']
    
    # Do preprocessing on the GPU!!
    # randomAugmentJax is found in preprocessing.py
    if is_training:
        apply_rng, aug_rng = jax.random.split(apply_rng)
        images = randomAugmentJax(images,aug_rng,get_policy(),
                                  thruLayer=FLAGS.thruLayer,
                                  onlyLayer=FLAGS.onlyLayer)

    # Normalize the inputs on the GPU!!
    STDDEV_RGB = jnp.array([42.48881573])
    MEAN_RGB = jnp.array([126.52482573])
    images -= MEAN_RGB
    images /= STDDEV_RGB


    net = resnets[FLAGS.resnet_num](FLAGS.layer_size,
                                    resnet_v2=FLAGS.model_resnet_v2,
                                    bn_config={'decay_rate': FLAGS.model_bn_decay})
    mlp = hk.nets.MLP(
        output_sizes=[FLAGS.layer_size, FLAGS.layer_size],
        activate_final=True
    )

    yHold = net(images, is_training=is_training)
    y = hk.BatchNorm(True, True, FLAGS.model_bn_decay)(yHold, is_training=is_training)
    y = mlp(y, FLAGS.dropout_rate, apply_rng) if apply_rng is not None else mlp(y)
    # skip connection
    y = y + yHold
    y = hk.Linear(1)(y)  # No final activation
    return y.flatten()
# Transform our forwards function into a pair of pure functions.
# This will later be run on all devices with jax.pmap()
forward = hk.transform_with_state(_forward)
</d-code>
</details>

<p>It may also be worth using half precision (f16) computation. I won’t get into it here, but the code found on the accompanying GitHub (link at the bottom of this post) allows this by simply changing the <code class="language-plaintext highlighter-rouge">mp_policy</code> flag to <code class="language-plaintext highlighter-rouge">'p=f32,c=f16,o=f32'</code>. In my testing this approximately doubles the speed, sometimes at the cost of training stability (you are likely to see NaNs until you fiddle with your learning rate and batch size to make them go away). So far I haven’t found much difference in the overall performance.</p>

<p>Last but not least, balance workload between CPU and GPU so that you’re not bottlenecked anywhere. This may be more hassle than it’s worth, but is worth considering if you have nothing to do but wait for your training runs to complete.</p>

<h2 id="further-optimization">Further optimization</h2>

<h3 id="hyperparameter-tuning">Hyperparameter tuning</h3>

<p>Once you have a reasonable h-param range and decent performance, optuna is your friend, but not before. You may have bugs in your code, or the model may under/overfit without the right details as above.</p>

<p>One of these hyperparameters may even be deeper networks. Now that we’ve added augmentation, we may squeeze a few % more out of the data with a higher capacity model.</p>

<p>I will leave this as an exercise for the reader :) .</p>

<h3 id="modern-architectures-and-more">Modern architectures and more</h3>

<p>I did not touch on it in this post, but at this point if the application requires, it may be worth checking out newer networks. Checking out the ImageNet leaderboard shows Vision Transformers, among many others, outperforming ResNet. However, you may find these have their own trade-offs, perhaps requiring much larger datasets or pre-training to get this performance.</p>

<p>Furthermore, in my experience, feature extraction from / fine-tuning a pre-trained network (e.g., a ResNet-50 trained on ImageNet) is likely to give you <strong>astounding</strong> results with minimal training time, in particular if your dataset of interest is similar to the images found in the larger dataset the network was pre-trained on. ImageNet may not help much with X-rays, but other applicable pre-trained networks may be out there.</p>

<p>Good luck in your exploration!</p>

<hr>

<p>Thanks for reading! Find the code on <a href="https://github.com/kjabon/image_regression" rel="external nofollow noopener" target="_blank">GitHub</a>.</p>

      </d-article>

      <d-appendix>
        <d-footnote-list></d-footnote-list>
        <d-citation-list></d-citation-list>
      </d-appendix>

      <d-bibliography src="/assets/bibliography/"></d-bibliography><div id="giscus_thread" style="max-width: 800px; margin: 0 auto;">
  <script>
    let giscusTheme = localStorage.getItem("theme");
    let giscusAttributes = {
        "src": "https://giscus.app/client.js",
        "data-repo": "kjabon/kjabon.github.io",
        "data-repo-id": "R_kgDOIuhwgg",
        "data-category": "Comments",
        "data-category-id": "DIC_kwDOIuhwgs4CVKPS",
        "data-mapping": "url",
        "data-strict": "0",
        "data-reactions-enabled": "1",
        "data-emit-metadata": "0",
        "data-input-position": "top",
        "data-theme": giscusTheme,
        "data-lang": "en",
        "crossorigin": "anonymous",
        "async": "",
    };


    let giscusScript = document.createElement("script");
    Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
    document.getElementById("giscus_thread").appendChild(giscusScript);
  </script>
  <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a>
</noscript>
</div>
</div>

    <!-- Footer -->    <footer class="sticky-bottom mt-5">
      <div class="container">
        © Copyright 2023 Kenneth  Jabon. Powered by al-folio.<br>Last updated: August 05, 2023.
      </div>
    </footer>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id="></script>
  <script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){ window.dataLayer.push(arguments); }
    gtag('js', new Date());
    gtag('config', '');
  </script>
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  
</body>
</html>
